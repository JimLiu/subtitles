1
00:00:05,275 --> 00:00:09,191
Sometimes people think of a large language model as a knowledge store,

2
00:00:09,192 --> 00:00:13,014
as if it's learned to memorize a lot of information, maybe off the internet,

3
00:00:13,015 --> 00:00:16,686
so when you ask the question, it can answer the question.

4
00:00:16,686 --> 00:00:23,056
But I think an even more useful way to think of a large language model is sometimes as a reasoning engine

5
00:00:23,057 --> 00:00:27,315
in which you can give it chunks of text or other sources of information.

6
00:00:27,315 --> 00:00:29,554
And then the large language model, LLM,

7
00:00:29,555 --> 00:00:32,986
will maybe use this background knowledge that's learned off the internet,

8
00:00:32,987 --> 00:00:41,209
but to use the new information you give it to help you answer questions or reason through content or decide even what to do next.

9
00:00:41,209 --> 00:00:44,612
And that's what LangChain's agents framework helps you to do.

10
00:00:45,674 --> 00:00:48,317
Agents are probably my favorite part of LangChain.

11
00:00:48,317 --> 00:00:52,280
I think they're also one of the most powerful parts, but they're also one of the newer parts.

12
00:00:52,280 --> 00:00:56,064
So we're seeing a lot of stuff emerge here that's really new to everyone in the field.

13
00:00:56,064 --> 00:00:59,899
And so this should be a very exciting lesson as we dive into what agents are,

14
00:00:59,900 --> 00:01:02,513
how to create and how to use agents,

15
00:01:02,514 --> 00:01:06,874
how to equip them with different types of tools like search engines that come built into LangChain.

16
00:01:07,406 --> 00:01:17,032
and then also how to create your own tools so that you can let agents interact with any data stores, any APIs, any functions that you might want them to.

17
00:01:17,032 --> 00:01:23,216
So this is exciting, cutting-edge stuff, but already with emerging important use cases.

18
00:01:23,216 --> 00:01:26,598
So with that, let's dive in.

19
00:01:26,598 --> 00:01:33,022
First, we're going to set the environment variables and import a bunch of stuff that we will use later on.

20
00:01:33,022 --> 00:01:35,163
Next, we're going to initialize a language model

21
00:01:35,777 --> 00:01:37,497
We're going to use ChatOpenAI.

22
00:01:37,497 --> 00:01:41,278
And importantly, we're going to set the temperature equal to zero.

23
00:01:41,278 --> 00:01:51,100
This is important because we're going to be using the language model as the reasoning engine of an agent where it's connecting to other sources of data and computation.

24
00:01:51,100 --> 00:01:56,281
And so we want this reasoning engine to be as good and as precise as possible.

25
00:01:56,281 --> 00:02:02,502
And so we're going to set it to zero to get rid of any randomness that might arise.

26
00:02:02,502 --> 00:02:04,002
Next, we're going to load some tools.

27
00:02:05,048 --> 00:02:10,911
The two tools that we're going to load are the LLM math tool and the Wikipedia tool.

28
00:02:10,911 --> 00:02:20,656
The LLM math tool is actually a chain itself, which uses a language model in conjunction with a calculator to do math problems.

29
00:02:20,656 --> 00:02:24,156
The Wikipedia tool is an API that connects to Wikipedia,

30
00:02:24,157 --> 00:02:28,781
allowing you to run search queries against Wikipedia and get back results.

31
00:02:31,686 --> 00:02:33,767
Next, we're going to initialize an agent.

32
00:02:33,767 --> 00:02:39,469
We're going to initialize the agent with the tools, the language model, and then an agent type.

33
00:02:39,469 --> 00:02:43,550
Here, we're going to use CHAT_ZERO_SHOT_REACT_DESCRIPTION.

34
00:02:43,550 --> 00:02:46,871
The important things to note here are first chat.

35
00:02:46,871 --> 00:02:50,812
This is an agent that has been optimized to work with chat models.

36
00:02:50,812 --> 00:02:52,713
And second react.

37
00:02:52,713 --> 00:02:58,495
This is a prompting technique designed to get the best reasoning performance out of language models.

38
00:02:59,818 --> 00:03:03,783
We're also going to pass in handle_parsing_errors equals true.

39
00:03:03,783 --> 00:03:14,716
This is useful when the language model might output something that is not able to be parsed into an action input, which is the desired output.

40
00:03:14,716 --> 00:03:19,862
When this happens, we'll actually pass the misformatted text back to the language model and ask it to correct itself.

41
00:03:21,095 --> 00:03:24,098
Finally, we're going to pass in verbose equals true.

42
00:03:24,098 --> 00:03:30,683
This is going to print out a bunch of steps that makes it really clear to us in the Jupyter notebook what's going on.

43
00:03:30,683 --> 00:03:34,986
We'll also set debug equals True at the global level later on in the notebook

44
00:03:35,000 --> 00:03:38,689
so we can see in more detail what exactly is happening.

45
00:03:38,689 --> 00:03:41,031
First, we're going to ask the agent a math question.

46
00:03:41,031 --> 00:03:42,513
What is 25% of 300?

47
00:03:42,513 --> 00:03:47,897
This is a pretty simple question, but it will be good to understand what exactly is going on.

48
00:03:49,234 --> 00:03:56,017
So we can see here that when it enters the AgentExecutor chain, that it first thinks about what it needs to do.

49
00:03:56,017 --> 00:03:57,314
So it has a thought.

50
00:03:57,958 --> 00:03:59,514
It then has an action.

51
00:03:59,579 --> 00:04:06,429
And this action is actually a JSON blob corresponding to two things, an action and an action input.

52
00:04:06,622 --> 00:04:09,103
The action corresponds to the tool to use.

53
00:04:09,103 --> 00:04:11,204
So here it says calculator.

54
00:04:11,204 --> 00:04:14,005
The action input is the input to that tool.

55
00:04:14,005 --> 00:04:15,806
And here it's a string of 300 times 0.25.

56
00:04:19,275 --> 00:04:24,059
Next, we can see that there's observation with answer in a separate color.

57
00:04:24,059 --> 00:04:33,467
This observation, answer equals 75.0, is actually coming from the calculator tool itself.

58
00:04:33,467 --> 00:04:37,351
Next, we go back to the language model when the text turns to green.

59
00:04:37,351 --> 00:04:39,012
We have the answer to the question.

60
00:04:39,012 --> 00:04:43,416
Final answer, 75.0, and that's the output that we get.

61
00:04:47,042 --> 00:04:53,246
This is a good time to pause and try out different math problems of your own.

62
00:04:53,246 --> 00:04:57,490
Next, we're going to go through an example using the Wikipedia API.

63
00:04:57,490 --> 00:04:59,957
Here, we're going to ask it a question about Tom Mitchell,

64
00:05:00,086 --> 00:05:04,094
and we can look at the intermediate steps to see what it does.

65
00:05:04,095 --> 00:05:08,371
We can see once again that it thinks and it correctly realizes that it should use Wikipedia.

66
00:05:10,419 --> 00:05:14,682
It says action equal to Wikipedia and action input equal to Tom M. Mitchell.

67
00:05:15,962 --> 00:05:18,929
The observation that comes back in yellow this time,

68
00:05:19,100 --> 00:05:22,029
and we use different colors to denote different tools,

69
00:05:22,143 --> 00:05:28,267
is the Wikipedia summary result for the Tom M. Mitchell page.

70
00:05:28,267 --> 00:05:30,605
The observation that comes back from Wikipedia

71
00:05:30,657 --> 00:05:32,742
is actually two results, two pages,

72
00:05:32,743 --> 00:05:34,570
as there's two different Tom M. Mitchells.

73
00:05:34,570 --> 00:05:37,659
We can see the first one covers the computer scientist,

74
00:05:37,671 --> 00:05:41,253
and the second one, it looks like it's an Australian footballer.

75
00:05:42,938 --> 00:05:45,843
We can see that the information needed to answer this question,

76
00:05:45,844 --> 00:05:49,186
namely the name of the book that he wrote, Machine Learning,

77
00:05:49,386 --> 00:05:55,144
is present in the summary of the first Tom Mitchell.

78
00:05:55,144 --> 00:05:59,586
We can see next that the agent tries to look up more information about this book.

79
00:05:59,586 --> 00:06:03,088
So it looks up Machine Learning book in Wikipedia.

80
00:06:03,088 --> 00:06:04,635
This isn't strictly necessary,

81
00:06:04,643 --> 00:06:08,811
and it's an interesting example to show how agents aren't perfectly reliable yet.

82
00:06:09,968 --> 00:06:11,729
We can see that after this lookup,

83
00:06:11,800 --> 00:06:18,400
the agent recognizes that it has all the information it needs to answer and responds with the correct answer, machine learning.

84
00:06:19,471 --> 00:06:21,952
The next example we're going to go through is a really cool one.

85
00:06:21,952 --> 00:06:28,943
If you've seen things like copilot or even ChatGPT with the code interpreter plugin enabled,

86
00:06:29,100 --> 00:06:34,536
one of the things they're doing is they're using the language model to write code and then executing that code.

87
00:06:34,536 --> 00:06:35,997
We can do the same exact thing here.

88
00:06:37,380 --> 00:06:43,343
So we're going to create a Python agent and we're going to use the same LLM as before.

89
00:06:43,343 --> 00:06:46,764
And we're going to give it a tool, the PythonREPLTool.

90
00:06:46,764 --> 00:06:49,245
A REPL is basically a way to interact with code.

91
00:06:49,245 --> 00:06:51,766
You can think of it as a Jupyter Notebook.

92
00:06:51,766 --> 00:06:55,588
So the agent can execute code with this REPL.

93
00:06:55,588 --> 00:07:04,232
It will then run and then we'll get back some results and those results will be passed back into the agent so it can decide what to do next.

94
00:07:05,750 --> 00:07:12,593
The problem that we're going to have this agent solve is we're going to give it a list of names and then ask it to sort them.

95
00:07:12,593 --> 00:07:21,936
So you can see here we have a list of names, "Harrison Chase", "Lang Chain", "Elle Elem", "Geoff Fusion", "Trance Former", "Jen Ayai".

96
00:07:21,936 --> 00:07:28,959
And we're going to ask the agent to first sort these names by last name and then first name and then print the output.

97
00:07:28,959 --> 00:07:33,801
Importantly, we're asking it to print the output so that it can actually see what the result is.

98
00:07:35,187 --> 00:07:39,543
These printed statements are what's going to be fed back into the language model later on,

99
00:07:39,909 --> 00:07:45,194
so it can reason about the output of the code that it just ran.

100
00:07:45,195 --> 00:07:49,019
Let's give this a try.

101
00:07:49,019 --> 00:07:51,743
We can see that when we go into the AgentExecutor chain,

102
00:07:52,800 --> 00:07:56,345
it first realizes that it can use the sorted function to list the customers.

103
00:08:00,408 --> 00:08:02,356
It's using a different agent type under the hood,

104
00:08:02,357 --> 00:08:08,375
which is why you can see that the action and action input is actually formatted slightly differently.

105
00:08:08,375 --> 00:08:11,600
Here, the action that it takes is to use the Python REPL,

106
00:08:11,601 --> 00:08:15,370
and then the action input that you can see is code,

107
00:08:15,371 --> 00:08:19,686
where it first writes out customers equals this list, it then sorts the customers,

108
00:08:19,687 --> 00:08:21,971
and then it goes through this list and print it.

109
00:08:23,527 --> 00:08:26,870
You can see the agent thinks about what to do and realizes that it needs to write some code.

110
00:08:28,405 --> 00:08:33,447
The format that it's using of action and action input is actually slightly different than before.

111
00:08:33,447 --> 00:08:36,969
It's using a different agent type under the hood.

112
00:08:36,969 --> 00:08:39,350
For the action, it's going to use the Python REPL.

113
00:08:39,350 --> 00:08:42,291
And for the action input, it's going to have a bunch of code.

114
00:08:42,291 --> 00:08:48,094
And so if we look at what this code is doing, it's first creating a variable to list out these customer names.

115
00:08:48,094 --> 00:08:50,735
It's then sorting that and creating a new variable.

116
00:08:50,735 --> 00:08:56,298
And it's then iterating through that new variable and printing out each line, just like we asked it to.

117
00:08:57,408 --> 00:09:01,512
We can see that we get the observation back and this is a list of names.

118
00:09:01,512 --> 00:09:07,198
And then the agent realizes that it's done and it returns these names.

119
00:09:07,198 --> 00:09:09,000
We can see from the stuff that's printed out,

121
00:09:11,072 --> 00:09:16,267
but let's dig a little bit deeper and run this with "langchain.debug" set to "True".

122
00:09:19,210 --> 00:09:23,714
As this prints out all the levels, all the different chains that are going on,

123
00:09:23,715 --> 00:09:27,158
let's go through them and see what exactly is happening.

124
00:09:27,158 --> 00:09:29,680
So first, we start with the agent executor.

125
00:09:29,680 --> 00:09:31,362
This is the top level agent runner.

126
00:09:31,362 --> 00:09:33,500
And we can see that we have here our input,

127
00:09:33,501 --> 00:09:38,368
sort these customers by last name and then first name and print the output.

128
00:09:38,368 --> 00:09:41,011
From here, we call an LLMChain.

129
00:09:41,011 --> 00:09:43,533
This is the LLMChain that the agent is using.

130
00:09:44,576 --> 00:09:48,838
The LLMChain, remember, is a combination of prompt and an LLM.

131
00:09:48,838 --> 00:09:52,657
At this point, it's only got the input, an agent scratchpad,

132
00:09:52,658 --> 00:09:59,400
we'll get back to that later, and then some stop sequences to tell the language model when to stop doing its generations.

133
00:10:00,945 --> 00:10:05,447
At the next level, we see the exact call to the language model.

134
00:10:05,447 --> 00:10:07,432
We can see the fully formatted prompt,

135
00:10:07,457 --> 00:10:12,551
which includes instructions about what tools it has access to as well as how to format its output.

136
00:10:15,297 --> 00:10:19,260
From there, we can then see the exact output of the language model.

137
00:10:19,260 --> 00:10:28,128
So we can see the text key where it has the thought and the action and the action input all in one string.

138
00:10:28,128 --> 00:10:32,412
It then wraps up the LLMChain as it exits through there.

139
00:10:32,412 --> 00:10:34,294
And the next thing that it calls is a tool.

140
00:10:34,294 --> 00:10:37,296
And here we can see the exact input to the tool.

141
00:10:37,296 --> 00:10:39,629
We can also see the name of the tool, Python REPL,

142
00:10:39,658 --> 00:10:41,620
and then we can see the input, which is this code.

143
00:10:43,968 --> 00:10:46,889
We can then see the output of this tool, which is this printed out string.

144
00:10:46,889 --> 00:10:54,212
And again, this happens because we specifically ask the Python REPL to print out what is going on.

145
00:10:54,212 --> 00:11:00,615
We can then see the next input to the LLMChain, which again, the LLMChain here is the agent.

146
00:11:00,615 --> 00:11:04,877
So here, if you look at the variables, there's the input.

147
00:11:04,877 --> 00:11:05,657
This is unchanged.

148
00:11:05,657 --> 00:11:10,359
This is the high level objective that we're asking, but now there's some new values for agent scratchpad.

149
00:11:11,356 --> 00:11:19,461
You can see here that this is actually a combination of the previous generation plus the tool output.

150
00:11:19,461 --> 00:11:21,328
And so we're passing this back in

151
00:11:21,329 --> 00:11:24,186
so that the language model can understand what happened previously

152
00:11:24,187 --> 00:11:26,629
and use that to reason about what to do next.

153
00:11:28,067 --> 00:11:32,700
The next few print statements are covering what happens as the language model realizes that

154
00:11:32,701 --> 00:11:35,272
it is basically finished with its job.

155
00:11:35,272 --> 00:11:38,134
So we can see here the fully formatted prompt to the language model.

156
00:11:39,273 --> 00:11:44,885
the response where it realizes that it is done and it says final answer,

157
00:11:44,886 --> 00:11:50,380
which here is the sequence that the agent uses to recognize that it's done with its job.

158
00:11:50,380 --> 00:11:53,782
We can then see it exiting the LLMChain and then exiting the agent executor.

159
00:11:55,093 --> 00:12:00,014
This should hopefully give you a pretty good idea of what's going on under the hood inside these agents.

160
00:12:00,394 --> 00:12:04,029
This should hopefully give you a pretty good idea of what's going on under the hood

161
00:12:04,086 --> 00:12:11,657
and is hopefully instructive as you pause and put your own objectives for this coding agent to try to accomplish.

162
00:12:11,657 --> 00:12:16,951
This debug mode can also be used to highlight what's going wrong as shown above in the Wikipedia example,

163
00:12:16,952 --> 00:12:22,720
sometimes agents act a little funny and so having all this information is really helpful for understanding what's going on.

164
00:12:23,874 --> 00:12:28,575
So far, we've used tools that come to find in LangChain already.

165
00:12:28,575 --> 00:12:35,356
But a big power of agents is that you can connect it to your own sources of information, your own APIs, your own data.

166
00:12:35,356 --> 00:12:40,958
So here, we're going to go over how you can create a custom tool so that you can connect it to whatever you want.

167
00:12:40,958 --> 00:12:44,938
Let's make a tool that's going to tell us what the current date is.

168
00:12:44,938 --> 00:12:47,739
First, we're going to import this tool decorator.

169
00:12:47,739 --> 00:12:52,180
This can be applied to any function, and it turns it into a tool that LangChain can use.

170
00:12:53,425 --> 00:12:58,669
Next, we're going to write a function called time, which shakes in any text string.

171
00:12:58,669 --> 00:13:00,110
We're not really going to use that.

172
00:13:00,110 --> 00:13:05,794
And it's going to return today's date by calling datetime.

173
00:13:05,794 --> 00:13:10,858
In addition to the name of the function, we're also going to write a really detailed docstring.

174
00:13:10,858 --> 00:13:16,442
That's because this is what the agent will use to know when it should call this tool and how it should call this tool.

175
00:13:18,728 --> 00:13:22,249
For example, here we say that the input should always be an empty string.

176
00:13:22,249 --> 00:13:24,130
That's because we don't use it.

177
00:13:24,130 --> 00:13:27,443
If we have more stringent requirements on what the input should be,

178
00:13:27,586 --> 00:13:32,871
for example, if we have a function that should always take in a search query or a SQL statement,

179
00:13:32,914 --> 00:13:35,286
you'll want to make sure to mention that here.

180
00:13:37,573 --> 00:13:39,274
We're now going to create another agent.

181
00:13:39,274 --> 00:13:42,755
This time we're adding the time tool to the list of existing tools.

182
00:13:45,897 --> 00:13:53,960
And finally, let's call the agent and ask it what the date today is.

183
00:13:53,960 --> 00:13:58,461
It recognizes that it needs to use the time tool, which it specifies here.

184
00:13:58,461 --> 00:14:00,602
It has the action input as an empty string.

185
00:14:00,602 --> 00:14:01,162
This is great.

186
00:14:01,162 --> 00:14:02,803
This is what we told it to do.

187
00:14:02,803 --> 00:14:04,963
And then it returns with an observation.

188
00:14:04,963 --> 00:14:09,005
And then finally, the language model takes that observation and responds to the user.

189
00:14:09,005 --> 00:14:09,905
Today's date is 2023-05-21.

190
00:14:12,765 --> 00:14:17,050
You should pause the video here and try putting in different inputs.

191
00:14:17,050 --> 00:14:19,572
This wraps up the lesson on agents.

192
00:14:19,572 --> 00:14:25,038
This is one of the newer and more exciting and more experimental pieces of LangChain.

193
00:14:25,038 --> 00:14:26,820
So I hope you enjoy using it.

194
00:14:26,820 --> 00:14:32,156
Hopefully it showed you how you can use a language model as a reasoning engine to take different actions

195
00:14:32,157 --> 00:14:34,148
and connect to other functions and data sources.
