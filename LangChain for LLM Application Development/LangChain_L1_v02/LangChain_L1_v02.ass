[Script Info]

Title: LangChain_L1_v02
ScriptType: v4.00+
WrapStyle: 0
Collisions: Reverse
PlayResX: 384
PlayResY: 288
Timer: 100.0000
ScaledBorderAndShadow: no
Last Style Storage: Default
Video Aspect Ratio: 0
Video Zoom: 6
Video Position: 0

[V4+ Styles]
Format: Name,Fontname,Fontsize,PrimaryColour,SecondaryColour,OutlineColour,BackColour,Bold,Italic,Underline,StrikeOut,ScaleX,ScaleY,Spacing,Angle,BorderStyle,Outline,Shadow,Alignment,MarginL,MarginR,MarginV,Encoding
Style: Default,LXGW WenKai,16,&H000092FE,&H000000FF,&H00000000,&H00000000,-1,0,0,0,100,100,0,0,1,2,2,2,1,1,6,1
Style: Secondary,Helvetica,10,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,-1,0,0,0,100,100,0,0,1,2,2,2,1,1,6,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 0,0:00:05.0,0:00:08.80,Secondary,,0,0,0,,In lesson one, we'll be covering models, prompts, and parsers.
Dialogue: 0,0:00:08.80,0:00:13.60,Secondary,,0,0,0,,So models refers to the language models underpinning a lot of it.
Dialogue: 0,0:00:13.60,0:00:18.76,Secondary,,0,0,0,,Prompts refers to the style of creating inputs to pass into the models.
Dialogue: 0,0:00:18.76,0:00:20.76,Secondary,,0,0,0,,And then parsers is on the opposite end.
Dialogue: 0,0:00:20.76,0:00:27.76,Secondary,,0,0,0,,It involves taking the output of these models and parsing it into a more structured format so that you can do things downstream with it.
Dialogue: 0,0:00:27.76,0:00:32.64,Secondary,,0,0,0,,So when you build an application using an LLM, there'll often be reusable models.
Dialogue: 0,0:00:32.64,0:00:40.14,Secondary,,0,0,0,,We have repeatedly prompted model, parsers output, and so LangChain gives an easy set of abstractions to do this type of operation.
Dialogue: 0,0:00:40.14,0:00:45.16,Secondary,,0,0,0,,So with that, let's jump in and take a look at models, prompts, and parsers.
Dialogue: 0,0:00:45.16,0:00:47.64,Secondary,,0,0,0,,So to get started, here's a little bit of starter code.
Dialogue: 0,0:00:47.64,0:00:53.72,Secondary,,0,0,0,,I'm going to import OS, import OpenAI, and load my OpenAI secret key.
Dialogue: 0,0:00:53.72,0:00:58.4,Secondary,,0,0,0,,The OpenAI library is already installed in my Jupyter notebook environment.
Dialogue: 0,0:00:58.4,0:01:03.44,Secondary,,0,0,0,,If you're running this locally and you don't have OpenAI installed yet, you might need to run that.
Dialogue: 0,0:01:03.44,0:01:07.36,Secondary,,0,0,0,,"!pip install openai", but I'm not going to do that here.
Dialogue: 0,0:01:07.36,0:01:09.0,Secondary,,0,0,0,,And then here's a helper function.
Dialogue: 0,0:01:09.0,0:01:20.68,Secondary,,0,0,0,,This is actually very similar to the helper function that you might have seen in the "ChatGPT Prompt Engineering for Developers" course that I offered together with OpenAI's Isa Fulford.
Dialogue: 0,0:01:20.68,0:01:26.96,Secondary,,0,0,0,,And so with this helper function, you can say get_completion on what is one plus one,
Dialogue: 0,0:01:26.96,0:01:36.20,Secondary,,0,0,0,,and this will call ChatGPT or technically the model GPT 3.5 Turbo to give you an answer back like this.
Dialogue: 0,0:01:36.20,0:01:49.84,Secondary,,0,0,0,,Now to motivate the LangChain abstractions for model prompts and parsers, let's say you get an email from a customer in a language other than English.
Dialogue: 0,0:01:49.84,0:01:55.29,Secondary,,0,0,0,,In order to make sure this is accessible, the other language I'm going to use is the English pirate language,
Dialogue: 0,0:01:55.29,0:02:02.68,Secondary,,0,0,0,,where the content says, "I be fuming that me blender lid flew off and splattered my kitchen walls with smoothie.
Dialogue: 0,0:02:02.68,0:02:06.8,Secondary,,0,0,0,,And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen.
Dialogue: 0,0:02:06.8,0:02:08.60,Secondary,,0,0,0,,I need your help right now, matey."
Dialogue: 0,0:02:08.60,0:02:18.36,Secondary,,0,0,0,,And so what we will do is ask this LLM to translate the text to American English in a calm and respectful tone.
Dialogue: 0,0:02:18.36,0:02:23.40,Secondary,,0,0,0,,So I'm going to set style to American English in a calm and respectful tone.
Dialogue: 0,0:02:23.40,0:02:29.32,Secondary,,0,0,0,,And so in order to actually accomplish this, if you've seen a little bit of prompting before,
Dialogue: 0,0:02:29.32,0:02:32.99,Secondary,,0,0,0,,I'm going to specify the prompt using an "f" string with the instructions,
Dialogue: 0,0:02:32.99,0:02:40.16,Secondary,,0,0,0,,Translate the text that is delimited by triple backticks into style that is "style", and then plug in these two styles.
Dialogue: 0,0:02:40.16,0:02:46.12,Secondary,,0,0,0,,And so this generates a prompt that says, "Translate the text," and so on.
Dialogue: 0,0:02:46.12,0:02:54.80,Secondary,,0,0,0,,I encourage you to pause the video and run the code, and also try modifying the prompt to see if you can get a different output.
Dialogue: 0,0:02:54.80,0:03:03.8,Secondary,,0,0,0,,You can then prompt the large language model to get a response.
Dialogue: 0,0:03:03.8,0:03:04.8,Secondary,,0,0,0,,Let's see what the response is.
Dialogue: 0,0:03:04.8,0:03:07.87,Secondary,,0,0,0,,It says, "Translated the English pirate's message into this very polite,
Dialogue: 0,0:03:07.87,0:03:13.54,Secondary,,0,0,0,,'I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie,'" and so on.
Dialogue: 0,0:03:13.54,0:03:16.52,Secondary,,0,0,0,,I could really use your help right now, my friend.
Dialogue: 0,0:03:16.52,0:03:18.76,Secondary,,0,0,0,,That sounds very nice.
Dialogue: 0,0:03:18.76,0:03:27.27,Secondary,,0,0,0,,So if you have different customers writing reviews in different languages,  not just English pirate, but French, German, Japanese, and so on,
Dialogue: 0,0:03:27.27,0:03:34.50,Secondary,,0,0,0,,you can imagine having to generate a whole sequence of prompts to generate such translations.
Dialogue: 0,0:03:34.50,0:03:40.36,Secondary,,0,0,0,,Let's look at how we can do this in a more convenient way using LangChain.
Dialogue: 0,0:03:40.36,0:03:44.60,Secondary,,0,0,0,,I'm going to import ChatOpenAI.
Dialogue: 0,0:03:44.60,0:03:49.78,Secondary,,0,0,0,,This is LangChain's abstraction for the ChatGPT API endpoint.
Dialogue: 0,0:03:49.78,0:03:54.16,Secondary,,0,0,0,,And so if I then set chat equals ChatOpenAI and look at what chat is,
Dialogue: 0,0:03:54.16,0:04:02.96,Secondary,,0,0,0,,it creates this object as follows that uses the ChatGPT model, which is also called gpt-3.5-turbo.
Dialogue: 0,0:04:02.96,0:04:09.74,Secondary,,0,0,0,,When I'm building applications, one thing I will often do is set the temperature parameter to be equal to zero.
Dialogue: 0,0:04:09.74,0:04:20.40,Secondary,,0,0,0,,So the default temperature is 0.7, but let me actually redo that with temperature equals 0.0.
Dialogue: 0,0:04:20.40,0:04:27.96,Secondary,,0,0,0,,And now the temperature is set to zero to make this output a little bit less random.
Dialogue: 0,0:04:27.96,0:04:30.96,Secondary,,0,0,0,,And now let me define the template string as follows.
Dialogue: 0,0:04:30.96,0:04:35.28,Secondary,,0,0,0,,Translate the text delimited by triple backticks into style is style.
Dialogue: 0,0:04:35.28,0:04:37.48,Secondary,,0,0,0,,And then here's the text.
Dialogue: 0,0:04:37.48,0:04:47.4,Secondary,,0,0,0,,And to repeatedly reuse this template, let's import LangChain's ChatPromptTemplate.
Dialogue: 0,0:04:47.4,0:05:00.76,Secondary,,0,0,0,,And then let me create a prompt template using that template string that we just wrote above.
Dialogue: 0,0:05:00.76,0:05:04.86,Secondary,,0,0,0,,From the prompt template, you can actually extract the original prompt.
Dialogue: 0,0:05:04.86,0:05:17.4,Secondary,,0,0,0,,And it realizes that this prompt has two input variables, the style and the text, which were shown here with the curly braces.
Dialogue: 0,0:05:17.4,0:05:19.72,Secondary,,0,0,0,,And here is the original template as well that we had specified.
Dialogue: 0,0:05:19.72,0:05:28.20,Secondary,,0,0,0,,In fact, if I print this out, it realizes it has two input variables, style and text.
Dialogue: 0,0:05:28.20,0:05:30.36,Secondary,,0,0,0,,Now let's specify the style.
Dialogue: 0,0:05:30.36,0:05:33.96,Secondary,,0,0,0,,This is a style that I want the customer message to be translated to.
Dialogue: 0,0:05:33.96,0:05:37.24,Secondary,,0,0,0,,So I'm going to call this customer style.
Dialogue: 0,0:05:37.24,0:05:44.96,Secondary,,0,0,0,,And here's my same customer email as before.
Dialogue: 0,0:05:44.96,0:05:59.64,Secondary,,0,0,0,,And now if I create customer messages, this will generate the prompt and will pass this a large language model in a minute to get a response.
Dialogue: 0,0:05:59.64,0:06:05.20,Secondary,,0,0,0,,So if you want to look at the types, the customer message is actually a list.
Dialogue: 0,0:06:05.20,0:06:16.40,Secondary,,0,0,0,,And if you look at the first element of the list, this is more or less that prompt that you would expect this to be creating.
Dialogue: 0,0:06:16.40,0:06:20.56,Secondary,,0,0,0,,Lastly, let's pass this prompt to the LLM.
Dialogue: 0,0:06:20.56,0:06:32.56,Secondary,,0,0,0,,So I'm going to call chat, which we had set earlier as a reference to the OpenAI ChatGPT endpoint.
Dialogue: 0,0:06:32.56,0:06:45.40,Secondary,,0,0,0,,If we print out the customer responses content, then it gives you back this text translated from English pirate to polite American English.
Dialogue: 0,0:06:45.40,0:06:50.56,Secondary,,0,0,0,,And of course, you can imagine other use cases where the customer emails are in other languages.
Dialogue: 0,0:06:50.56,0:06:58.24,Secondary,,0,0,0,,And this too can be used to translate the messages for an English speaking to understand and reply to.
Dialogue: 0,0:06:58.24,0:07:06.72,Secondary,,0,0,0,,I encourage you to pause the video and run the code and also try modifying the prompt to see if you can get a different output.
Dialogue: 0,0:07:06.72,0:07:11.88,Secondary,,0,0,0,,Now let's hope our customer service agent replied to the customer in their original language.
Dialogue: 0,0:07:11.88,0:07:17.4,Secondary,,0,0,0,,So let's say English speaking customer service agent writes this and says, "Hey there customer,
Dialogue: 0,0:07:17.4,0:07:20.40,Secondary,,0,0,0,,warranty does not cover, clean expenses for the kitchen because it's your fault.
Dialogue: 0,0:07:20.40,0:07:23.80,Secondary,,0,0,0,,You misused your blender by forgetting to put on the lid.
Dialogue: 0,0:07:23.80,0:07:25.80,Secondary,,0,0,0,,Tough luck. See ya."
Dialogue: 0,0:07:25.80,0:07:32.80,Secondary,,0,0,0,,Not a very polite message, but let's say this is what a customer service agent wants.
Dialogue: 0,0:07:32.80,0:07:39.60,Secondary,,0,0,0,,We are going to specify that the service message is going to be translated to this pirate style.
Dialogue: 0,0:07:39.60,0:07:45.52,Secondary,,0,0,0,,So we want it to be in a polite tone that speaks in English pirate.
Dialogue: 0,0:07:45.52,0:07:48.27,Secondary,,0,0,0,,And because we previously created that prompt template,
Dialogue: 0,0:07:48.27,0:07:58.92,Secondary,,0,0,0,,the cool thing is we can now reuse that prompt template and specify that the output style we want is this service style pirate and the text is this service reply.
Dialogue: 0,0:07:58.92,0:08:07.8,Secondary,,0,0,0,,And if we do that, that's the prompt.
Dialogue: 0,0:08:07.8,0:08:12.44,Secondary,,0,0,0,,And if we prompt on ChatGPT, this is the response it gives us back.
Dialogue: 0,0:08:12.44,0:08:20.44,Secondary,,0,0,0,,"Ahoy there, matey. I must kindly inform you that the warranty be not covering the expenses or cleaning your galley."
Dialogue: 0,0:08:21.44,0:08:23.92,Secondary,,0,0,0,,"Aye, tough luck. Farewell, me hearty."
Dialogue: 0,0:08:23.92,0:08:29.20,Secondary,,0,0,0,,So you might be wondering why are we using prompt templates instead of, you know, just an F string?
Dialogue: 0,0:08:29.20,0:08:35.84,Secondary,,0,0,0,,The answer is that as you build sophisticated applications, prompts can be quite long and detailed.
Dialogue: 0,0:08:35.84,0:08:43.36,Secondary,,0,0,0,,And so prompt templates are a useful abstraction to help you reuse good prompts when you can.
Dialogue: 0,0:08:43.36,0:08:52.48,Secondary,,0,0,0,,This is an example of a relatively long prompt to grade a student's submission for an online learning application.
Dialogue: 0,0:08:52.48,0:09:03.8,Secondary,,0,0,0,,And a prompt like this can be quite long in which you can ask the LLM to first solve the problem and then have the output in a certain format and output in a certain format.
Dialogue: 0,0:09:03.8,0:09:09.44,Secondary,,0,0,0,,And wrapping this in a LangChain prompt makes it easier to reuse a prompt like this.
Dialogue: 0,0:09:09.44,0:09:22.58,Secondary,,0,0,0,,So you see later that LangChain provides prompts for some common operations such as summarization or question answering or connecting to SQL databases or connecting to different APIs.
Dialogue: 0,0:09:22.58,0:09:32.30,Secondary,,0,0,0,,And so by using some of LangChain's built-in prompts, you can quickly get an application working without needing to engineer your own prompts.
Dialogue: 0,0:09:32.30,0:09:43.34,Secondary,,0,0,0,,One other aspect of LangChain's prompt libraries is that it also supports output parsing, which we'll get to in a minute.
Dialogue: 0,0:09:43.34,0:09:55.60,Secondary,,0,0,0,,But when you're building a complex application using an LLM, you often instruct the LLM to generate its output in a certain format, such as using specific keywords.
Dialogue: 0,0:09:55.60,0:10:06.54,Secondary,,0,0,0,,This example on the left illustrates using an LLM to carry out something called chain of thought reasoning using a framework called the ReAct framework.
Dialogue: 0,0:10:06.54,0:10:09.38,Secondary,,0,0,0,,But don't worry about the technical details.
Dialogue: 0,0:10:09.38,0:10:21.66,Secondary,,0,0,0,,But the keys of that is that the thought is what the LLM is thinking because by giving an LLM space to think, it can often get to more accurate conclusions.
Dialogue: 0,0:10:21.66,0:10:31.70,Secondary,,0,0,0,,Then action as a keyword to carry the specific action, and then observation to show what it learned from that action, and so on.
Dialogue: 0,0:10:31.70,0:10:39.60,Secondary,,0,0,0,,And if you have a prompt that instructs the LLM to use these specific keywords, thought, action, and observation,
Dialogue: 0,0:10:39.60,0:10:48.14,Secondary,,0,0,0,,then this prompt can be coupled with a parser to extract out the text that has been tagged with these specific keywords.
Dialogue: 0,0:10:48.14,0:11:01.46,Secondary,,0,0,0,,And so that together gives a very nice abstraction to specify the input to an LLM, and then also have a parser correctly interpret the output that the LLM gives.
Dialogue: 0,0:11:01.46,0:11:09.14,Secondary,,0,0,0,,And so with that, let's return to see an example of an output parser using LangChain.
Dialogue: 0,0:11:09.14,0:11:17.70,Secondary,,0,0,0,,In this example, let's take a look at how you can have an LLM output JSON and use LangChain to parse that output.
Dialogue: 0,0:11:17.70,0:11:29.24,Secondary,,0,0,0,,And the running example that I'll use will be to extract information from a product review and format that output in a JSON format.
Dialogue: 0,0:11:29.24,0:11:33.78,Secondary,,0,0,0,,So here's an example of how you would like the output to be formatted.
Dialogue: 0,0:11:33.78,0:11:45.14,Secondary,,0,0,0,,Technically, this is a Python dictionary where whether or not the product is a gift, maps the falls, the number of days it took to deliver, was five, and the price value was pretty affordable.
Dialogue: 0,0:11:45.14,0:11:48.86,Secondary,,0,0,0,,So this is one example of a desired output.
Dialogue: 0,0:11:48.86,0:11:57.58,Secondary,,0,0,0,,Here is an example of customer review as well as a template to try to get to that JSON output.
Dialogue: 0,0:11:57.58,0:11:58.58,Secondary,,0,0,0,,So here's a customer review.
Dialogue: 0,0:11:58.58,0:12:00.54,Secondary,,0,0,0,,It says, "This leaf blower is pretty amazing.
Dialogue: 0,0:12:00.54,0:12:05.26,Secondary,,0,0,0,,It has four settings, candle blower, gentle breeze, windy city, and tornado.
Dialogue: 0,0:12:05.26,0:12:08.62,Secondary,,0,0,0,,It arrived in two days, just in time for my wife's anniversary present.
Dialogue: 0,0:12:08.62,0:12:11.10,Secondary,,0,0,0,,I think my wife liked it so much she was speechless.
Dialogue: 0,0:12:11.10,0:12:14.34,Secondary,,0,0,0,,So far I've been the only one using it," and so on.
Dialogue: 0,0:12:14.34,0:12:15.62,Secondary,,0,0,0,,And here's a review_template.
Dialogue: 0,0:12:15.62,0:12:20.26,Secondary,,0,0,0,,For the following text, extract the following information, specify was this a gift
Dialogue: 0,0:12:20.26,0:12:24.18,Secondary,,0,0,0,,So in this case, it would be yes because this is a gift
Dialogue: 0,0:12:24.18,0:12:26.94,Secondary,,0,0,0,,And also delivery days, how long did it take to deliver.
Dialogue: 0,0:12:26.94,0:12:30.10,Secondary,,0,0,0,,It looks like in this case, it arrived in two days.
Dialogue: 0,0:12:30.10,0:12:32.14,Secondary,,0,0,0,,And what's the price value?
Dialogue: 0,0:12:32.14,0:12:36.22,Secondary,,0,0,0,,You know, slightly more expensive than the leaf b'l'o'w'ers, and so on.
Dialogue: 0,0:12:36.22,0:12:52.46,Secondary,,0,0,0,,So the review_template asks the LLM to take as input a customer review and extract these three fields, and then format the output as JSON, um, with the following keys.
Dialogue: 0,0:12:55.46,0:12:57.34,Secondary,,0,0,0,,All right.
Dialogue: 0,0:12:57.34,0:13:01.38,Secondary,,0,0,0,,So here's how you can wrap this in LongChain.
Dialogue: 0,0:13:01.38,0:13:03.2,Secondary,,0,0,0,,Let's import the ChatPrompt template.
Dialogue: 0,0:13:03.2,0:13:04.88,Secondary,,0,0,0,,We'd actually imported this already earlier.
Dialogue: 0,0:13:04.88,0:13:16.58,Secondary,,0,0,0,,So technically, this line is redundant, but I'll just import it again and then have the prompt templates, um, created from the review_template up on top.
Dialogue: 0,0:13:16.58,0:13:20.70,Secondary,,0,0,0,,And so here's the prompt template.
Dialogue: 0,0:13:20.70,0:13:33.2,Secondary,,0,0,0,,And now, similar to our early usage of a prompt template, let's create the messages to pass to the OpenAI, uh, endpoint.
Dialogue: 0,0:13:33.2,0:13:39.42,Secondary,,0,0,0,,Create the OpenAI endpoint, call that endpoint, and then let's print out the response.
Dialogue: 0,0:13:39.42,0:13:45.70,Secondary,,0,0,0,,I encourage you to pause the video and run the code.
Dialogue: 0,0:13:45.70,0:13:46.70,Secondary,,0,0,0,,And there it is.
Dialogue: 0,0:13:46.70,0:13:52.94,Secondary,,0,0,0,,It says, "Gift is true, delivery days is two," and the price value also looks pretty accurate.
Dialogue: 0,0:13:52.94,0:14:04.22,Secondary,,0,0,0,,Um, but note that if we check the type of the response, this is actually a string.
Dialogue: 0,0:14:04.22,0:14:09.58,Secondary,,0,0,0,,So it looks like JSON and looks like it has key value pairs, but it's actually not a dictionary.
Dialogue: 0,0:14:09.58,0:14:12.22,Secondary,,0,0,0,,This is just one long string.
Dialogue: 0,0:14:12.22,0:14:17.42,Secondary,,0,0,0,,So what I'd really like to do is go to the response content and get the value from the gift key, which should be true.
Dialogue: 0,0:14:17.42,0:14:22.2,Secondary,,0,0,0,,But I run this, this should generate an error because, well, this is actually a string.
Dialogue: 0,0:14:22.2,0:14:24.82,Secondary,,0,0,0,,This is not a Python dictionary.
Dialogue: 0,0:14:24.82,0:14:31.86,Secondary,,0,0,0,,So let's see how we would use LangChain's, um, parser in order to do this.
Dialogue: 0,0:14:31.86,0:14:40.14,Secondary,,0,0,0,,I'm going to import ResponseSchema and StructuredOutputParser from LangChain.
Dialogue: 0,0:14:40.14,0:14:46.48,Secondary,,0,0,0,,And I'm going to tell it what I want it to parse by specifying these response schemas.
Dialogue: 0,0:14:46.48,0:14:50.34,Secondary,,0,0,0,,So the gift schema is named gift, and here's the description.
Dialogue: 0,0:14:50.34,0:14:52.42,Secondary,,0,0,0,,Does the item purchase a gift for someone else?
Dialogue: 0,0:14:52.42,0:14:56.54,Secondary,,0,0,0,,Uh, answer true or yes, false if not or unknown, and so on.
Dialogue: 0,0:14:56.54,0:15:05.82,Secondary,,0,0,0,,So have a gift schema, delivery day schema, price value schema, and then let's put all three of them into a list as follows.
Dialogue: 0,0:15:05.82,0:15:20.38,Secondary,,0,0,0,,Now that I've specified the schema for these, um, LangChain can actually give you the prompt itself by having the output parser tell you what instructions it wants you to send to the LLM.
Dialogue: 0,0:15:20.38,0:15:34.98,Secondary,,0,0,0,,So if I were to print format instructions, she has a pretty precise set of instructions for the LLM that will cause it to generate an output that the output parser can process.
Dialogue: 0,0:15:34.98,0:15:44.6,Secondary,,0,0,0,,So here's the new review_template, and the review_template includes the format instructions that LangChain generated,
Dialogue: 0,0:15:44.6,0:15:58.70,Secondary,,0,0,0,,and so it can create a prompt from the review_template too, and then create the messages that will pass to the OpenAI endpoint.
Dialogue: 0,0:15:58.70,0:16:16.18,Secondary,,0,0,0,,If you want, you can take a look at the actual prompt, which gives you instructions to extract the fields, gift, delivery days, price value, here's the text, and then here are the formatting instructions.
Dialogue: 0,0:16:16.18,0:16:30.26,Secondary,,0,0,0,,Finally, if we call the OpenAI endpoint, let's take a look at what response we got.
Dialogue: 0,0:16:30.26,0:16:41.59,Secondary,,0,0,0,,It is now this, and now if we use the output_parser that we created earlier,
Dialogue: 0,0:16:41.60,0:16:48.84,Secondary,,0,0,0,,you can then parse this into an output dictionary, which if I print looks like this,
Dialogue: 0,0:16:48.84,0:16:56.59,Secondary,,0,0,0,,and notice that this is of type dictionary, not a string,
Dialogue: 0,0:16:56.59,0:17:09.2,Secondary,,0,0,0,,which is why I can now extract the value associated with the key gift and get true, or the value associated with delivery days and get two,
Dialogue: 0,0:17:09.2,0:17:14.76,Secondary,,0,0,0,,or you can also extract the value associated with price value.
Dialogue: 0,0:17:14.76,0:17:24.80,Secondary,,0,0,0,,So this is a nifty way to take your LLM output and parse it into a Python dictionary to make the output easier to use in downstream processing.
Dialogue: 0,0:17:24.80,0:17:29.20,Secondary,,0,0,0,,I encourage you to pause the video and run the code.
Dialogue: 0,0:17:29.20,0:17:32.78,Secondary,,0,0,0,,And so that's it for models, prompts, and parsers.
Dialogue: 0,0:17:32.78,0:17:37.96,Secondary,,0,0,0,,With these tools, hopefully you'll be able to reuse your own prompt templates easily,
Dialogue: 0,0:17:37.96,0:17:42.28,Secondary,,0,0,0,,share prompt templates with others that you're collaborating with, even use LangChain's built-in
Dialogue: 0,0:17:42.28,0:17:51.98,Secondary,,0,0,0,,prompt templates, which as you just saw, can often be coupled with an output_parser, so that the input prompt to output in a specific format
Dialogue: 0,0:17:51.98,0:18:02.28,Secondary,,0,0,0,,and then the parser parses that output to store the data in a Python dictionary or some other data structure that makes it easy for downstream processing.
Dialogue: 0,0:18:02.28,0:18:06.40,Secondary,,0,0,0,,I hope you find this useful in many of your applications.
Dialogue: 0,0:18:06.40,0:18:08.30,Secondary,,0,0,0,,And with that, let's go into the next video
Dialogue: 0,0:18:08.30,0:18:16.56,Secondary,,0,0,0,,where we'll see how LangChain can help you build better chatbots or have an LLM have more effective chats
Dialogue: 0,0:18:16.56,0:18:21.24,Secondary,,0,0,0,,by better managing what it remembers from the conversation you've had so far.
Dialogue: 0,0:00:05.0,0:00:08.80,Default,,0,0,0,,在第一课中，我们将\N介绍模型、提示和解析器。
Dialogue: 0,0:00:08.80,0:00:13.60,Default,,0,0,0,,所谓模型，是指\N作为基础的语言模型。
Dialogue: 0,0:00:13.60,0:00:18.76,Default,,0,0,0,,提示（Prompt）指创建\N输入，是用来给模型\N传递信息的一种方式。
Dialogue: 0,0:00:18.76,0:00:20.76,Default,,0,0,0,,而解析器则与之相反，
Dialogue: 0,0:00:20.76,0:00:27.76,Default,,0,0,0,,它接收模型的输出，并将输出\N结果解析成更结构化的格式，\N以便你可以对其进行后续操作。
Dialogue: 0,0:00:27.76,0:00:32.64,Default,,0,0,0,,因此，当您使用LLM构建应用程序\N时，通常会有可重用的模块。
Dialogue: 0,0:00:32.64,0:00:40.14,Default,,0,0,0,,我们反复提示模型，解析\N输出，所以LangChain提供了\N一套简单的抽象来执行此类操作。
Dialogue: 0,0:00:40.14,0:00:45.16,Default,,0,0,0,,那么，让我们开始\N研究模型、提示和解析器。
Dialogue: 0,0:00:45.16,0:00:47.64,Default,,0,0,0,,首先，这里有一些入门代码。
Dialogue: 0,0:00:47.64,0:00:53.72,Default,,0,0,0,,我将导入"os"库，导入"openai"库，\N并加载我的OpenAI密钥。
Dialogue: 0,0:00:53.72,0:00:58.4,Default,,0,0,0,,"openai"库已经安装在我的\NJupyter Notebook环境中。
Dialogue: 0,0:00:58.4,0:01:03.44,Default,,0,0,0,,如果你在本地运行，还没有\N安装"openai"，你可能需要运行它。
Dialogue: 0,0:01:03.44,0:01:07.36,Default,,0,0,0,,虽然可以用"!pip install openai"，\N但这里我不会这么做。
Dialogue: 0,0:01:07.36,0:01:09.0,Default,,0,0,0,,然后这里有一个辅助函数。
Dialogue: 0,0:01:09.0,0:01:20.68,Default,,0,0,0,,这个辅助函数实际上与我和OpenAI的Isa \NFulford一起讲过的《给开发者的ChatGPT\N提示工程》课程中的辅助函数非常相似。
Dialogue: 0,0:01:20.68,0:01:26.96,Default,,0,0,0,,有了这个辅助函数，\N你可以对“1+1是多少”这样的\N问题使用get_completion。
Dialogue: 0,0:01:26.96,0:01:36.20,Default,,0,0,0,,这将调用ChatGPT，或者\N更准确地说是GPT 3.5 Turbo\N模型，给你一个像这样的答案。
Dialogue: 0,0:01:36.20,0:01:49.84,Default,,0,0,0,,现在，为了阐明LangChain对模型\N提示和解析的抽象，假设你收到了\N一封用非英语写的客户邮件。
Dialogue: 0,0:01:49.84,0:01:55.29,Default,,0,0,0,,为了确保这是可接受的，我将\N使用另一种语言，海盗风格的英语。
Dialogue: 0,0:01:55.29,0:02:02.68,Default,,0,0,0,,邮件内容是：“我简直被气得七窍生烟，\N因为我的搅拌机盖子飞了出去，\N把我的厨房墙上都溅满了果汁。”
Dialogue: 0,0:02:02.68,0:02:06.8,Default,,0,0,0,,更糟糕的是，保修不包括清理厨房的费用。
Dialogue: 0,0:02:06.8,0:02:08.60,Default,,0,0,0,,我现在需要你的帮助，伙计。
Dialogue: 0,0:02:08.60,0:02:18.36,Default,,0,0,0,,我们要做的是让这个LLM把\N文本翻译成平静、尊重的美式英语。
Dialogue: 0,0:02:18.36,0:02:23.40,Default,,0,0,0,,所以我要把风格设为平\N静、尊重的美式英语。
Dialogue: 0,0:02:23.40,0:02:29.32,Default,,0,0,0,,为了实现这个目标，如果\N你之前看过一些提示相关的课程，
Dialogue: 0,0:02:29.32,0:02:32.99,Default,,0,0,0,,我会用"f"字符串和说明来指定提示，
Dialogue: 0,0:02:32.99,0:02:40.16,Default,,0,0,0,,把用三个反引号括起来的文本翻译成\N"style"那样的风格，然后插入这两种风格。
Dialogue: 0,0:02:40.16,0:02:46.12,Default,,0,0,0,,这样就生成了一个提示说：\N“翻译这段文本”等等。
Dialogue: 0,0:02:46.12,0:02:54.80,Default,,0,0,0,,我建议你暂停视频，运行代码，还可以\N尝试修改提示，看看能否得到不同的输出。
Dialogue: 0,0:02:54.80,0:03:03.8,Default,,0,0,0,,然后你可以提示大语言\N模型得到一个结果。
Dialogue: 0,0:03:03.8,0:03:04.8,Default,,0,0,0,,我们来看看返回的结果是什么。
Dialogue: 0,0:03:04.8,0:03:07.87,Default,,0,0,0,,它说把海盗风格英语的\N信息翻译成了这段礼貌的英语文字：
Dialogue: 0,0:03:07.87,0:03:13.54,Default,,0,0,0,,“我真的很伤心，因为我的搅拌机盖子\N飞了出去，把我的厨房墙上弄得一团糟…
Dialogue: 0,0:03:13.54,0:03:16.52,Default,,0,0,0,,我现在真的需要你的帮助，朋友！”
Dialogue: 0,0:03:16.52,0:03:18.76,Default,,0,0,0,,听起来很不错。
Dialogue: 0,0:03:18.76,0:03:27.27,Default,,0,0,0,,如果现在你有不同的客户，\N用不同的语言写评论，不仅仅是\N海盗风格英语，还有法语、德语、日语等等，
Dialogue: 0,0:03:27.27,0:03:34.50,Default,,0,0,0,,你可以想象，这需要生成一整套针对\N各种不同语言的提示来生成这样的翻译。
Dialogue: 0,0:03:34.50,0:03:40.36,Default,,0,0,0,,让我们看看如何用\NLangChain更方便地实现这一点。
Dialogue: 0,0:03:40.36,0:03:44.60,Default,,0,0,0,,我要导入ChatOpenAI。
Dialogue: 0,0:03:44.60,0:03:49.78,Default,,0,0,0,,这是LangChain对\NChatGPT API访问的抽象。
Dialogue: 0,0:03:49.78,0:03:54.16,Default,,0,0,0,,如果我把"chat"设置为ChatOpenAI的实例，\N然后看看"chat"是什么，
Dialogue: 0,0:03:54.16,0:04:02.96,Default,,0,0,0,,它会创建一个如下的对象，使用的\N是ChatGPT模型，也叫做gpt-3.5-turbo。
Dialogue: 0,0:04:02.96,0:04:09.74,Default,,0,0,0,,当我构建应用程序时，\N我经常会把temperature参数设置为0。
Dialogue: 0,0:04:09.74,0:04:20.40,Default,,0,0,0,,默认的temperature是0.7，但让我\N实际上用temperature等于0.0重做一遍。
Dialogue: 0,0:04:20.40,0:04:27.96,Default,,0,0,0,,现在temperature设为0，\N让输出的随机性降低一点。
Dialogue: 0,0:04:27.96,0:04:30.96,Default,,0,0,0,,现在让我定义一个模板字符串。
Dialogue: 0,0:04:30.96,0:04:35.28,Default,,0,0,0,,把用三个反引号分隔的\N文本翻译成"style"的风格。
Dialogue: 0,0:04:35.28,0:04:37.48,Default,,0,0,0,,然后是这里的文本。
Dialogue: 0,0:04:37.48,0:04:47.4,Default,,0,0,0,,为了反复使用这个模板，让我们\N导入LangChain的ChatPromptTemplate。
Dialogue: 0,0:04:47.4,0:05:00.76,Default,,0,0,0,,然后让我用我们刚才写的\N模板字符串创建一个提示模板。
Dialogue: 0,0:05:00.76,0:05:04.86,Default,,0,0,0,,从提示模板中，你可以提取原始的提示。
Dialogue: 0,0:05:04.86,0:05:17.4,Default,,0,0,0,,它意识到这个提示有两个输入变量，\N"style"和"text"，这里用大括号表示。
Dialogue: 0,0:05:17.4,0:05:19.72,Default,,0,0,0,,这里也有我们之前指定的原始模板。
Dialogue: 0,0:05:19.72,0:05:28.20,Default,,0,0,0,,实际上，如果我把这个打印出来，它知道\N它有两个输入变量，"style"和"text"。
Dialogue: 0,0:05:28.20,0:05:30.36,Default,,0,0,0,,现在让我们指定风格。
Dialogue: 0,0:05:30.36,0:05:33.96,Default,,0,0,0,,这是我希望客户信息被翻译成的风格。
Dialogue: 0,0:05:33.96,0:05:37.24,Default,,0,0,0,,所以我要把这个叫做customer_style。
Dialogue: 0,0:05:37.24,0:05:44.96,Default,,0,0,0,,这是我之前的相同客户邮件。
Dialogue: 0,0:05:44.96,0:05:59.64,Default,,0,0,0,,现在如果我创建customer_messages，\N这将生成提示，并在一分钟内\N传递给大型语言模型以获得返回结果。
Dialogue: 0,0:05:59.64,0:06:05.20,Default,,0,0,0,,所以如果你想看看类型，\Ncustomer_messages实际上是一个列表。
Dialogue: 0,0:06:05.20,0:06:16.40,Default,,0,0,0,,如果你看列表的第一个元素，\N这基本上就是你期望得到的提示。
Dialogue: 0,0:06:16.40,0:06:20.56,Default,,0,0,0,,最后，让我们把这个提示传给LLM。
Dialogue: 0,0:06:20.56,0:06:32.56,Default,,0,0,0,,接下来我要调用"chat"，我们之前\N把它设置为OpenAI ChatGPT的实例引用。
Dialogue: 0,0:06:32.56,0:06:45.40,Default,,0,0,0,,如果我们打印出客户回复\N的内容，那么它会把这段\N从海盗式英语翻译成礼貌的美式英语。
Dialogue: 0,0:06:45.40,0:06:50.56,Default,,0,0,0,,当然，你可以想象其他情况，\N比如客户邮件是用其他语言写的。
Dialogue: 0,0:06:50.56,0:06:58.24,Default,,0,0,0,,这也可以用来为英语使用者\N翻译消息，以便他们理解并回复。
Dialogue: 0,0:06:58.24,0:07:06.72,Default,,0,0,0,,我建议你暂停视频，运行代码，\N尝试修改提示，看看能否得到不同的输出。
Dialogue: 0,0:07:06.72,0:07:11.88,Default,,0,0,0,,现在让我们希望客服代表\N用客户的原始语言回复客户。
Dialogue: 0,0:07:11.88,0:07:17.4,Default,,0,0,0,,假设英语客服代表写了\N这个并说：“嘿，亲爱的客户，
Dialogue: 0,0:07:17.4,0:07:20.40,Default,,0,0,0,,保修不包括厨房的\N清洁费用，因为这是你的错。
Dialogue: 0,0:07:20.40,0:07:23.80,Default,,0,0,0,,你在使用搅拌机时忘了盖上盖子。
Dialogue: 0,0:07:23.80,0:07:25.80,Default,,0,0,0,,真倒霉，再见。”
Dialogue: 0,0:07:25.80,0:07:32.80,Default,,0,0,0,,这个回复不太礼貌，但假\N设这是客服想说的。
Dialogue: 0,0:07:32.80,0:07:39.60,Default,,0,0,0,,我们要把这个服务信息\N翻译成海盗式英语风格。
Dialogue: 0,0:07:39.60,0:07:45.52,Default,,0,0,0,,所以我们希望它是一种\N礼貌的海盗式英语语调。
Dialogue: 0,0:07:45.52,0:07:48.27,Default,,0,0,0,,因为我们之前创建了那个提示模板，
Dialogue: 0,0:07:48.27,0:07:58.92,Default,,0,0,0,,很酷的是，我们现在可以重复使用那个提示\N模板，并指定我们想要的输出风格是这种客服\N风格的海盗式英语，文本是这个服务回复。
Dialogue: 0,0:07:58.92,0:08:07.8,Default,,0,0,0,,如果我们这样做，那就是提示。
Dialogue: 0,0:08:07.8,0:08:12.44,Default,,0,0,0,,如果我们在ChatGPT上提示，\N这就是它给我们的回应。
Dialogue: 0,0:08:12.44,0:08:21.44,Default,,0,0,0,,“啊，伙计，我必须友善地告诉你，\N保修不包括费用或清洁你的厨房。”等等。
Dialogue: 0,0:08:21.44,0:08:23.92,Default,,0,0,0,,“哎，运气不好。再见，伙计。”
Dialogue: 0,0:08:23.92,0:08:29.20,Default,,0,0,0,,那么你可能会想，为什么我们\N要用提示模板而不是"f"字符串呢？
Dialogue: 0,0:08:29.20,0:08:35.84,Default,,0,0,0,,答案是，当你构建复杂的\N应用程序时，提示可能会很长且详细。
Dialogue: 0,0:08:35.84,0:08:43.36,Default,,0,0,0,,因此，提示模板是一种有用的抽象，\N可以帮助你在需要时重用好的提示。
Dialogue: 0,0:08:43.36,0:08:52.48,Default,,0,0,0,,这是一个相对较长的示例，用于为\N在线学习应用程序的学生提交作品评分。
Dialogue: 0,0:08:52.48,0:09:03.8,Default,,0,0,0,,像这样的提示可能会很长，你可以\N要求LLM先解决问题，然后以某种格式输出。
Dialogue: 0,0:09:03.8,0:09:09.44,Default,,0,0,0,,将这个提示包装在LangChain的\N提示中，可以更容易地重用这样的提示。
Dialogue: 0,0:09:09.44,0:09:22.58,Default,,0,0,0,,你会看到，LangChain为一些\N常见操作提供了提示，如摘要、回答问题、\N连接到SQL数据库或连接到不同的API。
Dialogue: 0,0:09:22.58,0:09:32.30,Default,,0,0,0,,通过使用LangChain的内置提示，你可以快速\N地使应用程序运行，而无需设计自己的提示。
Dialogue: 0,0:09:32.30,0:09:43.34,Default,,0,0,0,,LangChain的提示库的另一个方面\N是它还支持输出解析，我们稍后会讲到。
Dialogue: 0,0:09:43.34,0:09:55.60,Default,,0,0,0,,但是当你使用LLM构建一个复杂的\N应用程序时，你通常会指示LLM以\N某种格式生成输出，比如使用特定的关键词。
Dialogue: 0,0:09:55.60,0:10:06.54,Default,,0,0,0,,左边的这个例子展示了如何\N使用一个叫做"ReAct"的框架，\N让LLM执行一种叫做思维链推理的操作。
Dialogue: 0,0:10:06.54,0:10:09.38,Default,,0,0,0,,但不用担心技术细节。
Dialogue: 0,0:10:09.38,0:10:21.66,Default,,0,0,0,,关键是这个"Thought"就是LLM在\N思考的内容，因为给LLM留出思考\N的空间，它往往能得出更准确的结论。
Dialogue: 0,0:10:21.66,0:10:31.70,Default,,0,0,0,,然后用"Action"这个关键词\N来执行特定的动作，接着用"Observation"\N来展示它从这个动作中观察到了什么，等等。
Dialogue: 0,0:10:31.70,0:10:39.60,Default,,0,0,0,,如果你有一个提示让LLM\N使用这些特定的关键词：\N"Thought"、"Action"和"Observation"，
Dialogue: 0,0:10:39.60,0:10:48.14,Default,,0,0,0,,那么这个提示可以与一个解析器结合，\N提取出用这些特定关键词标记的文本。
Dialogue: 0,0:10:48.14,0:11:01.46,Default,,0,0,0,,这样一来，就可以很好地\N抽象地指定LLM的输入，然后\N让解析器正确地解释LLM给出的输出。
Dialogue: 0,0:11:01.46,0:11:09.14,Default,,0,0,0,,有了这个，让我们回到一个使用\NLangChain的输出解析器的例子。
Dialogue: 0,0:11:09.14,0:11:17.70,Default,,0,0,0,,在这个例子中，让我们看看如何让LLM\N输出JSON，并使用LangChain解析这个输出。
Dialogue: 0,0:11:17.70,0:11:29.24,Default,,0,0,0,,我将使用的运行示例是从产品评论中\N提取信息，并将输出格式化为JSON格式。
Dialogue: 0,0:11:29.24,0:11:33.78,Default,,0,0,0,,这是一个输出格式的示例。
Dialogue: 0,0:11:33.78,0:11:45.14,Default,,0,0,0,,从技术上讲，这是一个Python字典，其中\N“产品是否为礼品”是"False"，“交付\N所需的天数”是"5"，“价格值”是“相当实惠”。
Dialogue: 0,0:11:45.14,0:11:48.86,Default,,0,0,0,,这是一个期望输出的例子。
Dialogue: 0,0:11:48.86,0:11:57.58,Default,,0,0,0,,这是一个客户评论的例子，\N以及尝试获得JSON输出的模板。
Dialogue: 0,0:11:57.58,0:11:58.58,Default,,0,0,0,,这是一条客户评论。
Dialogue: 0,0:11:58.58,0:12:00.54,Default,,0,0,0,,它说，“这个吹叶机非常神奇。
Dialogue: 0,0:12:00.54,0:12:05.26,Default,,0,0,0,,它有四个档位：微风、\N和风，大风和龙卷风。
Dialogue: 0,0:12:05.26,0:12:08.62,Default,,0,0,0,,它在两天内送达，\N正好赶上我妻子的周年纪念礼物。
Dialogue: 0,0:12:08.62,0:12:11.10,Default,,0,0,0,,我想我的妻子也很喜欢，她都没说什么。
Dialogue: 0,0:12:11.10,0:12:14.34,Default,,0,0,0,,到目前为止，我是唯一\N一个使用它的人，等等。
Dialogue: 0,0:12:14.34,0:12:15.62,Default,,0,0,0,,这里有一个评论模板。
Dialogue: 0,0:12:15.62,0:12:20.26,Default,,0,0,0,,对于以下文本，请提取\N以下信息：“这是不是一个礼物？”
Dialogue: 0,0:12:20.26,0:12:24.18,Default,,0,0,0,,所以在这种情况下，答案是\N肯定的，因为这是一个礼物。
Dialogue: 0,0:12:24.18,0:12:26.94,Default,,0,0,0,,还有“送货天数”，需要多长时间送达。
Dialogue: 0,0:12:26.94,0:12:30.10,Default,,0,0,0,,看起来在这种情况下，它两天就到了。
Dialogue: 0,0:12:30.10,0:12:32.14,Default,,0,0,0,,“价格值”是多少？
Dialogue: 0,0:12:32.14,0:12:36.22,Default,,0,0,0,,稍微贵一点，比别的吹叶机贵一些，等等。
Dialogue: 0,0:12:36.22,0:12:52.46,Default,,0,0,0,,评论模板要求LLM将客户评论\N作为输入，提取这三个字段，然后\N将输出格式化为JSON，带有以下键。
Dialogue: 0,0:12:55.46,0:12:57.34,Default,,0,0,0,,好的。
Dialogue: 0,0:12:57.34,0:13:01.38,Default,,0,0,0,,这就是如何将其封装在LongChain中。
Dialogue: 0,0:13:01.38,0:13:03.2,Default,,0,0,0,,让我们导入聊天提示模板。
Dialogue: 0,0:13:03.2,0:13:04.88,Default,,0,0,0,,其实我们之前已经导入过了。
Dialogue: 0,0:13:04.88,0:13:16.58,Default,,0,0,0,,从技术上讲，这行是多余的，但我\N还是会再次导入，然后从上面的\Nreview_template创建提示模板。
Dialogue: 0,0:13:16.58,0:13:20.70,Default,,0,0,0,,这就是提示模板。
Dialogue: 0,0:13:20.70,0:13:33.2,Default,,0,0,0,,现在，与我们之前使用\N提示模板类似，让我们创建\N要传递给OpenAI API的消息。
Dialogue: 0,0:13:33.2,0:13:39.42,Default,,0,0,0,,创建OpenAI API，调用\N该API，然后打印出响应。
Dialogue: 0,0:13:39.42,0:13:45.70,Default,,0,0,0,,我建议你暂停视频并运行代码。
Dialogue: 0,0:13:45.70,0:13:46.70,Default,,0,0,0,,就是这样。
Dialogue: 0,0:13:46.70,0:13:52.94,Default,,0,0,0,,它显示：是礼物，送货天数为\N2天，价格值看起来也相当准确。
Dialogue: 0,0:13:52.94,0:14:04.22,Default,,0,0,0,,嗯，但请注意，如果我们检查\N返回数据的类型，这实际上是一个字符串。
Dialogue: 0,0:14:04.22,0:14:09.58,Default,,0,0,0,,它看起来像JSON，看起来\N有键值对，但实际上不是字典。
Dialogue: 0,0:14:09.58,0:14:12.22,Default,,0,0,0,,这只是一个很长的字符串。
Dialogue: 0,0:14:12.22,0:14:17.42,Default,,0,0,0,,所以我真正想做的是去解析返回的内容，\N从键"gift"中获取值，这应该是"True"。
Dialogue: 0,0:14:17.42,0:14:22.2,Default,,0,0,0,,但是当我运行这个时，这应该会产生一个\N错误，因为，嗯，这实际上是一个字符串。
Dialogue: 0,0:14:22.2,0:14:24.82,Default,,0,0,0,,这不是一个Python字典。
Dialogue: 0,0:14:24.82,0:14:31.86,Default,,0,0,0,,那么让我们看看如何使用\NLangChain的解析器来完成这个任务。
Dialogue: 0,0:14:31.86,0:14:40.14,Default,,0,0,0,,我将从LangChain\N导入ResponseSchema\N和StructuredOutputParser。
Dialogue: 0,0:14:40.14,0:14:46.48,Default,,0,0,0,,然后我将通过指定这些返回的JSON的\N格式规范告诉LangChain我希望它解析什么。
Dialogue: 0,0:14:46.48,0:14:50.34,Default,,0,0,0,,在JSON格式规范中代表“礼物”的\N名称是"gift"，这是它的描述。
Dialogue: 0,0:14:50.34,0:14:52.42,Default,,0,0,0,,这个物品是为别人购买礼物吗？
Dialogue: 0,0:14:52.42,0:14:56.54,Default,,0,0,0,,嗯，回答是或者对，\N否则回答不是或者未知等等。
Dialogue: 0,0:14:56.54,0:15:05.82,Default,,0,0,0,,所以有一个礼物规范、送货日期规范、价格值\N规范，然后像这样把它们都放到一个列表里。
Dialogue: 0,0:15:05.82,0:15:20.38,Default,,0,0,0,,现在我已经为这些指定了规范，呃，\NLangChain实际上可以通过让output_parser\N告诉你它希望你发送给LLM的指令来给你提示。
Dialogue: 0,0:15:20.38,0:15:34.98,Default,,0,0,0,,所以如果我要打印格式指令，\N它对LLM有一套相当精确的指令，可以让\NLLM生成指定的内容格式以便解析器可以解析。
Dialogue: 0,0:15:34.98,0:15:44.6,Default,,0,0,0,,所以这是新的review_template，\Nreview_template包括\NLangChain生成的格式指令，
Dialogue: 0,0:15:44.6,0:15:58.70,Default,,0,0,0,,这样它就可以从review_template中创建提示，\N然后创建传递给OpenAI API的消息。
Dialogue: 0,0:15:58.70,0:16:16.18,Default,,0,0,0,,如果你愿意，你可以看一下实际的提示，\N它会给你提取礼物、送货天数、价格值等字段\N的指令，这是文本，然后这是格式指令。
Dialogue: 0,0:16:16.18,0:16:30.26,Default,,0,0,0,,最后，如果我们调用OpenAI API，\N让我们看看会得到什么返回结果。
Dialogue: 0,0:16:30.26,0:16:41.59,Default,,0,0,0,,现在是这样的，现在如果\N我们使用之前创建的output_parser，
Dialogue: 0,0:16:41.60,0:16:48.84,Default,,0,0,0,,你就可以把这个解析成一个output_dict\N，如果我打印出来就是这样的，
Dialogue: 0,0:16:48.84,0:16:56.59,Default,,0,0,0,,注意到这是字典类型，而不是字符串，
Dialogue: 0,0:16:56.59,0:17:09.2,Default,,0,0,0,,这就是为什么我现在可以\N提取与“礼物”关联的值并得到"True"，\N或者与“送货天数”关联的值并得到"2"，
Dialogue: 0,0:17:09.2,0:17:14.76,Default,,0,0,0,,或者你还可以提取与“价格值”关联的值。
Dialogue: 0,0:17:14.76,0:17:24.80,Default,,0,0,0,,这是一种将LLM输出解析为Python字典的\N巧妙方法，使输出更易于在后续处理中使用。
Dialogue: 0,0:17:24.80,0:17:29.20,Default,,0,0,0,,我建议你暂停视频并运行代码。
Dialogue: 0,0:17:29.20,0:17:32.78,Default,,0,0,0,,关于模型、提示和解析器就是这些了。
Dialogue: 0,0:17:32.78,0:17:37.96,Default,,0,0,0,,有了这些工具，希望你\N能更轻松地重用自己的提示模板，
Dialogue: 0,0:17:37.96,0:17:42.28,Default,,0,0,0,,与合作伙伴共享提示模板，\N甚至使用LangChain的内置
Dialogue: 0,0:17:42.28,0:17:51.98,Default,,0,0,0,,提示模板，正如你刚刚看到的，\N通常可以与输出解析器配合\N使用，以便输入提示以特定格式输出
Dialogue: 0,0:17:51.98,0:18:02.28,Default,,0,0,0,,然后解析器解析输出，将\N数据存储在Python字典或其他\N数据结构中，以便用于后续处理的。
Dialogue: 0,0:18:02.28,0:18:06.40,Default,,0,0,0,,希望这些知识能对你有用，帮助\N你应用在自己的应用开发中。
Dialogue: 0,0:18:06.40,0:18:08.30,Default,,0,0,0,,说到这里，我们接下来将继续下一个视频
Dialogue: 0,0:18:08.30,0:18:16.56,Default,,0,0,0,,在那里我们将学习LangChain\N如何帮助你构建更好的聊天机器人，
Dialogue: 0,0:18:16.56,0:18:21.24,Default,,0,0,0,,让LLM通过更好地管理\N历史对话，达到更好的聊天效果。