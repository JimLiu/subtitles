1
00:00:05,000 --> 00:00:08,800
In lesson one, we'll be covering models, prompts, and parsers.

2
00:00:08,800 --> 00:00:13,600
So models refers to the language models underpinning a lot of it.

3
00:00:13,600 --> 00:00:18,760
Prompts refers to the style of creating inputs to pass into the models.

4
00:00:18,760 --> 00:00:20,760
And then parsers is on the opposite end.

5
00:00:20,760 --> 00:00:27,760
It involves taking the output of these models and parsing it into a more structured format so that you can do things downstream with it.

6
00:00:27,760 --> 00:00:32,640
So when you build an application using an LLM, there'll often be reusable models.

7
00:00:32,640 --> 00:00:40,140
We have repeatedly prompted model, parsers output, and so LangChain gives an easy set of abstractions to do this type of operation.

8
00:00:40,140 --> 00:00:45,160
So with that, let's jump in and take a look at models, prompts, and parsers.

9
00:00:45,160 --> 00:00:47,640
So to get started, here's a little bit of starter code.

10
00:00:47,640 --> 00:00:53,720
I'm going to import OS, import OpenAI, and load my OpenAI secret key.

11
00:00:53,720 --> 00:00:58,040
The OpenAI library is already installed in my Jupyter notebook environment.

12
00:00:58,040 --> 00:01:03,440
If you're running this locally and you don't have OpenAI installed yet, you might need to run that.

13
00:01:03,440 --> 00:01:07,360
"!pip install openai", but I'm not going to do that here.

14
00:01:07,360 --> 00:01:09,000
And then here's a helper function.

15
00:01:09,000 --> 00:01:20,680
This is actually very similar to the helper function that you might have seen in the "ChatGPT Prompt Engineering for Developers" course that I offered together with OpenAI's Isa Fulford.

16
00:01:20,680 --> 00:01:26,960
And so with this helper function, you can say get_completion on what is one plus one,

17
00:01:26,960 --> 00:01:36,200
and this will call ChatGPT or technically the model GPT 3.5 Turbo to give you an answer back like this.

18
00:01:36,200 --> 00:01:49,840
Now to motivate the LangChain abstractions for model prompts and parsers, let's say you get an email from a customer in a language other than English.

19
00:01:49,840 --> 00:01:55,285
In order to make sure this is accessible, the other language I'm going to use is the English pirate language,

20
00:01:55,286 --> 00:02:02,680
where the content says, "I be fuming that me blender lid flew off and splattered my kitchen walls with smoothie.

21
00:02:02,680 --> 00:02:06,080
And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen.

22
00:02:06,080 --> 00:02:08,600
I need your help right now, matey."

23
00:02:08,600 --> 00:02:18,360
And so what we will do is ask this LLM to translate the text to American English in a calm and respectful tone.

24
00:02:18,360 --> 00:02:23,400
So I'm going to set style to American English in a calm and respectful tone.

25
00:02:23,400 --> 00:02:29,320
And so in order to actually accomplish this, if you've seen a little bit of prompting before,

26
00:02:29,320 --> 00:02:32,986
I'm going to specify the prompt using an "f" string with the instructions,

27
00:02:32,987 --> 00:02:40,160
Translate the text that is delimited by triple backticks into style that is "style", and then plug in these two styles.

28
00:02:40,160 --> 00:02:46,120
And so this generates a prompt that says, "Translate the text," and so on.

29
00:02:46,120 --> 00:02:54,800
I encourage you to pause the video and run the code, and also try modifying the prompt to see if you can get a different output.

30
00:02:54,800 --> 00:03:03,080
You can then prompt the large language model to get a response.

31
00:03:03,080 --> 00:03:04,080
Let's see what the response is.

32
00:03:04,080 --> 00:03:07,871
It says, "Translated the English pirate's message into this very polite,

33
00:03:07,872 --> 00:03:13,540
'I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie,'" and so on.

34
00:03:13,540 --> 00:03:16,520
I could really use your help right now, my friend.

35
00:03:16,520 --> 00:03:18,760
That sounds very nice.

36
00:03:18,761 --> 00:03:27,271
So if you have different customers writing reviews in different languages,  not just English pirate, but French, German, Japanese, and so on,

37
00:03:27,272 --> 00:03:34,500
you can imagine having to generate a whole sequence of prompts to generate such translations.

38
00:03:34,500 --> 00:03:40,360
Let's look at how we can do this in a more convenient way using LangChain.

39
00:03:40,360 --> 00:03:44,600
I'm going to import ChatOpenAI.

40
00:03:44,600 --> 00:03:49,780
This is LangChain's abstraction for the ChatGPT API endpoint.

41
00:03:49,780 --> 00:03:54,157
And so if I then set chat equals ChatOpenAI and look at what chat is,

42
00:03:54,158 --> 00:04:02,960
it creates this object as follows that uses the ChatGPT model, which is also called gpt-3.5-turbo.

43
00:04:02,960 --> 00:04:09,740
When I'm building applications, one thing I will often do is set the temperature parameter to be equal to zero.

44
00:04:09,740 --> 00:04:20,400
So the default temperature is 0.7, but let me actually redo that with temperature equals 0.0.

45
00:04:20,400 --> 00:04:27,960
And now the temperature is set to zero to make this output a little bit less random.

46
00:04:27,960 --> 00:04:30,960
And now let me define the template string as follows.

47
00:04:30,960 --> 00:04:35,280
Translate the text delimited by triple backticks into style is style.

48
00:04:35,280 --> 00:04:37,480
And then here's the text.

49
00:04:37,480 --> 00:04:47,040
And to repeatedly reuse this template, let's import LangChain's ChatPromptTemplate.

50
00:04:47,040 --> 00:05:00,760
And then let me create a prompt template using that template string that we just wrote above.

51
00:05:00,760 --> 00:05:04,860
From the prompt template, you can actually extract the original prompt.

52
00:05:04,860 --> 00:05:17,040
And it realizes that this prompt has two input variables, the style and the text, which were shown here with the curly braces.

53
00:05:17,040 --> 00:05:19,720
And here is the original template as well that we had specified.

54
00:05:19,720 --> 00:05:28,200
In fact, if I print this out, it realizes it has two input variables, style and text.

55
00:05:28,200 --> 00:05:30,360
Now let's specify the style.

56
00:05:30,360 --> 00:05:33,960
This is a style that I want the customer message to be translated to.

57
00:05:33,960 --> 00:05:37,240
So I'm going to call this customer style.

58
00:05:37,240 --> 00:05:44,960
And here's my same customer email as before.

59
00:05:44,960 --> 00:05:59,640
And now if I create customer messages, this will generate the prompt and will pass this a large language model in a minute to get a response.

60
00:05:59,640 --> 00:06:05,200
So if you want to look at the types, the customer message is actually a list.

61
00:06:05,200 --> 00:06:16,400
And if you look at the first element of the list, this is more or less that prompt that you would expect this to be creating.

62
00:06:16,400 --> 00:06:20,560
Lastly, let's pass this prompt to the LLM.

63
00:06:20,560 --> 00:06:32,560
So I'm going to call chat, which we had set earlier as a reference to the OpenAI ChatGPT endpoint.

64
00:06:32,560 --> 00:06:45,400
If we print out the customer responses content, then it gives you back this text translated from English pirate to polite American English.

65
00:06:45,400 --> 00:06:50,560
And of course, you can imagine other use cases where the customer emails are in other languages.

66
00:06:50,560 --> 00:06:58,240
And this too can be used to translate the messages for an English speaking to understand and reply to.

67
00:06:58,240 --> 00:07:06,720
I encourage you to pause the video and run the code and also try modifying the prompt to see if you can get a different output.

68
00:07:06,720 --> 00:07:11,880
Now let's hope our customer service agent replied to the customer in their original language.

69
00:07:11,880 --> 00:07:17,040
So let's say English speaking customer service agent writes this and says, "Hey there customer,

70
00:07:17,040 --> 00:07:20,400
warranty does not cover, clean expenses for the kitchen because it's your fault.

71
00:07:20,400 --> 00:07:23,800
You misused your blender by forgetting to put on the lid.

72
00:07:23,800 --> 00:07:25,800
Tough luck. See ya."

73
00:07:25,800 --> 00:07:32,800
Not a very polite message, but let's say this is what a customer service agent wants.

74
00:07:32,800 --> 00:07:39,600
We are going to specify that the service message is going to be translated to this pirate style.

75
00:07:39,600 --> 00:07:45,520
So we want it to be in a polite tone that speaks in English pirate.

76
00:07:45,520 --> 00:07:48,271
And because we previously created that prompt template,

77
00:07:48,272 --> 00:07:58,920
the cool thing is we can now reuse that prompt template and specify that the output style we want is this service style pirate and the text is this service reply.

78
00:07:58,920 --> 00:08:07,080
And if we do that, that's the prompt.

79
00:08:07,080 --> 00:08:12,440
And if we prompt on ChatGPT, this is the response it gives us back.

80
00:08:12,440 --> 00:08:20,440
"Ahoy there, matey. I must kindly inform you that the warranty be not covering the expenses or cleaning your galley."

82
00:08:21,440 --> 00:08:23,920
"Aye, tough luck. Farewell, me hearty."

83
00:08:23,920 --> 00:08:29,200
So you might be wondering why are we using prompt templates instead of, you know, just an F string?

84
00:08:29,200 --> 00:08:35,840
The answer is that as you build sophisticated applications, prompts can be quite long and detailed.

85
00:08:35,840 --> 00:08:43,360
And so prompt templates are a useful abstraction to help you reuse good prompts when you can.

86
00:08:43,360 --> 00:08:52,480
This is an example of a relatively long prompt to grade a student's submission for an online learning application.

87
00:08:52,480 --> 00:09:03,080
And a prompt like this can be quite long in which you can ask the LLM to first solve the problem and then have the output in a certain format and output in a certain format.

88
00:09:03,080 --> 00:09:09,440
And wrapping this in a LangChain prompt makes it easier to reuse a prompt like this.

89
00:09:09,440 --> 00:09:22,580
So you see later that LangChain provides prompts for some common operations such as summarization or question answering or connecting to SQL databases or connecting to different APIs.

90
00:09:22,580 --> 00:09:32,300
And so by using some of LangChain's built-in prompts, you can quickly get an application working without needing to engineer your own prompts.

91
00:09:32,300 --> 00:09:43,340
One other aspect of LangChain's prompt libraries is that it also supports output parsing, which we'll get to in a minute.

92
00:09:43,340 --> 00:09:55,600
But when you're building a complex application using an LLM, you often instruct the LLM to generate its output in a certain format, such as using specific keywords.

93
00:09:55,600 --> 00:10:06,540
This example on the left illustrates using an LLM to carry out something called chain of thought reasoning using a framework called the ReAct framework.

94
00:10:06,540 --> 00:10:09,380
But don't worry about the technical details.

95
00:10:09,380 --> 00:10:21,660
But the keys of that is that the thought is what the LLM is thinking because by giving an LLM space to think, it can often get to more accurate conclusions.

96
00:10:21,660 --> 00:10:31,700
Then action as a keyword to carry the specific action, and then observation to show what it learned from that action, and so on.

97
00:10:31,700 --> 00:10:39,599
And if you have a prompt that instructs the LLM to use these specific keywords, thought, action, and observation,

98
00:10:39,600 --> 00:10:48,140
then this prompt can be coupled with a parser to extract out the text that has been tagged with these specific keywords.

99
00:10:48,140 --> 00:11:01,460
And so that together gives a very nice abstraction to specify the input to an LLM, and then also have a parser correctly interpret the output that the LLM gives.

100
00:11:01,460 --> 00:11:09,140
And so with that, let's return to see an example of an output parser using LangChain.

101
00:11:09,140 --> 00:11:17,700
In this example, let's take a look at how you can have an LLM output JSON and use LangChain to parse that output.

102
00:11:17,700 --> 00:11:29,240
And the running example that I'll use will be to extract information from a product review and format that output in a JSON format.

103
00:11:29,240 --> 00:11:33,780
So here's an example of how you would like the output to be formatted.

104
00:11:33,780 --> 00:11:45,140
Technically, this is a Python dictionary where whether or not the product is a gift, maps the falls, the number of days it took to deliver, was five, and the price value was pretty affordable.

105
00:11:45,140 --> 00:11:48,860
So this is one example of a desired output.

106
00:11:48,860 --> 00:11:57,580
Here is an example of customer review as well as a template to try to get to that JSON output.

107
00:11:57,580 --> 00:11:58,580
So here's a customer review.

108
00:11:58,580 --> 00:12:00,540
It says, "This leaf blower is pretty amazing.

109
00:12:00,540 --> 00:12:05,260
It has four settings, candle blower, gentle breeze, windy city, and tornado.

110
00:12:05,260 --> 00:12:08,620
It arrived in two days, just in time for my wife's anniversary present.

111
00:12:08,620 --> 00:12:11,100
I think my wife liked it so much she was speechless.

112
00:12:11,100 --> 00:12:14,340
So far I've been the only one using it," and so on.

113
00:12:14,340 --> 00:12:15,620
And here's a review_template.

114
00:12:15,620 --> 00:12:20,260
For the following text, extract the following information, specify was this a gift

115
00:12:20,260 --> 00:12:24,180
So in this case, it would be yes because this is a gift

116
00:12:24,180 --> 00:12:26,940
And also delivery days, how long did it take to deliver.

117
00:12:26,940 --> 00:12:30,100
It looks like in this case, it arrived in two days.

118
00:12:30,100 --> 00:12:32,140
And what's the price value?

119
00:12:32,140 --> 00:12:36,220
You know, slightly more expensive than the leaf b'l'o'w'ers, and so on.

120
00:12:36,220 --> 00:12:52,460
So the review_template asks the LLM to take as input a customer review and extract these three fields, and then format the output as JSON, um, with the following keys.

121
00:12:55,460 --> 00:12:57,340
All right.

122
00:12:57,340 --> 00:13:01,380
So here's how you can wrap this in LongChain.

123
00:13:01,380 --> 00:13:03,020
Let's import the ChatPrompt template.

124
00:13:03,020 --> 00:13:04,880
We'd actually imported this already earlier.

125
00:13:04,880 --> 00:13:16,580
So technically, this line is redundant, but I'll just import it again and then have the prompt templates, um, created from the review_template up on top.

126
00:13:16,580 --> 00:13:20,700
And so here's the prompt template.

127
00:13:20,700 --> 00:13:33,020
And now, similar to our early usage of a prompt template, let's create the messages to pass to the OpenAI, uh, endpoint.

128
00:13:33,020 --> 00:13:39,420
Create the OpenAI endpoint, call that endpoint, and then let's print out the response.

129
00:13:39,420 --> 00:13:45,700
I encourage you to pause the video and run the code.

130
00:13:45,700 --> 00:13:46,700
And there it is.

131
00:13:46,700 --> 00:13:52,940
It says, "Gift is true, delivery days is two," and the price value also looks pretty accurate.

132
00:13:52,940 --> 00:14:04,220
Um, but note that if we check the type of the response, this is actually a string.

133
00:14:04,220 --> 00:14:09,580
So it looks like JSON and looks like it has key value pairs, but it's actually not a dictionary.

134
00:14:09,580 --> 00:14:12,220
This is just one long string.

135
00:14:12,220 --> 00:14:17,420
So what I'd really like to do is go to the response content and get the value from the gift key, which should be true.

136
00:14:17,420 --> 00:14:22,020
But I run this, this should generate an error because, well, this is actually a string.

137
00:14:22,020 --> 00:14:24,820
This is not a Python dictionary.

138
00:14:24,820 --> 00:14:31,860
So let's see how we would use LangChain's, um, parser in order to do this.

139
00:14:31,860 --> 00:14:40,140
I'm going to import ResponseSchema and StructuredOutputParser from LangChain.

140
00:14:40,140 --> 00:14:46,480
And I'm going to tell it what I want it to parse by specifying these response schemas.

141
00:14:46,480 --> 00:14:50,340
So the gift schema is named gift, and here's the description.

142
00:14:50,340 --> 00:14:52,420
Does the item purchase a gift for someone else?

143
00:14:52,420 --> 00:14:56,540
Uh, answer true or yes, false if not or unknown, and so on.

144
00:14:56,540 --> 00:15:05,820
So have a gift schema, delivery day schema, price value schema, and then let's put all three of them into a list as follows.

145
00:15:05,820 --> 00:15:20,380
Now that I've specified the schema for these, um, LangChain can actually give you the prompt itself by having the output parser tell you what instructions it wants you to send to the LLM.

146
00:15:20,380 --> 00:15:34,980
So if I were to print format instructions, she has a pretty precise set of instructions for the LLM that will cause it to generate an output that the output parser can process.

147
00:15:34,980 --> 00:15:44,056
So here's the new review_template, and the review_template includes the format instructions that LangChain generated,

148
00:15:44,057 --> 00:15:58,700
and so it can create a prompt from the review_template too, and then create the messages that will pass to the OpenAI endpoint.

149
00:15:58,700 --> 00:16:16,180
If you want, you can take a look at the actual prompt, which gives you instructions to extract the fields, gift, delivery days, price value, here's the text, and then here are the formatting instructions.

150
00:16:16,180 --> 00:16:30,260
Finally, if we call the OpenAI endpoint, let's take a look at what response we got.

151
00:16:30,260 --> 00:16:41,586
It is now this, and now if we use the output_parser that we created earlier,

152
00:16:41,600 --> 00:16:48,843
you can then parse this into an output dictionary, which if I print looks like this,

153
00:16:48,844 --> 00:16:56,586
and notice that this is of type dictionary, not a string,

154
00:16:56,587 --> 00:17:09,020
which is why I can now extract the value associated with the key gift and get true, or the value associated with delivery days and get two,

155
00:17:09,020 --> 00:17:14,760
or you can also extract the value associated with price value.

156
00:17:14,760 --> 00:17:24,800
So this is a nifty way to take your LLM output and parse it into a Python dictionary to make the output easier to use in downstream processing.

157
00:17:24,800 --> 00:17:29,200
I encourage you to pause the video and run the code.

158
00:17:29,200 --> 00:17:32,780
And so that's it for models, prompts, and parsers.

159
00:17:32,780 --> 00:17:37,960
With these tools, hopefully you'll be able to reuse your own prompt templates easily,

160
00:17:37,960 --> 00:17:42,280
share prompt templates with others that you're collaborating with, even use LangChain's built-in

161
00:17:42,280 --> 00:17:51,975
prompt templates, which as you just saw, can often be coupled with an output_parser, so that the input prompt to output in a specific format

162
00:17:51,976 --> 00:18:02,280
and then the parser parses that output to store the data in a Python dictionary or some other data structure that makes it easy for downstream processing.

163
00:18:02,280 --> 00:18:06,400
I hope you find this useful in many of your applications.

164
00:18:06,400 --> 00:18:08,300
And with that, let's go into the next video

165
00:18:08,301 --> 00:18:16,557
where we'll see how LangChain can help you build better chatbots or have an LLM have more effective chats

166
00:18:16,558 --> 00:18:21,240
by better managing what it remembers from the conversation you've had so far.
