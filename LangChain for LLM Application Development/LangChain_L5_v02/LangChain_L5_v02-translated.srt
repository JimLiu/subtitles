1
00:00:04,962 --> 00:00:08,129
当使用LLM构建复杂应用程序时，

2
00:00:08,135 --> 00:00:11,414
一个重要但有时有点棘手的步骤是

3
00:00:11,429 --> 00:00:14,367
如何评估应用程序的表现？

4
00:00:14,367 --> 00:00:17,149
它是否达到了某种验收标准？

5
00:00:17,149 --> 00:00:20,854
此外，如果你决定换一种实现方式，

6
00:00:20,855 --> 00:00:23,229
可能换到不同的LLM，

7
00:00:23,257 --> 00:00:28,071
或者更改如何使用向量数据库的策略，或者使用其他方式检索数据，

8
00:00:28,072 --> 00:00:30,594
或者更改系统中的其他参数。

9
00:00:30,594 --> 00:00:33,775
你要怎么样知道结果是比以前更好了？还是更糟糕了？

10
00:00:33,775 --> 00:00:40,356
在这个视频中，Harrison将深入探讨一些框架，以思考如何评估基于LLM的应用程序，

11
00:00:40,357 --> 00:00:43,366
同时会介绍一些帮助你评估的工具。

12
00:00:43,367 --> 00:00:47,589
这些应用实际上是许多不同步骤的链和序列。

13
00:00:47,589 --> 00:00:50,700
我们的首要任务是

14
00:00:50,701 --> 00:00:54,491
了解每个步骤的输入和输出到底是什么。

15
00:00:54,491 --> 00:00:58,933
因此，我们会用到一些可视化工具和调试工具。

16
00:00:59,953 --> 00:01:05,418
使用大量不同的数据集来测试模型，这有助于我们全面了解模型的表现。

17
00:01:05,418 --> 00:01:07,800
观察事物的一种方法是用肉眼看。

18
00:01:07,800 --> 00:01:15,927
但还有一个非常酷的想法，就是使用语言模型和链来评估其他语言模型、其他链和其他应用程序。

19
00:01:15,927 --> 00:01:17,328
我们也会深入探讨这个想法。

20
00:01:17,978 --> 00:01:19,682
这个视频中有很多有趣的话题，

21
00:01:19,683 --> 00:01:26,943
我发现随着越来越多的开发转向基于Prompt的开发，使用LLM开发应用程序，

22
00:01:26,944 --> 00:01:31,326
整个工作流程、评估过程正在被重新思考。

23
00:01:31,326 --> 00:01:34,267
这个视频里有很多激动人心的概念。

24
00:01:34,267 --> 00:01:36,568
让我们开始吧。

25
00:01:36,568 --> 00:01:38,930
好的，让我们开始设置评估所需要的环境。

26
00:01:39,877 --> 00:01:45,919
首先，我们需要有一个链或应用程序，以便进行评估。

27
00:01:45,919 --> 00:01:50,581
我们将使用上一课的文档问答链。

28
00:01:50,581 --> 00:01:52,742
所以我们要导入所需的相关库。

29
00:01:52,742 --> 00:01:59,125
我们将加载上一课中使用过的相同数据。

30
00:01:59,125 --> 00:02:02,266
我们将用一行代码创建索引。

31
00:02:03,110 --> 00:02:17,273
然后，我们将通过指定语言模型、链类型、检索器以及verbosity（要打印日志的详细程度）这些参数来创建RetrievalQA链。

32
00:02:17,273 --> 00:02:18,934
现在我们有了这个应用程序。

33
00:02:18,934 --> 00:02:21,086
我们需要做的第一件事就是

34
00:02:21,087 --> 00:02:27,455
真正搞清楚我们需要用什么样的数据集来评估。

35
00:02:27,455 --> 00:02:29,976
我将介绍几种不同的方法来实现这个目标。

36
00:02:30,953 --> 00:02:33,385
第一个方法最简单，

37
00:02:33,386 --> 00:02:38,998
就是自己想出一些好的示例数据集。

38
00:02:38,999 --> 00:02:43,571
为了做到这一点，我们可以查看一些数据并提出示例问题

39
00:02:43,572 --> 00:02:49,766
然后提供可以用于评估的标准答案。

40
00:02:49,766 --> 00:02:51,948
让我们看一下这里的几份文档，

41
00:02:53,037 --> 00:02:56,338
可以了解到文档里面都有什么。

42
00:02:56,338 --> 00:03:00,220
看起来第一个文档是关于一套连帽衫的。

43
00:03:00,220 --> 00:03:06,322
第二个文档是关于一件夹克的，都有很多详细信息。

44
00:03:08,400 --> 00:03:13,223
从这些详细信息中，我们可以创建若干组示例问题和答案。

45
00:03:13,223 --> 00:03:19,107
第一个问题，我们可以简单地问：“舒适保暖连帽衫套装有侧口袋吗？”

46
00:03:19,107 --> 00:03:25,431
我们可以通过上面看到，它确实有侧口袋。

47
00:03:25,431 --> 00:03:31,794
对于第二个，我们可以看到这件夹克来自某个系列，DownTek系列。

48
00:03:31,794 --> 00:03:35,456
所以我们可以问这个问题："这件夹克来自哪个系列？"

49
00:03:35,456 --> 00:03:37,837
答案是："DownTek系列"。

50
00:03:37,837 --> 00:03:41,138
所以在这里我们创建了两组问答示例。

51
00:03:41,138 --> 00:03:43,679
但是，这种方式并不易于扩展。

52
00:03:43,679 --> 00:03:47,021
需要花费一些时间来查看每个示例，弄清楚发生了什么。

53
00:03:47,021 --> 00:03:50,142
那么有没有办法让我们自动化这个过程？

54
00:03:50,142 --> 00:03:54,064
借助语言模型来自动化这个过程是一种非常酷的方式。

55
00:03:55,731 --> 00:03:59,613
在LangChain中有一个链可以做到这一点。

56
00:03:59,613 --> 00:04:01,400
我们可以导入QAGenerationChain

57
00:04:01,401 --> 00:04:08,216
它可以读取文档并从每个文档中生成一组问题和答案。

58
00:04:08,216 --> 00:04:10,638
它将借助语言模型来实现。

59
00:04:10,638 --> 00:04:15,640
因此，我们需要通过传入ChatOpenAI语言模型来创建这个链。

60
00:04:15,640 --> 00:04:18,321
然后我们可以用它创建很多组问题和答案示例。

61
00:04:19,255 --> 00:04:25,442
我们将使用apply_and_parse方法，应用解析器来解析输出的结果。

62
00:04:25,442 --> 00:04:30,587
因为我们想要得到一系列包含一组问题和答案的字典对象，而不是一个文本字符串。

63
00:04:38,006 --> 00:04:41,574
现在如果我们看一下返回的内容，

64
00:04:41,575 --> 00:04:44,428
我们可以看到一个问题和一个答案。

65
00:04:44,428 --> 00:04:49,110
让我们检查一下这是哪个文档的问题和答案。

66
00:04:49,110 --> 00:04:51,651
我们可以看到它在问这个的重量是多少。

67
00:04:51,651 --> 00:04:54,172
我们可以看到它是从这里获取重量的。

68
00:04:54,172 --> 00:04:54,752
看看这个，

69
00:04:54,752 --> 00:04:57,313
我们刚刚生成了很多组问题和答案，

70
00:04:57,313 --> 00:04:59,474
不需要去一个个手动生成。

71
00:04:59,474 --> 00:05:02,475
这样可以帮我们节省很多时间，可以去做更多其他有意思的事情。

72
00:05:04,260 --> 00:05:09,602
现在让我们把这些示例加入到我们已经创建的示例中。

73
00:05:09,602 --> 00:05:15,343
现在我们有了这些示例数据，但是我们如何评估效果如何？

74
00:05:15,343 --> 00:05:21,865
首先，我们要做的就是将其中某个示例传入链并运行，然后观察它输出的结果。

75
00:05:21,865 --> 00:05:24,506
这里我们输入一个问题，然后得到返回结果。

76
00:05:25,965 --> 00:05:32,290
但我们无法观察到链的内部到底发生了什么！

77
00:05:32,290 --> 00:05:35,012
例如传给语言模型中的Prompt是什么？

78
00:05:35,012 --> 00:05:37,754
它检索到的文档有哪些？

79
00:05:37,754 --> 00:05:41,957
如果这是一个包含多个步骤的复杂链，那么每一步的中间结果是什么？

80
00:05:42,890 --> 00:05:47,214
仅仅观察最终答案通常不足以理解

81
00:05:47,243 --> 00:05:51,435
链中哪里出错或可能出错。

82
00:05:51,435 --> 00:05:59,921
为了解决这个问题，LangChain中提供了一个有意思的小工具，叫做“langchain debug”。

83
00:05:59,921 --> 00:06:03,143
如果我们把 "langchain.debug" 设为 "True",

84
00:06:03,574 --> 00:06:06,823
然后把之前的示例再运行一遍，

85
00:06:08,471 --> 00:06:11,508
就可以看到它输出了更多的信息。

86
00:06:12,455 --> 00:06:14,729
如果我们看看它到底输出了什么，

87
00:06:14,730 --> 00:06:18,920
我们可以看到它首先调用了RetrievalQA链，

88
00:06:18,920 --> 00:06:21,583
然后它又调用了StuffDocumentsChain。

89
00:06:21,583 --> 00:06:25,005
上一节课讲到过，我们使用了 "stuff" 方法。

90
00:06:25,005 --> 00:06:29,389
现在它调用了LLMChain，有几个不同的输入参数：

91
00:06:29,389 --> 00:06:32,471
可以看到原始问题，

92
00:06:32,471 --> 00:06:34,393
传入的上下文，

93
00:06:34,393 --> 00:06:40,118
可以看到这个上下文是根据问题检索到的几个文档块内容。

94
00:06:40,758 --> 00:06:42,314
在做问答时，

95
00:06:42,315 --> 00:06:48,602
往往当返回错误的结果时，不一定是语言模型本身出了问题。

96
00:06:48,602 --> 00:06:51,504
实际上，可能是在检索的步骤出了问题。

97
00:06:51,504 --> 00:06:54,757
仔细查看问题的确切内容

98
00:06:54,758 --> 00:06:59,808
和上下文的确切内容可以帮助调试，找出问题在哪。

99
00:06:59,808 --> 00:07:06,532
我们可以再深入一层，看看究竟传入语言模型（ChatOpenAI）的Prompt究竟是什么。

100
00:07:07,420 --> 00:07:09,902
这里我们可以看到传入语言模型的完整Prompt：

101
00:07:09,902 --> 00:07:12,384
有一个系统消息。

102
00:07:12,384 --> 00:07:14,506
有一个对Prompt的描述。

103
00:07:14,506 --> 00:07:18,815
这就是问答链在底层使用的Prompt，

104
00:07:18,816 --> 00:07:21,243
我们直到现在才看到。

105
00:07:21,244 --> 00:07:27,135
我们可以看到Prompt输出：“使用以下上下文信息来回答用户的问题，

106
00:07:27,135 --> 00:07:29,297
如果你不知道答案，就说不知道，

107
00:07:29,297 --> 00:07:30,638
不要试图编造答案。”

108
00:07:30,638 --> 00:07:34,181
然后我们看到之前插入的一堆上下文。

109
00:07:34,181 --> 00:07:37,123
接着我们看到一个人类提出的问题，这就是我们问它的问题。

110
00:07:37,683 --> 00:07:40,705
我们还可以看到更多语言模型返回的结果信息。

111
00:07:40,705 --> 00:07:45,514
所以不仅仅是一个字符串，我们还得到了诸如Token使用量这样的信息，

112
00:07:45,515 --> 00:07:50,951
比如Prompt消耗了多少Token、返回的结果消耗了多少Token、总共消耗的Token数和模型名称。

113
00:07:50,951 --> 00:07:58,013
这些信息对于跟踪在链或调用语言模型中使用了多少Token非常有用，

114
00:07:58,314 --> 00:08:02,543
根据消耗的Token数和模型，可以算出来花费了多少成本。

115
00:08:03,608 --> 00:08:05,543
因为这是一个相对简单的链，

116
00:08:05,557 --> 00:08:07,937
我们现在可以看到最后的返回结果：

117
00:08:07,957 --> 00:08:11,514
“舒适保暖连帽衫套装，条纹款，有侧口袋。”，

118
00:08:11,515 --> 00:08:16,297
通过链传递并返回给用户。

119
00:08:16,297 --> 00:08:19,386
我们刚刚讲解了如何查看和调试

120
00:08:19,460 --> 00:08:22,442
这个链中单个输入的情况。

121
00:08:22,442 --> 00:08:24,643
但是如何输入我们前面创建的所有示例呢？

122
00:08:24,643 --> 00:08:27,005
我们该如何评估它们呢？

123
00:08:27,005 --> 00:08:30,448
和创建示例数据的方法类似，一种方法是手动操作。

124
00:08:31,019 --> 00:08:33,600
我们可以在所有示例上运行链，

125
00:08:33,601 --> 00:08:39,771
然后观察输出，搞清楚发生了什么，是否正确、错误或部分正确。

126
00:08:39,771 --> 00:08:44,177
与创建示例数据类似，手动操作的方法随着时间的推移会变得有些繁琐。

127
00:08:44,177 --> 00:08:46,900
所以让我们回到大家最喜欢的解决方案。

128
00:08:46,900 --> 00:08:48,883
能让语言模型来完成吗？

129
00:08:50,220 --> 00:08:53,382
首先，我们需要为所有示例生成实际答案。

130
00:08:53,382 --> 00:08:54,300
在此之前，

131
00:08:54,314 --> 00:09:00,943
我实际上要关闭调试模式，以免把所有内容都打印出来，

132
00:09:00,944 --> 00:09:06,152
然后我将为所有不同的示例生成实际答案。

133
00:09:06,152 --> 00:09:08,743
我想我们总共有七个示例，

134
00:09:08,744 --> 00:09:13,718
我们将循环七次，为每个示例生成一个实际答案。

135
00:09:31,559 --> 00:09:35,301
现在我们有了这些示例，可以考虑评估它们。

136
00:09:35,301 --> 00:09:39,343
我们将导入QAEvalChain。

137
00:09:39,343 --> 00:09:42,600
我们将用语言模型创建这个链，

138
00:09:42,601 --> 00:09:51,849
因为我们将使用语言模型来帮助评估。

139
00:09:51,849 --> 00:09:54,450
然后我们将在这个链上调用"evaluate"方法。

140
00:09:54,450 --> 00:09:57,057
并传入示例列表和实际答案列表，

141
00:09:57,071 --> 00:09:59,273
然后我们将得到与这组示例列表相对应的一组评估打分结果，并保存到了"graded_outputs"变量。

142
00:10:01,002 --> 00:10:06,656
为了查看每个示例的具体情况，

143
00:10:06,657 --> 00:10:08,524
我们将遍历它们。

144
00:10:08,524 --> 00:10:10,144
我们要打印出问题。

145
00:10:10,144 --> 00:10:13,025
再次说明，这是由语言模型生成的。

146
00:10:13,025 --> 00:10:15,005
我们要打印出标准答案。

147
00:10:15,005 --> 00:10:20,227
同样，这是语言模型基于整个文档的内容生成的，

148
00:10:20,227 --> 00:10:22,047
所以它生成的标准答案是可靠的。

149
00:10:23,138 --> 00:10:24,819
我们要打印出实际答案。

150
00:10:24,819 --> 00:10:28,129
这是在语言模型和问答链生成的，

151
00:10:28,130 --> 00:10:32,929
先对问题生成Embedding，然后去向量数据库检索相似文档，再将检索出来的文档传递给语言模型，

152
00:10:32,930 --> 00:10:36,202
然后语言模型生成实际答案。

153
00:10:36,202 --> 00:10:38,123
我们还会打印出评分。

154
00:10:38,123 --> 00:10:40,900
再次强调，这也是由语言模型生成的

155
00:10:40,901 --> 00:10:45,571
评估链对标准答案和实际答案进行对比，判断对错，得出一个评分。

156
00:10:45,985 --> 00:10:48,571
所以当我们遍历所有这些示例并打印它们时，

157
00:10:48,572 --> 00:10:51,687
可以详细查看每个示例。

158
00:10:54,612 --> 00:10:57,474
看起来每一个示例都做对了。

159
00:10:57,474 --> 00:11:02,418
这是一个相对简单的检索问题，所以这结果还是靠得住的。

160
00:11:02,418 --> 00:11:04,920
那我们来看第一个示例。

161
00:11:04,920 --> 00:11:09,343
这里的问题是：“舒适保暖套装有侧面口袋吗？”

162
00:11:09,343 --> 00:11:12,365
我们创建的标准答案是：“是的”。

163
00:11:12,365 --> 00:11:16,288
语言模型生成的实际答案是：

164
00:11:16,289 --> 00:11:19,751
“舒适保暖套装，条纹款确实有侧面口袋。”

165
00:11:20,627 --> 00:11:23,769
所以我们可以认为这个答案是正确的。

166
00:11:23,769 --> 00:11:27,592
实际上，语言模型也知道，并且它把它标记为正确。

167
00:11:27,592 --> 00:11:32,215
但让我们想想为什么我们需要使用语言模型。

168
00:11:32,215 --> 00:11:35,898
因为这两个字符串实际上一点都不像。

169
00:11:36,685 --> 00:11:37,445
它们非常不同。

170
00:11:37,445 --> 00:11:39,507
一个很短，一个很长。

171
00:11:39,507 --> 00:11:42,828
“是的”这个词在这个长字符串里都没有出现。

172
00:11:42,828 --> 00:11:47,657
所以如果我们试图进行字符串匹配、精确匹配或者使用正则表达式，

173
00:11:47,986 --> 00:11:50,753
是无法对两个字符串进行比较的。

174
00:11:50,753 --> 00:11:51,953
它们不是同一回事。

175
00:11:51,953 --> 00:11:56,696
这就突显了在这里使用语言模型进行评估的重要性。

176
00:11:56,696 --> 00:11:58,057
你有这些答案，

177
00:11:58,717 --> 00:12:01,639
它们可能是任意的字符串。

178
00:12:01,639 --> 00:12:05,982
答案不是唯一的，

179
00:12:05,982 --> 00:12:07,944
有很多不同的变体。

180
00:12:07,944 --> 00:12:10,700
只要它们意思相同，

181
00:12:10,701 --> 00:12:13,267
它们就应该被看做是相似的。

182
00:12:13,267 --> 00:12:16,970
这就是语言模型的作用，而不仅仅是进行精确匹配。

183
00:12:17,532 --> 00:12:24,718
之所以对语言模型的评估如此困难，就是因为很难对字符串进行比较。

184
00:12:24,718 --> 00:12:29,062
我们将语言模型用于这些非常开放的任务，让它们生成文本。

185
00:12:29,062 --> 00:12:34,206
这在模型上从来没有真正做过，直到最近模型变的足够好了，才好来做这个。

186
00:12:34,206 --> 00:12:38,570
目前为止很多现存的评估指标都不够好。

187
00:12:38,690 --> 00:12:42,992
我们不得不发明新的指标和新的启发式方法。

188
00:12:42,992 --> 00:12:48,241
目前最有趣和最受欢迎的启发式方法实际上是

189
00:12:48,242 --> 00:12:50,596
使用语言模型进行评估。

190
00:12:50,596 --> 00:12:56,438
这节关于评估的课程到此就结束了，但我还想给你们展示一下LangChain的评估平台。

191
00:12:56,438 --> 00:13:02,381
借助评估平台可以在Notebook中完成我们刚做的所有事情，并且可以在UI中显示，并对数据持久化。

192
00:13:02,381 --> 00:13:03,582
那么让我们看看。

193
00:13:04,607 --> 00:13:06,128
在这里，可以看到有一个会话。

194
00:13:06,128 --> 00:13:08,268
我们称之为"deeplearningai"。

195
00:13:08,268 --> 00:13:14,411
可以看到实际上已经保存了我们在Notebook中运行的所有记录。

196
00:13:14,411 --> 00:13:17,886
这是一个跟踪输入和输出的好方法，

197
00:13:17,943 --> 00:13:22,654
也是一个查看底层究竟发生了什么的好方法。

198
00:13:22,654 --> 00:13:28,258
这与我们在Notebook中打开调试模式时打印出的信息相同，

199
00:13:28,259 --> 00:13:31,917
但它在UI中以更好的方式呈现。

200
00:13:32,497 --> 00:13:36,078
我们可以看到链的输入和每个步骤中链的输出。

201
00:13:36,078 --> 00:13:39,290
然后我们可以进一步深入链中，

202
00:13:39,490 --> 00:13:43,240
了解更多实际传入的信息。

203
00:13:43,240 --> 00:13:45,329
如果我们一直到最底层，

204
00:13:45,330 --> 00:13:48,461
可以看到究竟向聊天模型传入了什么内容。

205
00:13:48,461 --> 00:13:50,582
这里有系统消息。

206
00:13:50,582 --> 00:13:52,702
这里有人类的问题。

207
00:13:52,702 --> 00:13:54,783
这里有来自聊天模型的返回结果。

208
00:13:54,783 --> 00:13:56,183
还有一些元数据的输出。

209
00:13:57,462 --> 00:14:02,384
我们在这里添加的另一个功能是将这些示例添加到数据集中。

210
00:14:02,384 --> 00:14:06,171
如果你还有印象的话，在一开始创建那些示例数据集时，

211
00:14:06,172 --> 00:14:10,567
我们是通过手工和语言模型部分创建的。

212
00:14:10,567 --> 00:14:13,671
我们可以通过点击这个小按钮将其添加到数据集中，

213
00:14:13,672 --> 00:14:18,430
现在我们有了输入问题和输出结果。

214
00:14:18,430 --> 00:14:20,010
这样我们就可以创建一个数据集。

215
00:14:20,010 --> 00:14:22,431
我们可以给它命名为"deeplearning"。

216
00:14:25,371 --> 00:14:28,413
然后我们可以开始向这个数据集添加示例。

217
00:14:28,413 --> 00:14:32,600
回到我们在课程开始时解决的原始问题，

218
00:14:32,601 --> 00:14:36,356
我们需要创建这些数据集以便进行评估。

219
00:14:36,356 --> 00:14:40,059
这是一种在后台运行的好方法，

220
00:14:40,259 --> 00:14:44,529
然后随着时间的推移向示例数据集中添加内容并开始积累这些示例，

221
00:14:44,530 --> 00:14:49,503
这样你就可以开始将这些数据集用于评估，并让评估的飞轮开始转动起来。
