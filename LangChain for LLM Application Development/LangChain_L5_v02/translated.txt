当使用LLM构建复杂应用程序时，
一个重要但有时有点棘手的步骤是
如何评估应用程序的表现？
它是否达到了某种验收标准？
此外，如果你决定换一种实现方式，
可能换到不同的LLM，
或者更改如何使用向量数据库的策略，或者使用其他方式检索数据，
或者更改系统中的其他参数。
你要怎么样知道结果是比以前更好了？还是更糟糕了？
在这个视频中，Harrison将深入探讨一些框架，以思考如何评估基于LLM的应用程序，
同时会介绍一些帮助你评估的工具。
这些应用实际上是许多不同步骤的链和序列。
我们的首要任务是
了解每个步骤的输入和输出到底是什么。
因此，我们会用到一些可视化工具和调试工具。
使用大量不同的数据集来测试模型，这有助于我们全面了解模型的表现。
观察事物的一种方法是用肉眼看。
但还有一个非常酷的想法，就是使用语言模型和链来评估其他语言模型、其他链和其他应用程序。
我们也会深入探讨这个想法。
这个视频中有很多有趣的话题，
我发现随着越来越多的开发转向基于Prompt的开发，使用LLM开发应用程序，
整个工作流程、评估过程正在被重新思考。
这个视频里有很多激动人心的概念。
让我们开始吧。
好的，让我们开始设置评估所需要的环境。
首先，我们需要有一个链或应用程序，以便进行评估。
我们将使用上一课的文档问答链。
所以我们要导入所需的相关库。
我们将加载上一课中使用过的相同数据。
我们将用一行代码创建索引。
然后，我们将通过指定语言模型、链类型、检索器以及verbosity（要打印日志的详细程度）这些参数来创建RetrievalQA链。
现在我们有了这个应用程序。
我们需要做的第一件事就是
真正搞清楚我们需要用什么样的数据集来评估。
我将介绍几种不同的方法来实现这个目标。
第一个方法最简单，
就是自己想出一些好的示例数据集。
为了做到这一点，我们可以查看一些数据并提出示例问题
然后提供可以用于评估的标准答案。
让我们看一下这里的几份文档，
可以了解到文档里面都有什么。
看起来第一个文档是关于一套连帽衫的。
第二个文档是关于一件夹克的，都有很多详细信息。
从这些详细信息中，我们可以创建若干组示例问题和答案。
第一个问题，我们可以简单地问：“舒适保暖连帽衫套装有侧口袋吗？”
我们可以通过上面看到，它确实有侧口袋。
对于第二个，我们可以看到这件夹克来自某个系列，DownTek系列。
所以我们可以问这个问题："这件夹克来自哪个系列？"
答案是："DownTek系列"。
所以在这里我们创建了两组问答示例。
但是，这种方式并不易于扩展。
需要花费一些时间来查看每个示例，弄清楚发生了什么。
那么有没有办法让我们自动化这个过程？
借助语言模型来自动化这个过程是一种非常酷的方式。
在LangChain中有一个链可以做到这一点。
我们可以导入QAGenerationChain
它可以读取文档并从每个文档中生成一组问题和答案。
它将借助语言模型来实现。
因此，我们需要通过传入ChatOpenAI语言模型来创建这个链。
然后我们可以用它创建很多组问题和答案示例。
我们将使用apply_and_parse方法，应用解析器来解析输出的结果。
因为我们想要得到一系列包含一组问题和答案的字典对象，而不是一个文本字符串。
现在如果我们看一下返回的内容，
我们可以看到一个问题和一个答案。
让我们检查一下这是哪个文档的问题和答案。
我们可以看到它在问这个的重量是多少。
我们可以看到它是从这里获取重量的。
看看这个，
我们刚刚生成了很多组问题和答案，
不需要去一个个手动生成。
这样可以帮我们节省很多时间，可以去做更多其他有意思的事情。
现在让我们把这些示例加入到我们已经创建的示例中。
现在我们有了这些示例数据，但是我们如何评估效果如何？
首先，我们要做的就是将其中某个示例传入链并运行，然后观察它输出的结果。
这里我们输入一个问题，然后得到返回结果。
但我们无法观察到链的内部到底发生了什么！
例如传给语言模型中的Prompt是什么？
它检索到的文档有哪些？
如果这是一个包含多个步骤的复杂链，那么每一步的中间结果是什么？
仅仅观察最终答案通常不足以理解
链中哪里出错或可能出错。
为了解决这个问题，LangChain中提供了一个有意思的小工具，叫做“langchain debug”。
如果我们把 "langchain.debug" 设为 "True",
然后把之前的示例再运行一遍，
就可以看到它输出了更多的信息。
如果我们看看它到底输出了什么，
我们可以看到它首先调用了RetrievalQA链，
然后它又调用了StuffDocumentsChain。
上一节课讲到过，我们使用了 "stuff" 方法。
现在它调用了LLMChain，有几个不同的输入参数：
可以看到原始问题，
传入的上下文，
可以看到这个上下文是根据问题检索到的几个文档块内容。
在做问答时，
往往当返回错误的结果时，不一定是语言模型本身出了问题。
实际上，可能是在检索的步骤出了问题。
仔细查看问题的确切内容
和上下文的确切内容可以帮助调试，找出问题在哪。
我们可以再深入一层，看看究竟传入语言模型（ChatOpenAI）的Prompt究竟是什么。
这里我们可以看到传入语言模型的完整Prompt：
有一个系统消息。
有一个对Prompt的描述。
这就是问答链在底层使用的Prompt，
我们直到现在才看到。
我们可以看到Prompt输出：“使用以下上下文信息来回答用户的问题，
如果你不知道答案，就说不知道，
不要试图编造答案。”
然后我们看到之前插入的一堆上下文。
接着我们看到一个人类提出的问题，这就是我们问它的问题。
我们还可以看到更多语言模型返回的结果信息。
所以不仅仅是一个字符串，我们还得到了诸如Token使用量这样的信息，
比如Prompt消耗了多少Token、返回的结果消耗了多少Token、总共消耗的Token数和模型名称。
这些信息对于跟踪在链或调用语言模型中使用了多少Token非常有用，
根据消耗的Token数和模型，可以算出来花费了多少成本。
因为这是一个相对简单的链，
我们现在可以看到最后的返回结果：
“舒适保暖连帽衫套装，条纹款，有侧口袋。”，
通过链传递并返回给用户。
我们刚刚讲解了如何查看和调试
这个链中单个输入的情况。
但是如何输入我们前面创建的所有示例呢？
我们该如何评估它们呢？
和创建示例数据的方法类似，一种方法是手动操作。
我们可以在所有示例上运行链，
然后观察输出，搞清楚发生了什么，是否正确、错误或部分正确。
与创建示例数据类似，手动操作的方法随着时间的推移会变得有些繁琐。
所以让我们回到大家最喜欢的解决方案。
能让语言模型来完成吗？
首先，我们需要为所有示例生成实际答案。
在此之前，
我实际上要关闭调试模式，以免把所有内容都打印出来，
然后我将为所有不同的示例生成实际答案。
我想我们总共有七个示例，
我们将循环七次，为每个示例生成一个实际答案。
现在我们有了这些示例，可以考虑评估它们。
我们将导入QAEvalChain。
我们将用语言模型创建这个链，
因为我们将使用语言模型来帮助评估。
然后我们将在这个链上调用"evaluate"方法。
并传入示例列表和实际答案列表，
然后我们将得到与这组示例列表相对应的一组评估打分结果，并保存到了"graded_outputs"变量。
为了查看每个示例的具体情况，
我们将遍历它们。
我们要打印出问题。
再次说明，这是由语言模型生成的。
我们要打印出标准答案。
同样，这是语言模型基于整个文档的内容生成的，
所以它生成的标准答案是可靠的。
我们要打印出实际答案。
这是在语言模型和问答链生成的，
先对问题生成Embedding，然后去向量数据库检索相似文档，再将检索出来的文档传递给语言模型，
然后语言模型生成实际答案。
我们还会打印出评分。
再次强调，这也是由语言模型生成的
评估链对标准答案和实际答案进行对比，判断对错，得出一个评分。
所以当我们遍历所有这些示例并打印它们时，
可以详细查看每个示例。
看起来每一个示例都做对了。
这是一个相对简单的检索问题，所以这结果还是靠得住的。
那我们来看第一个示例。
这里的问题是：“舒适保暖套装有侧面口袋吗？”
我们创建的标准答案是：“是的”。
语言模型生成的实际答案是：
“舒适保暖套装，条纹款确实有侧面口袋。”
所以我们可以认为这个答案是正确的。
实际上，语言模型也知道，并且它把它标记为正确。
但让我们想想为什么我们需要使用语言模型。
因为这两个字符串实际上一点都不像。
它们非常不同。
一个很短，一个很长。
“是的”这个词在这个长字符串里都没有出现。
所以如果我们试图进行字符串匹配、精确匹配或者使用正则表达式，
是无法对两个字符串进行比较的。
它们不是同一回事。
这就突显了在这里使用语言模型进行评估的重要性。
你有这些答案，
它们可能是任意的字符串。
答案不是唯一的，
有很多不同的变体。
只要它们意思相同，
它们就应该被看做是相似的。
这就是语言模型的作用，而不仅仅是进行精确匹配。
之所以对语言模型的评估如此困难，就是因为很难对字符串进行比较。
我们将语言模型用于这些非常开放的任务，让它们生成文本。
这在模型上从来没有真正做过，直到最近模型变的足够好了，才好来做这个。
目前为止很多现存的评估指标都不够好。
我们不得不发明新的指标和新的启发式方法。
目前最有趣和最受欢迎的启发式方法实际上是
使用语言模型进行评估。
这节关于评估的课程到此就结束了，但我还想给你们展示一下LangChain的评估平台。
借助评估平台可以在Notebook中完成我们刚做的所有事情，并且可以在UI中显示，并对数据持久化。
那么让我们看看。
在这里，可以看到有一个会话。
我们称之为"deeplearningai"。
可以看到实际上已经保存了我们在Notebook中运行的所有记录。
这是一个跟踪输入和输出的好方法，
也是一个查看底层究竟发生了什么的好方法。
这与我们在Notebook中打开调试模式时打印出的信息相同，
但它在UI中以更好的方式呈现。
我们可以看到链的输入和每个步骤中链的输出。
然后我们可以进一步深入链中，
了解更多实际传入的信息。
如果我们一直到最底层，
可以看到究竟向聊天模型传入了什么内容。
这里有系统消息。
这里有人类的问题。
这里有来自聊天模型的返回结果。
还有一些元数据的输出。
我们在这里添加的另一个功能是将这些示例添加到数据集中。
如果你还有印象的话，在一开始创建那些示例数据集时，
我们是通过手工和语言模型部分创建的。
我们可以通过点击这个小按钮将其添加到数据集中，
现在我们有了输入问题和输出结果。
这样我们就可以创建一个数据集。
我们可以给它命名为"deeplearning"。
然后我们可以开始向这个数据集添加示例。
回到我们在课程开始时解决的原始问题，
我们需要创建这些数据集以便进行评估。
这是一种在后台运行的好方法，
然后随着时间的推移向示例数据集中添加内容并开始积累这些示例，
这样你就可以开始将这些数据集用于评估，并让评估的飞轮开始转动起来。