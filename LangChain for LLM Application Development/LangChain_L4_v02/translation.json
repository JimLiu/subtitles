{
  "chunks": [
    {
      "items": [
        {
          "id": "1",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 5,
            "milliseconds": 0
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 15,
            "milliseconds": 240
          },
          "text": "One of the most common complex applications that people are building using an LLM is a system that can answer questions on top of or about a document."
        },
        {
          "id": "2",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 15,
            "milliseconds": 240
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 23,
            "milliseconds": 886
          },
          "text": "So given a piece of text may be extracted from a PDF file or from a webpage or from some company's intranet internal document collection,"
        },
        {
          "id": "3",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 23,
            "milliseconds": 914
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 33,
            "milliseconds": 460
          },
          "text": "can you use an LLM to answer questions about the content of those documents to help users gain a deeper understanding and get access to the information that they need?"
        },
        {
          "id": "4",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 33,
            "milliseconds": 460
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 39,
            "milliseconds": 300
          },
          "text": "This is really powerful because it starts to combine these language models with data that they weren't originally trained on."
        },
        {
          "id": "5",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 39,
            "milliseconds": 300
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 42,
            "milliseconds": 540
          },
          "text": "So it makes them much more flexible and adaptable to your use case."
        },
        {
          "id": "6",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 42,
            "milliseconds": 540
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 51,
            "milliseconds": 543
          },
          "text": "It's also really exciting because we'll start to move beyond language models, prompts, and output parsers and start introducing some more of the key components of LangChain,"
        },
        {
          "id": "7",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 51,
            "milliseconds": 557
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 54,
            "milliseconds": 580
          },
          "text": "such as embedding models and vector stores."
        },
        {
          "id": "8",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 54,
            "milliseconds": 580
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 58,
            "milliseconds": 620
          },
          "text": "As Andrew mentioned, this is one of the more popular chains that we've got, so I hope you're excited."
        },
        {
          "id": "9",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 58,
            "milliseconds": 620
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 3,
            "milliseconds": 580
          },
          "text": "In fact, embeddings and vector stores are some of the most powerful modern techniques."
        },
        {
          "id": "10",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 3,
            "milliseconds": 580
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 8,
            "milliseconds": 300
          },
          "text": "So if you have not seen them yet, they are very much worth learning about."
        }
      ],
      "source": [
        "One of the most common complex applications that people are building using an LLM is a system that can answer questions on top of or about a document.",
        "So given a piece of text may be extracted from a PDF file or from a webpage or from some company's intranet internal document collection,",
        "can you use an LLM to answer questions about the content of those documents to help users gain a deeper understanding and get access to the information that they need?",
        "This is really powerful because it starts to combine these language models with data that they weren't originally trained on.",
        "So it makes them much more flexible and adaptable to your use case.",
        "It's also really exciting because we'll start to move beyond language models, prompts, and output parsers and start introducing some more of the key components of LangChain,",
        "such as embedding models and vector stores.",
        "As Andrew mentioned, this is one of the more popular chains that we've got, so I hope you're excited.",
        "In fact, embeddings and vector stores are some of the most powerful modern techniques.",
        "So if you have not seen them yet, they are very much worth learning about."
      ],
      "result": [
        "文档问答系统是一种常见的用LLM构建的复杂应用程序。",
        "给定一段可能从PDF文件、网页或某公司的内部文档库中提取的文本，",
        "你能否使用LLM回答关于这些文档内容的问题，帮助用户深入了解并获取他们需要的信息？",
        "这样的应用非常强大，因为它可以将大语言模型与完全没被训练的数据相结合。",
        "这使得它们可以灵活的适应你的应用场景。",
        "这也非常令人兴奋，因为我们将开始超越语言模型、Prompt和输出解析器，引入LangChain的更多关键组件，",
        "例如Embedding模型和向量存储（Vector Stores）。",
        "正如Andrew提到的，这是LangChain最受欢迎的链之一，希望你能对这个有兴趣。",
        "事实上，Embedding和向量存储是两种强大的前沿技术。",
        "如果你还没有了解过这些技术，那么绝对值得学习。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "11",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 8,
            "milliseconds": 300
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 10,
            "milliseconds": 60
          },
          "text": "So with that, let's dive in."
        },
        {
          "id": "12",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 10,
            "milliseconds": 60
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 11,
            "milliseconds": 60
          },
          "text": "Let's do it."
        },
        {
          "id": "13",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 11,
            "milliseconds": 60
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 16,
            "milliseconds": 300
          },
          "text": "So we're going to start by importing the environment variables as we always do."
        },
        {
          "id": "14",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 16,
            "milliseconds": 300
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 20,
            "milliseconds": 60
          },
          "text": "Now we're going to import some things that will help us when building this chain."
        },
        {
          "id": "15",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 20,
            "milliseconds": 60
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 24,
            "milliseconds": 340
          },
          "text": "We're going to import the RetrievalQA chain. This will do retrieval over some documents."
        },
        {
          "id": "16",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 24,
            "milliseconds": 340
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 28,
            "milliseconds": 100
          },
          "text": "We're going to import our favorite ChatOpenAI language model."
        },
        {
          "id": "17",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 28,
            "milliseconds": 100
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 29,
            "milliseconds": 700
          },
          "text": "We're going to import a document loader."
        },
        {
          "id": "18",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 29,
            "milliseconds": 700
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 34,
            "milliseconds": 500
          },
          "text": "This is going to be used to load some proprietary data that we're going to combine with the language model."
        },
        {
          "id": "19",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 34,
            "milliseconds": 500
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 36,
            "milliseconds": 380
          },
          "text": "In this case, it's going to be in a CSV."
        },
        {
          "id": "20",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 36,
            "milliseconds": 380
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 38,
            "milliseconds": 380
          },
          "text": "So we're going to import the CSVLoader."
        }
      ],
      "source": [
        "So with that, let's dive in.",
        "Let's do it.",
        "So we're going to start by importing the environment variables as we always do.",
        "Now we're going to import some things that will help us when building this chain.",
        "We're going to import the RetrievalQA chain. This will do retrieval over some documents.",
        "We're going to import our favorite ChatOpenAI language model.",
        "We're going to import a document loader.",
        "This is going to be used to load some proprietary data that we're going to combine with the language model.",
        "In this case, it's going to be in a CSV.",
        "So we're going to import the CSVLoader."
      ],
      "result": [
        "那让我们开始吧！",
        "开始吧！",
        "首先，我们会像往常一样导入环境变量。",
        "现在我们要导入一些在构建这个链时需要用到的库。",
        "我们将导入RetrievalQA链，它可以帮助检索文档。",
        "我们将导入大家都喜欢的ChatOpenAI语言模型。",
        "我们要导入一个文档加载器，",
        "后面会用它加载一些专用数据，用来与语言模型结合使用。",
        "在这里我们要加载的是一个CSV文件，",
        "所以要导入CSVLoader。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "21",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 38,
            "milliseconds": 380
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 41,
            "milliseconds": 220
          },
          "text": "Finally, we're going to import a vector store."
        },
        {
          "id": "22",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 41,
            "milliseconds": 220
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 48,
            "milliseconds": 900
          },
          "text": "There are many different types of vector stores, and we'll cover what exactly these are later on, but we're going to get started with the DocArrayInMemorySearch vector store."
        },
        {
          "id": "23",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 48,
            "milliseconds": 900
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 57,
            "milliseconds": 580
          },
          "text": "This is really nice because it's an in-memory vector store, and it doesn't require connecting to an external database of any kind, so it makes it really easy to get started."
        },
        {
          "id": "24",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 57,
            "milliseconds": 580
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 4,
            "milliseconds": 500
          },
          "text": "We're also going to import display and markdown, two common utilities for displaying information in Jupyter Notebooks."
        },
        {
          "id": "25",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 4,
            "milliseconds": 500
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 10,
            "milliseconds": 500
          },
          "text": "We've provided a CSV of outdoor clothing that we're going to use to combine with the language model."
        },
        {
          "id": "26",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 10,
            "milliseconds": 500
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 18,
            "milliseconds": 180
          },
          "text": "Here, we're going to initialize a loader, the CSVLoader, with a path to this file."
        },
        {
          "id": "27",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 18,
            "milliseconds": 180
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 22,
            "milliseconds": 800
          },
          "text": "We're next going to import an index, the VectorStoreIndexCreator."
        },
        {
          "id": "28",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 22,
            "milliseconds": 800
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 25,
            "milliseconds": 980
          },
          "text": "This will help us create a vector store really easily."
        },
        {
          "id": "29",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 25,
            "milliseconds": 980
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 33,
            "milliseconds": 940
          },
          "text": "As we can see below, it'll only be a few lines of code to create this."
        },
        {
          "id": "30",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 33,
            "milliseconds": 940
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 37,
            "milliseconds": 20
          },
          "text": "To create it, we're going to specify two things."
        }
      ],
      "source": [
        "Finally, we're going to import a vector store.",
        "There are many different types of vector stores, and we'll cover what exactly these are later on, but we're going to get started with the DocArrayInMemorySearch vector store.",
        "This is really nice because it's an in-memory vector store, and it doesn't require connecting to an external database of any kind, so it makes it really easy to get started.",
        "We're also going to import display and markdown, two common utilities for displaying information in Jupyter Notebooks.",
        "We've provided a CSV of outdoor clothing that we're going to use to combine with the language model.",
        "Here, we're going to initialize a loader, the CSVLoader, with a path to this file.",
        "We're next going to import an index, the VectorStoreIndexCreator.",
        "This will help us create a vector store really easily.",
        "As we can see below, it'll only be a few lines of code to create this.",
        "To create it, we're going to specify two things."
      ],
      "result": [
        "最后，我们要导入一个向量存储。",
        "有很多不同类型的向量存储，我们稍后会介绍它是什么，让我们先从向量存储DocArrayInMemorySearch开始。",
        "这个向量存储非常好用，因为它是内存存储，不需要连接任何外部数据库，所以很容易上手。",
        "我们还将导入\"display\"和\"Markdown\"，常用来在Jupyter Notebook中显示信息。",
        "我们提供了一个户外服装产品目录的CSV文件，会用它和语言模型结合使用。",
        "在这里，我们将初始化一个名叫CSVLoader的加载器（Loader），并为其指定文件路径。",
        "接下来，我们要导入一个叫\\NVectorStoreIndexCreator\\N的索引（Index）。",
        "借助它们，我们可以很方便地创建一个向量存储。",
        "如下所示，只需几行代码就可以创建。",
        "要创建向量存储，我们需要指定两件事。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "31",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 37,
            "milliseconds": 20
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 40,
            "milliseconds": 540
          },
          "text": "First, we're going to specify the vector store class."
        },
        {
          "id": "32",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 40,
            "milliseconds": 540
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 46,
            "milliseconds": 100
          },
          "text": "As mentioned before, we're going to use this vector store, as it's a particularly easy one to get started with."
        },
        {
          "id": "33",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 46,
            "milliseconds": 100
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 51,
            "milliseconds": 820
          },
          "text": "After it's been created, we're then going to call from loaders, which takes in a list of document loaders."
        },
        {
          "id": "34",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 51,
            "milliseconds": 820
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 58,
            "milliseconds": 460
          },
          "text": "We've only got one loader that we really care about, so that's what we're passing in here."
        },
        {
          "id": "35",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 58,
            "milliseconds": 460
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 2,
            "milliseconds": 220
          },
          "text": "It's now been created, and we can start to ask questions about it."
        },
        {
          "id": "36",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 2,
            "milliseconds": 220
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 6,
            "milliseconds": 900
          },
          "text": "Below we'll cover what exactly happened under the hood, so let's not worry about that for now."
        },
        {
          "id": "37",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 6,
            "milliseconds": 900
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 9,
            "milliseconds": 140
          },
          "text": "Here, we'll start with a query."
        },
        {
          "id": "38",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 9,
            "milliseconds": 140
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 15,
            "milliseconds": 729
          },
          "text": "We'll then create a response using \"index.query\" and pass in this query."
        },
        {
          "id": "39",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 16,
            "milliseconds": 340
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 20,
            "milliseconds": 900
          },
          "text": "Again, we'll cover what's going on under the hood down below."
        },
        {
          "id": "40",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 20,
            "milliseconds": 900
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 23,
            "milliseconds": 100
          },
          "text": "For now, we'll just wait for it to respond."
        }
      ],
      "source": [
        "First, we're going to specify the vector store class.",
        "As mentioned before, we're going to use this vector store, as it's a particularly easy one to get started with.",
        "After it's been created, we're then going to call from loaders, which takes in a list of document loaders.",
        "We've only got one loader that we really care about, so that's what we're passing in here.",
        "It's now been created, and we can start to ask questions about it.",
        "Below we'll cover what exactly happened under the hood, so let's not worry about that for now.",
        "Here, we'll start with a query.",
        "We'll then create a response using \"index.query\" and pass in this query.",
        "Again, we'll cover what's going on under the hood down below.",
        "For now, we'll just wait for it to respond."
      ],
      "result": [
        "第一件事是我们要指定一个向量存储类。",
        "如前所述，我们将使用这个向量存储，因为它特别容易上手。",
        "第二件事是，创建后我们将调用文档加载器(Loader)，传入一个包含一个或多个加载器的列表。",
        "我们只需要一个加载器，也就是传入的这个。",
        "现在已经创建好了向量存储，我们可以开始提问了。",
        "稍后将会向你介绍这背后到底发生了什么。",
        "我们先定义一个包含查询内容的字符串变量\"query\"。（内容是：“请在表格中列出你所有带防晒功能的衬衫，并为这些衬衫写一份摘要。”）",
        "然后我们用\"index.query\"生成一个响应，并传入\"query\"变量。",
        "同样，稍后将会向你介绍这背后到底发生了什么",
        "现在，我们来等等看它返回什么结果。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "41",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 30,
            "milliseconds": 380
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 34,
            "milliseconds": 540
          },
          "text": "After it finishes, we can now take a look at what exactly was returned."
        },
        {
          "id": "42",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 34,
            "milliseconds": 540
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 40,
            "milliseconds": 780
          },
          "text": "We've gotten back a table in Markdown with names and descriptions for all shirts with sun protection."
        },
        {
          "id": "43",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 40,
            "milliseconds": 780
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 45,
            "milliseconds": 540
          },
          "text": "We've also got a nice little summary that the language model has provided us."
        },
        {
          "id": "44",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 45,
            "milliseconds": 540
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 52,
            "milliseconds": 180
          },
          "text": "So we've gone over how to do question answering over your documents, but what exactly is going on underneath the hood?"
        },
        {
          "id": "45",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 52,
            "milliseconds": 180
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 54,
            "milliseconds": 340
          },
          "text": "First let's think about the general idea."
        },
        {
          "id": "46",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 54,
            "milliseconds": 340
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 59,
            "milliseconds": 900
          },
          "text": "We want to use language models and combine it with a lot of our documents, but there's a key issue."
        },
        {
          "id": "47",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 59,
            "milliseconds": 900
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 3,
            "milliseconds": 460
          },
          "text": "Language models can only inspect a few thousand words at a time."
        },
        {
          "id": "48",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 3,
            "milliseconds": 460
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 10,
            "milliseconds": 300
          },
          "text": "So if we have really large documents, how can we get the language model to answer questions about everything that's in there?"
        },
        {
          "id": "49",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 10,
            "milliseconds": 300
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 14,
            "milliseconds": 460
          },
          "text": "This is where embeddings and vector stores come into play."
        },
        {
          "id": "50",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 14,
            "milliseconds": 460
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 17,
            "milliseconds": 780
          },
          "text": "First let's talk about embeddings."
        }
      ],
      "source": [
        "After it finishes, we can now take a look at what exactly was returned.",
        "We've gotten back a table in Markdown with names and descriptions for all shirts with sun protection.",
        "We've also got a nice little summary that the language model has provided us.",
        "So we've gone over how to do question answering over your documents, but what exactly is going on underneath the hood?",
        "First let's think about the general idea.",
        "We want to use language models and combine it with a lot of our documents, but there's a key issue.",
        "Language models can only inspect a few thousand words at a time.",
        "So if we have really large documents, how can we get the language model to answer questions about everything that's in there?",
        "This is where embeddings and vector stores come into play.",
        "First let's talk about embeddings."
      ],
      "result": [
        "在完成后，我们可以看看到底返回了什么。",
        "我们得到了一个Markdown格式的表格，列出了所有具有防晒功能的衬衫的名称和描述。",
        "我们还得到了一个很好的小结，是语言模型为我们生成的。",
        "我们已经介绍了如何对文档中的内容进行问答，但底层到底是怎么实现的呢？",
        "首先让我们来梳理一下思路。",
        "想要使用语言模型，并且与大量文档相结合，存在一个关键问题：",
        "语言模型一次只能接收几千个单词。",
        "那么，如果我们有一个很大的文档，如何让语言模型对文档所有内容进行问答呢？",
        "这里就需要Embedding和向量存储发挥作用了。",
        "首先让我们谈谈Embedding。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "51",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 17,
            "milliseconds": 780
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 21,
            "milliseconds": 740
          },
          "text": "Embeddings create numerical representations for pieces of text."
        },
        {
          "id": "52",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 21,
            "milliseconds": 740
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 28,
            "milliseconds": 0
          },
          "text": "This numerical representation captures the semantic meaning of the piece of text that it's been run over."
        },
        {
          "id": "53",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 28,
            "milliseconds": 0
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 31,
            "milliseconds": 920
          },
          "text": "Pieces of text with similar content will have similar vectors."
        },
        {
          "id": "54",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 31,
            "milliseconds": 920
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 35,
            "milliseconds": 300
          },
          "text": "This lets us compare pieces of text in the vector space."
        },
        {
          "id": "55",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 35,
            "milliseconds": 300
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 38,
            "milliseconds": 940
          },
          "text": "In the example below, we can see that we have three sentences."
        },
        {
          "id": "56",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 38,
            "milliseconds": 940
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 43,
            "milliseconds": 220
          },
          "text": "The first two are about pets, while the third is about a car."
        },
        {
          "id": "57",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 43,
            "milliseconds": 220
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 54,
            "milliseconds": 180
          },
          "text": "If we look at the representation in the numeric space, we can see that when we compare the two vectors on the pieces of text corresponding to the sentences about pets, they're very similar."
        },
        {
          "id": "58",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 54,
            "milliseconds": 180
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 58,
            "milliseconds": 260
          },
          "text": "While if we compare it to the one that talks about a car, they're not similar at all."
        },
        {
          "id": "59",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 58,
            "milliseconds": 260
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 2,
            "milliseconds": 100
          },
          "text": "This will let us easily figure out which pieces of text are like each other,"
        },
        {
          "id": "60",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 2,
            "milliseconds": 101
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 9,
            "milliseconds": 940
          },
          "text": "which will be very useful as we think about which pieces of text we want to include when passing to the language model to answer a question."
        }
      ],
      "source": [
        "Embeddings create numerical representations for pieces of text.",
        "This numerical representation captures the semantic meaning of the piece of text that it's been run over.",
        "Pieces of text with similar content will have similar vectors.",
        "This lets us compare pieces of text in the vector space.",
        "In the example below, we can see that we have three sentences.",
        "The first two are about pets, while the third is about a car.",
        "If we look at the representation in the numeric space, we can see that when we compare the two vectors on the pieces of text corresponding to the sentences about pets, they're very similar.",
        "While if we compare it to the one that talks about a car, they're not similar at all.",
        "This will let us easily figure out which pieces of text are like each other,",
        "which will be very useful as we think about which pieces of text we want to include when passing to the language model to answer a question."
      ],
      "result": [
        "Embedding将一段文本转换成数字，用一组数字表示这段文本。",
        "这组数字捕捉了它所代表的文字片段的内容含义。",
        "内容相似的文本片段会有相似的向量值。",
        "这样我们就可以在向量空间中比较文本片段。",
        "在下面的例子中，我们可以看到有三个句子，",
        "前两个是关于宠物的，而第三个是关于汽车的。",
        "如果我们观察数值空间中的表示，可以看到当我们比较关于两个关于宠物的句子的向量时，它们相似度非常高。",
        "而如果我们将其中一个与汽车相关的那个句子向量进行比较，它们相似度很低。",
        "利用向量相似度可以让我们轻松地找出哪些文本片段相似，",
        "这对我们非常有用，因为我们可以利用这种技术从文档中找出跟问题相似的文本片段，一起传递给语言模型来帮助回答问题。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "61",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 9,
            "milliseconds": 940
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 13,
            "milliseconds": 20
          },
          "text": "The next component that we're going to cover is the vector database."
        },
        {
          "id": "62",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 13,
            "milliseconds": 20
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 18,
            "milliseconds": 60
          },
          "text": "A vector database is a way to store these vector representations that we created in the previous step."
        },
        {
          "id": "63",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 18,
            "milliseconds": 60
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 23,
            "milliseconds": 780
          },
          "text": "The way that we create this vector database is we populate it with chunks of text coming from incoming documents."
        },
        {
          "id": "64",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 23,
            "milliseconds": 780
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 29,
            "milliseconds": 20
          },
          "text": "When we get a big incoming document, we're first going to break it up into smaller chunks."
        },
        {
          "id": "65",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 29,
            "milliseconds": 20
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 33,
            "milliseconds": 429
          },
          "text": "This helps create pieces of text that are smaller than the original document,"
        },
        {
          "id": "66",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 33,
            "milliseconds": 457
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 37,
            "milliseconds": 900
          },
          "text": "which is useful because we may not be able to pass the whole document to the language model."
        },
        {
          "id": "67",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 37,
            "milliseconds": 900
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 43,
            "milliseconds": 300
          },
          "text": "So we want to create these small chunks so we can only pass the most relevant ones to the language model."
        },
        {
          "id": "68",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 43,
            "milliseconds": 300
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 48,
            "milliseconds": 940
          },
          "text": "We then create an embedding for each of these chunks, and then we store those in a vector database."
        },
        {
          "id": "69",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 48,
            "milliseconds": 940
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 51,
            "milliseconds": 640
          },
          "text": "That's what happens when we create the index."
        },
        {
          "id": "70",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 51,
            "milliseconds": 640
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 58,
            "milliseconds": 420
          },
          "text": "Now that we've got this index, we can use it during runtime to find the pieces of text most relevant to an incoming query."
        }
      ],
      "source": [
        "The next component that we're going to cover is the vector database.",
        "A vector database is a way to store these vector representations that we created in the previous step.",
        "The way that we create this vector database is we populate it with chunks of text coming from incoming documents.",
        "When we get a big incoming document, we're first going to break it up into smaller chunks.",
        "This helps create pieces of text that are smaller than the original document,",
        "which is useful because we may not be able to pass the whole document to the language model.",
        "So we want to create these small chunks so we can only pass the most relevant ones to the language model.",
        "We then create an embedding for each of these chunks, and then we store those in a vector database.",
        "That's what happens when we create the index.",
        "Now that we've got this index, we can use it during runtime to find the pieces of text most relevant to an incoming query."
      ],
      "result": [
        "接下来我们要介绍的组件是向量数据库。",
        "向量数据库是一种存储方法，可以存储我们在前面创建的那种矢量数字数组。",
        "往向量数据库中新建数据的方式，就是将文档拆分成块，每块生成Embedding，然后把Embedding和原始块一起存储到数据库中。",
        "当我们在处理大一点的文档时，首先要将其拆分成较小的文本块，",
        "",
        "因为可能无法将整个文档的内容都传给语言模型。",
        "所以需要把文档拆分成小块，这样每次就只用把最相关的几块内容传递给语言模型。",
        "然后，把每个文本块生成一个Embedding，然后将这些Embedding存储在向量数据库中。",
        "这就是我们创建索引时发生的事情。",
        "索引创建后，我们可以用它来找到与查询内容最相关的几个文本片段。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "71",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 58,
            "milliseconds": 420
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 2,
            "milliseconds": 220
          },
          "text": "When a query comes in, we first create an embedding for that query."
        },
        {
          "id": "72",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 2,
            "milliseconds": 220
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 8,
            "milliseconds": 0
          },
          "text": "We then compare it to all the vectors in the vector database, and we pick the n most similar."
        },
        {
          "id": "73",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 8,
            "milliseconds": 0
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 14,
            "milliseconds": 380
          },
          "text": "These are then returned, and we can pass those in the prompt to the language model to get back a final answer."
        },
        {
          "id": "74",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 14,
            "milliseconds": 380
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 17,
            "milliseconds": 780
          },
          "text": "So above, we created this chain and only a few lines of code."
        },
        {
          "id": "75",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 17,
            "milliseconds": 780
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 19,
            "milliseconds": 180
          },
          "text": "That's great for getting started quickly."
        },
        {
          "id": "76",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 19,
            "milliseconds": 180
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 25,
            "milliseconds": 260
          },
          "text": "Well, let's now do it a bit more step by step and understand what exactly is going on under the hood."
        },
        {
          "id": "77",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 25,
            "milliseconds": 260
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 27,
            "milliseconds": 220
          },
          "text": "The first step is similar to above."
        },
        {
          "id": "78",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 27,
            "milliseconds": 220
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 36,
            "milliseconds": 560
          },
          "text": "We're going to create a document loader, loading from that CSV with all the descriptions of the products that we want to do question answering over."
        },
        {
          "id": "79",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 36,
            "milliseconds": 560
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 41,
            "milliseconds": 460
          },
          "text": "We can then load documents from this document loader."
        },
        {
          "id": "80",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 41,
            "milliseconds": 460
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 49,
            "milliseconds": 740
          },
          "text": "If we look at the individual documents, we can see that each document corresponds to one of the products in the CSV."
        }
      ],
      "source": [
        "When a query comes in, we first create an embedding for that query.",
        "We then compare it to all the vectors in the vector database, and we pick the n most similar.",
        "These are then returned, and we can pass those in the prompt to the language model to get back a final answer.",
        "So above, we created this chain and only a few lines of code.",
        "That's great for getting started quickly.",
        "Well, let's now do it a bit more step by step and understand what exactly is going on under the hood.",
        "The first step is similar to above.",
        "We're going to create a document loader, loading from that CSV with all the descriptions of the products that we want to do question answering over.",
        "We can then load documents from this document loader.",
        "If we look at the individual documents, we can see that each document corresponds to one of the products in the CSV."
      ],
      "result": [
        "当一个查询进来时，我们首先将查询的内容生成Embedding，得到一个数字数组。",
        "然后将这个数字数组与向量数据库中的所有向量进行比较，选择最相似的前若干个文本块。",
        "拿到这些文本块后，将这些文本块和原始的查询内容一起传递给语言模型，这样可以让语言模型根据检索出来的文档内容生成最终答案。",
        "在上面，我们用几行代码创建了这个链。",
        "这对于快速入门非常有帮助。",
        "好吧，现在让我们详细了解一下底层到底发生了什么。",
        "第一步与上面类似。",
        "我们将创建一个文档加载器，加载前面提到的产品描述的CSV文件，用做后面问答的文档数据。",
        "然后我们可以借助这个文档加载器中加载所有文档。",
        "如果查看单个文档，可以看到每个文档对应CSV文件中的一个产品。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "81",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 49,
            "milliseconds": 740
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 53,
            "milliseconds": 60
          },
          "text": "Previously, we talked about creating chunks."
        },
        {
          "id": "82",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 53,
            "milliseconds": 60
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 57,
            "milliseconds": 100
          },
          "text": "Because these documents are already so small, we actually don't need to do any chunking here."
        },
        {
          "id": "83",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 57,
            "milliseconds": 100
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 1,
            "milliseconds": 140
          },
          "text": "And so we can create embeddings directly."
        },
        {
          "id": "84",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 1,
            "milliseconds": 140
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 5,
            "milliseconds": 260
          },
          "text": "To create embeddings, we're going to use OpenAIEmbedding class."
        },
        {
          "id": "85",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 5,
            "milliseconds": 260
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 8,
            "milliseconds": 220
          },
          "text": "We can import it and initialize it here."
        },
        {
          "id": "86",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 8,
            "milliseconds": 221
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 15,
            "milliseconds": 600
          },
          "text": "If we want to see what these embeddings do, we can actually take a look at what happens when we embed a particular piece of text."
        },
        {
          "id": "87",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 21,
            "milliseconds": 180
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 26,
            "milliseconds": 200
          },
          "text": "Let's use the \"embed_query\" method on the embeddings object to create embeddings for a particular piece of text."
        },
        {
          "id": "88",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 26,
            "milliseconds": 200
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 28,
            "milliseconds": 660
          },
          "text": "In this case, the sentence, \"Hi, my name is Harrison.\""
        },
        {
          "id": "89",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 31,
            "milliseconds": 629
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 36,
            "milliseconds": 57
          },
          "text": "If we take a look at this embedding, we can see that there are over a thousand different elements."
        },
        {
          "id": "90",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 42,
            "milliseconds": 20
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 44,
            "milliseconds": 620
          },
          "text": "Each of these elements is a different numerical value."
        }
      ],
      "source": [
        "Previously, we talked about creating chunks.",
        "Because these documents are already so small, we actually don't need to do any chunking here.",
        "And so we can create embeddings directly.",
        "To create embeddings, we're going to use OpenAIEmbedding class.",
        "We can import it and initialize it here.",
        "If we want to see what these embeddings do, we can actually take a look at what happens when we embed a particular piece of text.",
        "Let's use the \"embed_query\" method on the embeddings object to create embeddings for a particular piece of text.",
        "In this case, the sentence, \"Hi, my name is Harrison.\"",
        "If we take a look at this embedding, we can see that there are over a thousand different elements.",
        "Each of these elements is a different numerical value."
      ],
      "result": [
        "之前我们讨论过分块。",
        "因为这些文档已经很小了，所以这里我们不需要分块。",
        "可以直接生成Embedding。",
        "要生成Embedding，我们将使用OpenAIEmbedding类。",
        "让我们导入并初始化它。",
        "如果想看看这些Embedding是如何工作的，可以实际看看一段特定的文本时生成的Embedding是什么。",
        "让我们使用Embedding对象上的\"embed_query\"方法为特定文本生成Embedding。",
        "在这种情况下，句子是\"嗨，我叫Harrison。\"",
        "如果我们看一下这个Embedding，可以看到有超过一千个(1536)不同的元素。",
        "每个元素都是一个不同的数字。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "91",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 44,
            "milliseconds": 620
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 51,
            "milliseconds": 340
          },
          "text": "Combined, this creates the overall numerical representation for this piece of text."
        },
        {
          "id": "92",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 51,
            "milliseconds": 340
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 58,
            "milliseconds": 500
          },
          "text": "We want to create embeddings for all the pieces of text that we just loaded, and then we also want to store them in a vector store."
        },
        {
          "id": "93",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 58,
            "milliseconds": 500
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 3,
            "milliseconds": 820
          },
          "text": "We can do that by using the from documents method on the vector store."
        },
        {
          "id": "94",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 3,
            "milliseconds": 820
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 12,
            "milliseconds": 260
          },
          "text": "This method takes in a list of documents, an embedding object, and then we'll create an overall vector store."
        },
        {
          "id": "95",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 12,
            "milliseconds": 260
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 18,
            "milliseconds": 20
          },
          "text": "We can now use this vector store to find pieces of text similar to an incoming query."
        },
        {
          "id": "96",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 18,
            "milliseconds": 20
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 23,
            "milliseconds": 60
          },
          "text": "So let's look at the query, \"Please suggest a shirt with sunblocking.\""
        },
        {
          "id": "97",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 23,
            "milliseconds": 60
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 29,
            "milliseconds": 100
          },
          "text": "If we use the \"similarity_search\" method on the vector store and pass in a query, we will get back a list of documents."
        },
        {
          "id": "98",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 36,
            "milliseconds": 860
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 47,
            "milliseconds": 400
          },
          "text": "We can see that it returns four documents, and if we look at the first one, we can see that it is indeed a shirt about sunblocking."
        },
        {
          "id": "99",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 48,
            "milliseconds": 40
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 53,
            "milliseconds": 60
          },
          "text": "So how do we use this to do question answering over our own documents?"
        },
        {
          "id": "100",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 53,
            "milliseconds": 60
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 57,
            "milliseconds": 100
          },
          "text": "First, we need to create a retriever from this vector store."
        }
      ],
      "source": [
        "Combined, this creates the overall numerical representation for this piece of text.",
        "We want to create embeddings for all the pieces of text that we just loaded, and then we also want to store them in a vector store.",
        "We can do that by using the from documents method on the vector store.",
        "This method takes in a list of documents, an embedding object, and then we'll create an overall vector store.",
        "We can now use this vector store to find pieces of text similar to an incoming query.",
        "So let's look at the query, \"Please suggest a shirt with sunblocking.\"",
        "If we use the \"similarity_search\" method on the vector store and pass in a query, we will get back a list of documents.",
        "We can see that it returns four documents, and if we look at the first one, we can see that it is indeed a shirt about sunblocking.",
        "So how do we use this to do question answering over our own documents?",
        "First, we need to create a retriever from this vector store."
      ],
      "result": [
        "这些数字就是这段文字的Embedding，1536维向量数字。",
        "我们希望给刚刚加载的所有文本片段生成Embedding，并将它们存储在一个向量存储器中。",
        "通过在向量存储器上调用\"from_documents\"方法来实现这一点。",
        "这个方法需要一个文档列表、一个Embedding对象，然后我们将创建一个向量存储器。",
        "现在我们可以用这个向量存储器来找到与输入的查询内容类似的文本片段。",
        "让我们看一下查询的内容：“请推荐一件防晒衣”。",
        "在向量存储器上调用“similarity_search”方法并传入查询的内容，就可以查询到一个文档列表。",
        "它返回了四个文档，如果我们看第一个，可以看到它确实是一件关于防晒的衣服。",
        "那么我们如何利用这个来回答我们自己文档中的问题呢？",
        "首先，需要从这个向量存储器创建一个检索器（Retriever）。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "101",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 57,
            "milliseconds": 100
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 3,
            "milliseconds": 820
          },
          "text": "A retriever is a generic interface that can be underpinned by any method that takes in a query and returns documents."
        },
        {
          "id": "102",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 3,
            "milliseconds": 820
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 11,
            "milliseconds": 260
          },
          "text": "Vector stores and embeddings are one such method to do so, although there are plenty of different methods, some less advanced, some more advanced."
        },
        {
          "id": "103",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 11,
            "milliseconds": 260
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 20,
            "milliseconds": 920
          },
          "text": "Next, because we want to do text generation and return a natural language response, we're going to import a language model, and we're going to use ChatOpenAI."
        },
        {
          "id": "104",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 20,
            "milliseconds": 920
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 28,
            "milliseconds": 780
          },
          "text": "If we were doing this by hand, what we would do is we would combine the documents into a single piece of text."
        },
        {
          "id": "105",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 28,
            "milliseconds": 780
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 34,
            "milliseconds": 729
          },
          "text": "So we'd do something like this, where we join all the page content in the documents into a variable,"
        },
        {
          "id": "106",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 34,
            "milliseconds": 743
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 48,
            "milliseconds": 980
          },
          "text": "and then we'd pass this variable or a variant on the question, like \"Please list all your shirts with sun protection in a table with markdown,\" and summarize each one into the language model."
        },
        {
          "id": "107",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 48,
            "milliseconds": 980
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 55,
            "milliseconds": 780
          },
          "text": "And if we print out the response here, we can see that we get back a table exactly as we asked for."
        },
        {
          "id": "108",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 55,
            "milliseconds": 780
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 59,
            "milliseconds": 900
          },
          "text": "All of those steps can be encapsulated with the LangChain chain."
        },
        {
          "id": "109",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 59,
            "milliseconds": 900
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 2,
            "milliseconds": 980
          },
          "text": "So here we can create a RetrievalQA chain."
        },
        {
          "id": "110",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 2,
            "milliseconds": 980
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 6,
            "milliseconds": 860
          },
          "text": "This does retrieval and then does question answering over the retrieved documents."
        }
      ],
      "source": [
        "A retriever is a generic interface that can be underpinned by any method that takes in a query and returns documents.",
        "Vector stores and embeddings are one such method to do so, although there are plenty of different methods, some less advanced, some more advanced.",
        "Next, because we want to do text generation and return a natural language response, we're going to import a language model, and we're going to use ChatOpenAI.",
        "If we were doing this by hand, what we would do is we would combine the documents into a single piece of text.",
        "So we'd do something like this, where we join all the page content in the documents into a variable,",
        "and then we'd pass this variable or a variant on the question, like \"Please list all your shirts with sun protection in a table with markdown,\" and summarize each one into the language model.",
        "And if we print out the response here, we can see that we get back a table exactly as we asked for.",
        "All of those steps can be encapsulated with the LangChain chain.",
        "So here we can create a RetrievalQA chain.",
        "This does retrieval and then does question answering over the retrieved documents."
      ],
      "result": [
        "检索器是一个通用接口，这个接口定义了一个接收查询内容并返回相似文档的方法。",
        "实现检索器的方法有很多种，基于向量存储和Embedding的检索是其中的一种，除此之外还有其他方法，有简单的有复杂的。",
        "接下来，我们想要返回一个自然语言的回应，所以要导入一个语言模型，我们将使用ChatOpenAI。",
        "接下来我们将手动把检索出来的文档合并成一段文本，",
        "将所有文档中的内容连接起来，并将结果保存到一个变量中，",
        "然后我们会将这个变量的内容和一个问题一起传给LLM，问题内容是：“请用Markdown表格列出所有具有防晒功能的衬衫，并为这些衬衫写一份摘要。”",
        "如果我们在这里打印出返回的结果，可以看到一个完全符合要求的表格。",
        "所有这些步骤都可以用LangChain链来封装。",
        "在这里我们可以创建一个RetrievalQA链，",
        "这个链会对查询进行检索，然后在检索到的文档上进行问答。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "111",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 6,
            "milliseconds": 860
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 9,
            "milliseconds": 860
          },
          "text": "To create such a chain, we'll pass in a few different things."
        },
        {
          "id": "112",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 9,
            "milliseconds": 860
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 12,
            "milliseconds": 200
          },
          "text": "First, we'll pass in the language model."
        },
        {
          "id": "113",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 12,
            "milliseconds": 200
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 15,
            "milliseconds": 260
          },
          "text": "This will be used for doing the text generation at the end."
        },
        {
          "id": "114",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 15,
            "milliseconds": 260
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 17,
            "milliseconds": 660
          },
          "text": "Next, we'll pass in the chain type."
        },
        {
          "id": "115",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 17,
            "milliseconds": 660
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 18,
            "milliseconds": 660
          },
          "text": "We're going to use stuff."
        },
        {
          "id": "116",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 18,
            "milliseconds": 660
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 25,
            "milliseconds": 380
          },
          "text": "This is the simplest method, as it just stuffs all the documents into context and makes one call to a language model."
        },
        {
          "id": "117",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 25,
            "milliseconds": 380
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 31,
            "milliseconds": 940
          },
          "text": "There are a few other methods that you can use to do question answering that I'll maybe touch on at the end, but we're not going to look at in detail."
        },
        {
          "id": "118",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 31,
            "milliseconds": 940
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 34,
            "milliseconds": 460
          },
          "text": "Third, we're going to pass in a retriever."
        },
        {
          "id": "119",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 34,
            "milliseconds": 460
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 38,
            "milliseconds": 660
          },
          "text": "The retriever we created above is just an interface for fetching documents."
        },
        {
          "id": "120",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 38,
            "milliseconds": 660
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 41,
            "milliseconds": 860
          },
          "text": "This will be used to fetch documents and pass it to the language model."
        }
      ],
      "source": [
        "To create such a chain, we'll pass in a few different things.",
        "First, we'll pass in the language model.",
        "This will be used for doing the text generation at the end.",
        "Next, we'll pass in the chain type.",
        "We're going to use stuff.",
        "This is the simplest method, as it just stuffs all the documents into context and makes one call to a language model.",
        "There are a few other methods that you can use to do question answering that I'll maybe touch on at the end, but we're not going to look at in detail.",
        "Third, we're going to pass in a retriever.",
        "The retriever we created above is just an interface for fetching documents.",
        "This will be used to fetch documents and pass it to the language model."
      ],
      "result": [
        "为了创建这个链，我们需要传入一些不同的东西。",
        "首先，我们要传入语言模型。",
        "这将用于最后的文本生成。",
        "接下来，我们要传入链类型。",
        "我们将使用\"stuff\"方法。",
        "这是最简单的方法，因为它只是在调用语言模型时将所有文档内容都一起放到上下文中。",
        "还有一些其他方法可以用来进行问答，我可能会在最后简单提一下，但不会详细讨论。",
        "第三，我们要传入一个检索器。",
        "我们在上面创建的检索器只是一个检索文档的接口，",
        "它将用于检索文档并将结果传递给语言模型。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "121",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 41,
            "milliseconds": 860
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 46,
            "milliseconds": 340
          },
          "text": "And then finally, we're going to set verbose equals to true."
        },
        {
          "id": "122",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 46,
            "milliseconds": 340
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 50,
            "milliseconds": 929
          },
          "text": "Now we can create a query and we can run the chain on this query."
        },
        {
          "id": "123",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 8,
            "milliseconds": 460
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 14,
            "milliseconds": 860
          },
          "text": "When we get the response, we can again display it using the display and Markdown utilities."
        },
        {
          "id": "124",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 14,
            "milliseconds": 860
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 20,
            "milliseconds": 940
          },
          "text": "You can pause the video here and try it out with a bunch of different queries."
        },
        {
          "id": "125",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 20,
            "milliseconds": 940
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 22,
            "milliseconds": 240
          },
          "text": "So that's how you do it in detail."
        },
        {
          "id": "126",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 22,
            "milliseconds": 240
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 26,
            "milliseconds": 560
          },
          "text": "But remember that we can still do it pretty easily with just the one line that we had up above."
        },
        {
          "id": "127",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 26,
            "milliseconds": 560
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 30,
            "milliseconds": 260
          },
          "text": "So these two things equate to the same result."
        },
        {
          "id": "128",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 30,
            "milliseconds": 260
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 32,
            "milliseconds": 720
          },
          "text": "And that's part of the interesting stuff about LangChain."
        },
        {
          "id": "129",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 32,
            "milliseconds": 720
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 38,
            "milliseconds": 140
          },
          "text": "You can do it in one line or you can look at the individual things and break it down into five more detailed ones."
        },
        {
          "id": "130",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 38,
            "milliseconds": 140
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 43,
            "milliseconds": 0
          },
          "text": "The five more detailed ones let you set more specifics about what exactly is going on."
        }
      ],
      "source": [
        "And then finally, we're going to set verbose equals to true.",
        "Now we can create a query and we can run the chain on this query.",
        "When we get the response, we can again display it using the display and Markdown utilities.",
        "You can pause the video here and try it out with a bunch of different queries.",
        "So that's how you do it in detail.",
        "But remember that we can still do it pretty easily with just the one line that we had up above.",
        "So these two things equate to the same result.",
        "And that's part of the interesting stuff about LangChain.",
        "You can do it in one line or you can look at the individual things and break it down into five more detailed ones.",
        "The five more detailed ones let you set more specifics about what exactly is going on."
      ],
      "result": [
        "最后，把\"verbose\"参数设置为\"True\"，这样可以打印详细日志。",
        "现在我们可以创建一个查询，并把查询的内容传入链并运行。",
        "当我们得到返回结果后，可以再次使用disaplay和Markdown工具显示结果。",
        "建议你暂停视频，尝试不同的查询内容，并且运行看看结果。",
        "这就是详细操作方法。",
        "但如果你还记得一开始我们用这一行代码就可以轻松完成文档问答。",
        "这两种方式得到的结果是相同的。",
        "这也是LangChain有意思的一点。",
        "你可以用一行代码完成，也可以把它分成五个详细的步骤，可以查看每一步的详细结果。",
        "五个详细的步骤可以让你更具体地知道到底发生了什么。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "131",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 43,
            "milliseconds": 0
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 44,
            "milliseconds": 740
          },
          "text": "But the one-liner is easy to get started."
        },
        {
          "id": "132",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 44,
            "milliseconds": 740
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 48,
            "milliseconds": 420
          },
          "text": "So up to you as to how you'd prefer to go forward."
        },
        {
          "id": "133",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 48,
            "milliseconds": 420
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 51,
            "milliseconds": 760
          },
          "text": "We can also customize the index when we're creating it."
        },
        {
          "id": "134",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 51,
            "milliseconds": 760
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 55,
            "milliseconds": 260
          },
          "text": "And so if you remember, when we created it by hand, we specified an embedding."
        },
        {
          "id": "135",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 55,
            "milliseconds": 260
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 57,
            "milliseconds": 740
          },
          "text": "And we can specify an embedding here as well."
        },
        {
          "id": "136",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 57,
            "milliseconds": 740
          },
          "endTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 1,
            "milliseconds": 860
          },
          "text": "And so this will give us flexibility over how the embeddings themselves are created."
        },
        {
          "id": "137",
          "startTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 1,
            "milliseconds": 860
          },
          "endTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 6,
            "milliseconds": 620
          },
          "text": "And we can also swap out the vector store here for a different type of vector store."
        },
        {
          "id": "138",
          "startTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 6,
            "milliseconds": 620
          },
          "endTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 15,
            "milliseconds": 100
          },
          "text": "So there's the same level of customization that you did when you created by hand that's also available when you create the index here."
        },
        {
          "id": "139",
          "startTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 15,
            "milliseconds": 100
          },
          "endTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 16,
            "milliseconds": 820
          },
          "text": "We use the stuff method in this notebook."
        },
        {
          "id": "140",
          "startTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 16,
            "milliseconds": 820
          },
          "endTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 19,
            "milliseconds": 380
          },
          "text": "The stuff method is really nice because it's pretty simple."
        }
      ],
      "source": [
        "But the one-liner is easy to get started.",
        "So up to you as to how you'd prefer to go forward.",
        "We can also customize the index when we're creating it.",
        "And so if you remember, when we created it by hand, we specified an embedding.",
        "And we can specify an embedding here as well.",
        "And so this will give us flexibility over how the embeddings themselves are created.",
        "And we can also swap out the vector store here for a different type of vector store.",
        "So there's the same level of customization that you did when you created by hand that's also available when you create the index here.",
        "We use the stuff method in this notebook.",
        "The stuff method is really nice because it's pretty simple."
      ],
      "result": [
        "一句话总结就是很容易上手。",
        "由你自己决定如何使用。",
        "在创建索引时，也可以自定义索引。",
        "如果你还记得，我们手动创建索引时指定了一个Embedding。",
        "我们也可以在这里指定一个Embedding。",
        "这将使我们在生成Embedding时具有更大的灵活性。",
        "我们还可以将这里的向量存储替换为其他类型的向量存储。",
        "在手动创建向量存储时，跟创建索引一样也可以很灵活的定制。",
        "在这个Notebook中我们使用\"stuff\"方法。",
        "\"stuff\"方法非常好，因为它很简单。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "141",
          "startTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 19,
            "milliseconds": 380
          },
          "endTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 25,
            "milliseconds": 220
          },
          "text": "You just put all of it into one prompt and send that to the language model and get back one response."
        },
        {
          "id": "142",
          "startTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 25,
            "milliseconds": 220
          },
          "endTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 27,
            "milliseconds": 780
          },
          "text": "So it's quite simple to understand what's going on."
        },
        {
          "id": "143",
          "startTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 27,
            "milliseconds": 780
          },
          "endTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 30,
            "milliseconds": 500
          },
          "text": "It's quite cheap and it works pretty well."
        },
        {
          "id": "144",
          "startTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 30,
            "milliseconds": 500
          },
          "endTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 32,
            "milliseconds": 740
          },
          "text": "But that doesn't always work okay."
        },
        {
          "id": "145",
          "startTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 32,
            "milliseconds": 740
          },
          "endTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 39,
            "milliseconds": 340
          },
          "text": "So if you remember, when we fetched the documents in the notebook, we only got four documents back and they were relatively small."
        },
        {
          "id": "146",
          "startTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 39,
            "milliseconds": 340
          },
          "endTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 44,
            "milliseconds": 660
          },
          "text": "But what if you wanted to do the same type of question answering over lots of different types of chunks?"
        },
        {
          "id": "147",
          "startTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 44,
            "milliseconds": 660
          },
          "endTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 47,
            "milliseconds": 60
          },
          "text": "Then there are a few different methods that we can use."
        },
        {
          "id": "148",
          "startTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 47,
            "milliseconds": 60
          },
          "endTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 48,
            "milliseconds": 820
          },
          "text": "The first is Map_reduce."
        },
        {
          "id": "149",
          "startTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 48,
            "milliseconds": 820
          },
          "endTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 50,
            "milliseconds": 343
          },
          "text": "This basically takes all the chunks,"
        },
        {
          "id": "150",
          "startTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 50,
            "milliseconds": 344
          },
          "endTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 55,
            "milliseconds": 343
          },
          "text": "passes them along with the question to a language model, gets back a response,"
        },
        {
          "id": "151",
          "startTime": {
            "hours": 0,
            "minutes": 12,
            "seconds": 55,
            "milliseconds": 344
          },
          "endTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 2,
            "milliseconds": 440
          },
          "text": "and then uses another language model call to summarize all of the individual responses into a final answer."
        }
      ],
      "source": [
        "You just put all of it into one prompt and send that to the language model and get back one response.",
        "So it's quite simple to understand what's going on.",
        "It's quite cheap and it works pretty well.",
        "But that doesn't always work okay.",
        "So if you remember, when we fetched the documents in the notebook, we only got four documents back and they were relatively small.",
        "But what if you wanted to do the same type of question answering over lots of different types of chunks?",
        "Then there are a few different methods that we can use.",
        "The first is Map_reduce.",
        "This basically takes all the chunks,",
        "passes them along with the question to a language model, gets back a response,",
        "and then uses another language model call to summarize all of the individual responses into a final answer."
      ],
      "result": [
        "你只需把所有内容放到Prompt里，然后发送给语言模型，得到返回结果。",
        "这很容易理解发生了什么。",
        "非常便宜，效果也相当好。",
        "但这并不总是有效。",
        "如果你还记得，在Notebook中获取文档时，我们只得到了四个相对较小的文档。",
        "但如果你想在许多不同类型的分块上进行同样类型的问题回答呢？",
        "那么我们可以使用几种不同的方法。",
        "第一个是Map_reduce。",
        "基本就是对所有的分块，把每一块的内容连同问题一起传递给语言模型，得到一个独立返回结果，",
        "",
        "然后每一块得到的结果都合并在一起，再使用语言模型来对这些结果进行总结，得到最终答案。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "152",
          "startTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 2,
            "milliseconds": 440
          },
          "endTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 6,
            "milliseconds": 900
          },
          "text": "This is really powerful because it can operate over any number of documents."
        },
        {
          "id": "153",
          "startTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 6,
            "milliseconds": 900
          },
          "endTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 11,
            "milliseconds": 860
          },
          "text": "And it's also really powerful because you can do the individual questions in parallel."
        },
        {
          "id": "154",
          "startTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 11,
            "milliseconds": 860
          },
          "endTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 13,
            "milliseconds": 660
          },
          "text": "But it does take a lot more calls."
        },
        {
          "id": "155",
          "startTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 13,
            "milliseconds": 660
          },
          "endTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 19,
            "milliseconds": 60
          },
          "text": "And it does treat all the documents as independent, which may not always be the most desired thing."
        },
        {
          "id": "156",
          "startTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 19,
            "milliseconds": 60
          },
          "endTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 24,
            "milliseconds": 260
          },
          "text": "Refine, which is another method, is again used to loop over many documents."
        },
        {
          "id": "157",
          "startTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 24,
            "milliseconds": 260
          },
          "endTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 25,
            "milliseconds": 580
          },
          "text": "But it actually does it iteratively."
        },
        {
          "id": "158",
          "startTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 25,
            "milliseconds": 580
          },
          "endTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 28,
            "milliseconds": 780
          },
          "text": "It builds upon the answer from the previous document."
        },
        {
          "id": "159",
          "startTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 28,
            "milliseconds": 780
          },
          "endTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 33,
            "milliseconds": 900
          },
          "text": "So this is really good for combining information and building up an answer over time."
        },
        {
          "id": "160",
          "startTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 33,
            "milliseconds": 900
          },
          "endTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 36,
            "milliseconds": 580
          },
          "text": "It will generally lead to longer answers."
        },
        {
          "id": "161",
          "startTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 36,
            "milliseconds": 580
          },
          "endTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 39,
            "milliseconds": 740
          },
          "text": "And it's also not as fast because now the calls aren't independent."
        }
      ],
      "source": [
        "This is really powerful because it can operate over any number of documents.",
        "And it's also really powerful because you can do the individual questions in parallel.",
        "But it does take a lot more calls.",
        "And it does treat all the documents as independent, which may not always be the most desired thing.",
        "Refine, which is another method, is again used to loop over many documents.",
        "But it actually does it iteratively.",
        "It builds upon the answer from the previous document.",
        "So this is really good for combining information and building up an answer over time.",
        "It will generally lead to longer answers.",
        "And it's also not as fast because now the calls aren't independent."
      ],
      "result": [
        "这真的很强大，因为它可以处理任意数量的文档。",
        "而且它也很强大，因为你可以并行处理，同时处理多个分块。",
        "但是这种方法需要更多的调用语言模型。",
        "而且它把所有文档都独立处理，这可能并不总是最理想的结果。",
        "另一种方法是\"Refine\"，也是用来处理多个文档的。",
        "但它实际上是迭代进行的。",
        "它基于前一个文档的答案。",
        "所以这对于需要整合信息，以及随着时间推移构建答案非常有用。",
        "但它通常会导致更长的答案，",
        "而且速度也不那么快，因为现在每一个文档无法被独立调用，"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "162",
          "startTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 39,
            "milliseconds": 740
          },
          "endTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 43,
            "milliseconds": 380
          },
          "text": "They depend on the result of previous calls."
        },
        {
          "id": "163",
          "startTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 43,
            "milliseconds": 380
          },
          "endTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 49,
            "milliseconds": 980
          },
          "text": "This means that it often takes a good while longer and takes just as many calls as Map_reduce, basically."
        },
        {
          "id": "164",
          "startTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 49,
            "milliseconds": 980
          },
          "endTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 57,
            "milliseconds": 980
          },
          "text": "Map_rerank is a pretty interesting and a bit more experimental one where you do a single call to the language model for each document."
        },
        {
          "id": "165",
          "startTime": {
            "hours": 0,
            "minutes": 13,
            "seconds": 57,
            "milliseconds": 980
          },
          "endTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 0,
            "milliseconds": 300
          },
          "text": "And you also ask it to return a score."
        },
        {
          "id": "166",
          "startTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 0,
            "milliseconds": 300
          },
          "endTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 2,
            "milliseconds": 620
          },
          "text": "And then you select the highest score."
        },
        {
          "id": "167",
          "startTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 2,
            "milliseconds": 620
          },
          "endTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 6,
            "milliseconds": 220
          },
          "text": "This relies on the language model to know what the score should be."
        },
        {
          "id": "168",
          "startTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 6,
            "milliseconds": 220
          },
          "endTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 12,
            "milliseconds": 580
          },
          "text": "So you often have to tell it, hey, it should be a high score if it's relevant to the document and really refine the instructions there."
        },
        {
          "id": "169",
          "startTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 12,
            "milliseconds": 580
          },
          "endTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 15,
            "milliseconds": 140
          },
          "text": "Similar to Map_reduce, all the calls are independent."
        },
        {
          "id": "170",
          "startTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 15,
            "milliseconds": 140
          },
          "endTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 16,
            "milliseconds": 380
          },
          "text": "So you can batch them."
        },
        {
          "id": "171",
          "startTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 16,
            "milliseconds": 380
          },
          "endTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 18,
            "milliseconds": 220
          },
          "text": "And it's relatively fast."
        }
      ],
      "source": [
        "They depend on the result of previous calls.",
        "This means that it often takes a good while longer and takes just as many calls as Map_reduce, basically.",
        "Map_rerank is a pretty interesting and a bit more experimental one where you do a single call to the language model for each document.",
        "And you also ask it to return a score.",
        "And then you select the highest score.",
        "This relies on the language model to know what the score should be.",
        "So you often have to tell it, hey, it should be a high score if it's relevant to the document and really refine the instructions there.",
        "Similar to Map_reduce, all the calls are independent.",
        "So you can batch them.",
        "And it's relatively fast."
      ],
      "result": [
        "必须依赖前面的结果。",
        "这意味着它通常需要更长的时间，而且调用次数与Map_reduce一样多。",
        "\"Map_rerank\"是一个相当有趣但还处于实验阶段的方法，对于每个文档，你只需对语言模型进行一次调用。",
        "另外还需要让它返回一个评分。",
        "然后选择最高分的结果。",
        "这依赖于语言模型知道分数应该是多少。",
        "所以你需要告诉给它指令：“嘿，如果与文档相关，分数应该很高”，并且需要具体优化那部分指令。",
        "与\"Map_reduce\"方法类似，所有调用都是独立的。",
        "所以你可以批量处理它们。",
        "而且速度相对较快。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    },
    {
      "items": [
        {
          "id": "172",
          "startTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 18,
            "milliseconds": 220
          },
          "endTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 20,
            "milliseconds": 780
          },
          "text": "But again, you're making a bunch of language model calls."
        },
        {
          "id": "173",
          "startTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 20,
            "milliseconds": 780
          },
          "endTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 22,
            "milliseconds": 740
          },
          "text": "So it will be a bit more expensive."
        },
        {
          "id": "174",
          "startTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 22,
            "milliseconds": 740
          },
          "endTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 29,
            "milliseconds": 220
          },
          "text": "The most common of these methods is the stuff method, which we used in the notebook to combine it all into one document."
        },
        {
          "id": "175",
          "startTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 29,
            "milliseconds": 220
          },
          "endTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 35,
            "milliseconds": 580
          },
          "text": "The second most common is the Map_reduce method, which takes these chunks and sends them to the language model."
        },
        {
          "id": "176",
          "startTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 35,
            "milliseconds": 580
          },
          "endTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 42,
            "milliseconds": 740
          },
          "text": "These methods here, stuff, Map_reduce, refine, and rerank, can also be used for lots of other chains besides just question answering."
        },
        {
          "id": "177",
          "startTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 42,
            "milliseconds": 740
          },
          "endTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 47,
            "milliseconds": 629
          },
          "text": "For example, a really common use case of the Map_reduce chain is for summarization,"
        },
        {
          "id": "178",
          "startTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 47,
            "milliseconds": 630
          },
          "endTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 53,
            "milliseconds": 820
          },
          "text": "where you have a really long document and you want to recursively summarize pieces of information."
        },
        {
          "id": "179",
          "startTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 53,
            "milliseconds": 820
          },
          "endTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 56,
            "milliseconds": 180
          },
          "text": "That's it for question answering over documents."
        },
        {
          "id": "180",
          "startTime": {
            "hours": 0,
            "minutes": 14,
            "seconds": 56,
            "milliseconds": 180
          },
          "endTime": {
            "hours": 0,
            "minutes": 15,
            "seconds": 0,
            "milliseconds": 580
          },
          "text": "As you may have noticed, there's a lot going on in the different chains that we have here."
        },
        {
          "id": "181",
          "startTime": {
            "hours": 0,
            "minutes": 15,
            "seconds": 0,
            "milliseconds": 580
          },
          "endTime": {
            "hours": 0,
            "minutes": 15,
            "seconds": 1,
            "milliseconds": 891
          },
          "text": "And so in the next section,"
        },
        {
          "id": "182",
          "startTime": {
            "hours": 0,
            "minutes": 15,
            "seconds": 1,
            "milliseconds": 892
          },
          "endTime": {
            "hours": 0,
            "minutes": 15,
            "seconds": 6,
            "milliseconds": 260
          },
          "text": "we'll cover ways to better understand what exactly is going on inside all of these chains."
        }
      ],
      "source": [
        "But again, you're making a bunch of language model calls.",
        "So it will be a bit more expensive.",
        "The most common of these methods is the stuff method, which we used in the notebook to combine it all into one document.",
        "The second most common is the Map_reduce method, which takes these chunks and sends them to the language model.",
        "These methods here, stuff, Map_reduce, refine, and rerank, can also be used for lots of other chains besides just question answering.",
        "For example, a really common use case of the Map_reduce chain is for summarization,",
        "where you have a really long document and you want to recursively summarize pieces of information.",
        "That's it for question answering over documents.",
        "As you may have noticed, there's a lot going on in the different chains that we have here.",
        "And so in the next section,",
        "we'll cover ways to better understand what exactly is going on inside all of these chains."
      ],
      "result": [
        "不过，你要调用很多次语言模型。",
        "所以会有点贵。",
        "这些方法中最常见的是\"stuff\"方法，我们在Notebook中用它把所有内容合并成一个文档。",
        "第二常见的是\"Map_reduce\"方法，它将这些块分别发送到语言模型。",
        "除了问答之外，像stuff、Map_reduce、refine和rerank这些方法，你也可用于其他链上。",
        "比如说\"Map_reduce\"链用来生成摘要，",
        "在你需要对一个长文档分段递归生成摘要时很常用。",
        "关于文档问答就讲到这里了。",
        "你可能已经注意到，这些链都可以做很多事情。",
        "接下来的部分，",
        "我们将介绍如何更好地理解这些链内部到底发生了什么。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false
    }
  ],
  "sourcePath": "input/LangChain for LLM Application Development/LangChain_L4_v02.srt",
  "ouputBasePath": "input/LangChain for LLM Application Development/LangChain_L4_v02",
  "totalCost": 0.45396000000000003,
  "translationPath": "input/LangChain for LLM Application Development/LangChain_L4_v02/translation.json"
}
