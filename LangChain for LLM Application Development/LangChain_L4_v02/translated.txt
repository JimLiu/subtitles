文档问答系统是一种常见的用LLM构建的复杂应用程序。
给定一段可能从PDF文件、网页或某公司的内部文档库中提取的文本，
你能否使用LLM回答关于这些文档内容的问题，帮助用户深入了解并获取他们需要的信息？
这样的应用非常强大，因为它可以将大语言模型与完全没被训练的数据相结合。
这使得它们可以灵活的适应你的应用场景。
这也非常令人兴奋，因为我们将开始超越语言模型、Prompt和输出解析器，引入LangChain的更多关键组件，
例如Embedding模型和向量存储（Vector Stores）。
正如Andrew提到的，这是LangChain最受欢迎的链之一，希望你能对这个有兴趣。
事实上，Embedding和向量存储是两种强大的前沿技术。
如果你还没有了解过这些技术，那么绝对值得学习。
那让我们开始吧！
开始吧！
首先，我们会像往常一样导入环境变量。
现在我们要导入一些在构建这个链时需要用到的库。
我们将导入RetrievalQA链，它可以帮助检索文档。
我们将导入大家都喜欢的ChatOpenAI语言模型。
我们要导入一个文档加载器，
后面会用它加载一些专用数据，用来与语言模型结合使用。
在这里我们要加载的是一个CSV文件，
所以要导入CSVLoader。
最后，我们要导入一个向量存储。
有很多不同类型的向量存储，我们稍后会介绍它是什么，让我们先从向量存储DocArrayInMemorySearch开始。
这个向量存储非常好用，因为它是内存存储，不需要连接任何外部数据库，所以很容易上手。
我们还将导入"display"和"Markdown"，常用来在Jupyter Notebook中显示信息。
我们提供了一个户外服装产品目录的CSV文件，会用它和语言模型结合使用。
在这里，我们将初始化一个名叫CSVLoader的加载器（Loader），并为其指定文件路径。
接下来，我们要导入一个叫\NVectorStoreIndexCreator\N的索引（Index）。
借助它们，我们可以很方便地创建一个向量存储。
如下所示，只需几行代码就可以创建。
要创建向量存储，我们需要指定两件事。
第一件事是我们要指定一个向量存储类。
如前所述，我们将使用这个向量存储，因为它特别容易上手。
第二件事是，创建后我们将调用文档加载器(Loader)，传入一个包含一个或多个加载器的列表。
我们只需要一个加载器，也就是传入的这个。
现在已经创建好了向量存储，我们可以开始提问了。
稍后将会向你介绍这背后到底发生了什么。
我们先定义一个包含查询内容的字符串变量"query"。（内容是：“请在表格中列出你所有带防晒功能的衬衫，并为这些衬衫写一份摘要。”）
然后我们用"index.query"生成一个响应，并传入"query"变量。
同样，稍后将会向你介绍这背后到底发生了什么
现在，我们来等等看它返回什么结果。
在完成后，我们可以看看到底返回了什么。
我们得到了一个Markdown格式的表格，列出了所有具有防晒功能的衬衫的名称和描述。
我们还得到了一个很好的小结，是语言模型为我们生成的。
我们已经介绍了如何对文档中的内容进行问答，但底层到底是怎么实现的呢？
首先让我们来梳理一下思路。
想要使用语言模型，并且与大量文档相结合，存在一个关键问题：
语言模型一次只能接收几千个单词。
那么，如果我们有一个很大的文档，如何让语言模型对文档所有内容进行问答呢？
这里就需要Embedding和向量存储发挥作用了。
首先让我们谈谈Embedding。
Embedding将一段文本转换成数字，用一组数字表示这段文本。
这组数字捕捉了它所代表的文字片段的内容含义。
内容相似的文本片段会有相似的向量值。
这样我们就可以在向量空间中比较文本片段。
在下面的例子中，我们可以看到有三个句子，
前两个是关于宠物的，而第三个是关于汽车的。
如果我们观察数值空间中的表示，可以看到当我们比较关于两个关于宠物的句子的向量时，它们相似度非常高。
而如果我们将其中一个与汽车相关的那个句子向量进行比较，它们相似度很低。
利用向量相似度可以让我们轻松地找出哪些文本片段相似，
这对我们非常有用，因为我们可以利用这种技术从文档中找出跟问题相似的文本片段，一起传递给语言模型来帮助回答问题。
接下来我们要介绍的组件是向量数据库。
向量数据库是一种存储方法，可以存储我们在前面创建的那种矢量数字数组。
往向量数据库中新建数据的方式，就是将文档拆分成块，每块生成Embedding，然后把Embedding和原始块一起存储到数据库中。
当我们在处理大一点的文档时，首先要将其拆分成较小的文本块，

因为可能无法将整个文档的内容都传给语言模型。
所以需要把文档拆分成小块，这样每次就只用把最相关的几块内容传递给语言模型。
然后，把每个文本块生成一个Embedding，然后将这些Embedding存储在向量数据库中。
这就是我们创建索引时发生的事情。
索引创建后，我们可以用它来找到与查询内容最相关的几个文本片段。
当一个查询进来时，我们首先将查询的内容生成Embedding，得到一个数字数组。
然后将这个数字数组与向量数据库中的所有向量进行比较，选择最相似的前若干个文本块。
拿到这些文本块后，将这些文本块和原始的查询内容一起传递给语言模型，这样可以让语言模型根据检索出来的文档内容生成最终答案。
在上面，我们用几行代码创建了这个链。
这对于快速入门非常有帮助。
好吧，现在让我们详细了解一下底层到底发生了什么。
第一步与上面类似。
我们将创建一个文档加载器，加载前面提到的产品描述的CSV文件，用做后面问答的文档数据。
然后我们可以借助这个文档加载器中加载所有文档。
如果查看单个文档，可以看到每个文档对应CSV文件中的一个产品。
之前我们讨论过分块。
因为这些文档已经很小了，所以这里我们不需要分块。
可以直接生成Embedding。
要生成Embedding，我们将使用OpenAIEmbedding类。
让我们导入并初始化它。
如果想看看这些Embedding是如何工作的，可以实际看看一段特定的文本时生成的Embedding是什么。
让我们使用Embedding对象上的"embed_query"方法为特定文本生成Embedding。
在这种情况下，句子是"嗨，我叫Harrison。"
如果我们看一下这个Embedding，可以看到有超过一千个(1536)不同的元素。
每个元素都是一个不同的数字。
这些数字就是这段文字的Embedding，1536维向量数字。
我们希望给刚刚加载的所有文本片段生成Embedding，并将它们存储在一个向量存储器中。
通过在向量存储器上调用"from_documents"方法来实现这一点。
这个方法需要一个文档列表、一个Embedding对象，然后我们将创建一个向量存储器。
现在我们可以用这个向量存储器来找到与输入的查询内容类似的文本片段。
让我们看一下查询的内容：“请推荐一件防晒衣”。
在向量存储器上调用“similarity_search”方法并传入查询的内容，就可以查询到一个文档列表。
它返回了四个文档，如果我们看第一个，可以看到它确实是一件关于防晒的衣服。
那么我们如何利用这个来回答我们自己文档中的问题呢？
首先，需要从这个向量存储器创建一个检索器（Retriever）。
检索器是一个通用接口，这个接口定义了一个接收查询内容并返回相似文档的方法。
实现检索器的方法有很多种，基于向量存储和Embedding的检索是其中的一种，除此之外还有其他方法，有简单的有复杂的。
接下来，我们想要返回一个自然语言的回应，所以要导入一个语言模型，我们将使用ChatOpenAI。
接下来我们将手动把检索出来的文档合并成一段文本，
将所有文档中的内容连接起来，并将结果保存到一个变量中，
然后我们会将这个变量的内容和一个问题一起传给LLM，问题内容是：“请用Markdown表格列出所有具有防晒功能的衬衫，并为这些衬衫写一份摘要。”
如果我们在这里打印出返回的结果，可以看到一个完全符合要求的表格。
所有这些步骤都可以用LangChain链来封装。
在这里我们可以创建一个RetrievalQA链，
这个链会对查询进行检索，然后在检索到的文档上进行问答。
为了创建这个链，我们需要传入一些不同的东西。
首先，我们要传入语言模型。
这将用于最后的文本生成。
接下来，我们要传入链类型。
我们将使用"stuff"方法。
这是最简单的方法，因为它只是在调用语言模型时将所有文档内容都一起放到上下文中。
还有一些其他方法可以用来进行问答，我可能会在最后简单提一下，但不会详细讨论。
第三，我们要传入一个检索器。
我们在上面创建的检索器只是一个检索文档的接口，
它将用于检索文档并将结果传递给语言模型。
最后，把"verbose"参数设置为"True"，这样可以打印详细日志。
现在我们可以创建一个查询，并把查询的内容传入链并运行。
当我们得到返回结果后，可以再次使用disaplay和Markdown工具显示结果。
建议你暂停视频，尝试不同的查询内容，并且运行看看结果。
这就是详细操作方法。
但如果你还记得一开始我们用这一行代码就可以轻松完成文档问答。
这两种方式得到的结果是相同的。
这也是LangChain有意思的一点。
你可以用一行代码完成，也可以把它分成五个详细的步骤，可以查看每一步的详细结果。
五个详细的步骤可以让你更具体地知道到底发生了什么。
一句话总结就是很容易上手。
由你自己决定如何使用。
在创建索引时，也可以自定义索引。
如果你还记得，我们手动创建索引时指定了一个Embedding。
我们也可以在这里指定一个Embedding。
这将使我们在生成Embedding时具有更大的灵活性。
我们还可以将这里的向量存储替换为其他类型的向量存储。
在手动创建向量存储时，跟创建索引一样也可以很灵活的定制。
在这个Notebook中我们使用"stuff"方法。
"stuff"方法非常好，因为它很简单。
你只需把所有内容放到Prompt里，然后发送给语言模型，得到返回结果。
这很容易理解发生了什么。
非常便宜，效果也相当好。
但这并不总是有效。
如果你还记得，在Notebook中获取文档时，我们只得到了四个相对较小的文档。
但如果你想在许多不同类型的分块上进行同样类型的问题回答呢？
那么我们可以使用几种不同的方法。
第一个是Map_reduce。
基本就是对所有的分块，把每一块的内容连同问题一起传递给语言模型，得到一个独立返回结果，

然后每一块得到的结果都合并在一起，再使用语言模型来对这些结果进行总结，得到最终答案。
这真的很强大，因为它可以处理任意数量的文档。
而且它也很强大，因为你可以并行处理，同时处理多个分块。
但是这种方法需要更多的调用语言模型。
而且它把所有文档都独立处理，这可能并不总是最理想的结果。
另一种方法是"Refine"，也是用来处理多个文档的。
但它实际上是迭代进行的。
它基于前一个文档的答案。
所以这对于需要整合信息，以及随着时间推移构建答案非常有用。
但它通常会导致更长的答案，
而且速度也不那么快，因为现在每一个文档无法被独立调用，
必须依赖前面的结果。
这意味着它通常需要更长的时间，而且调用次数与Map_reduce一样多。
"Map_rerank"是一个相当有趣但还处于实验阶段的方法，对于每个文档，你只需对语言模型进行一次调用。
另外还需要让它返回一个评分。
然后选择最高分的结果。
这依赖于语言模型知道分数应该是多少。
所以你需要告诉给它指令：“嘿，如果与文档相关，分数应该很高”，并且需要具体优化那部分指令。
与"Map_reduce"方法类似，所有调用都是独立的。
所以你可以批量处理它们。
而且速度相对较快。
不过，你要调用很多次语言模型。
所以会有点贵。
这些方法中最常见的是"stuff"方法，我们在Notebook中用它把所有内容合并成一个文档。
第二常见的是"Map_reduce"方法，它将这些块分别发送到语言模型。
除了问答之外，像stuff、Map_reduce、refine和rerank这些方法，你也可用于其他链上。
比如说"Map_reduce"链用来生成摘要，
在你需要对一个长文档分段递归生成摘要时很常用。
关于文档问答就讲到这里了。
你可能已经注意到，这些链都可以做很多事情。
接下来的部分，
我们将介绍如何更好地理解这些链内部到底发生了什么。