1
00:00:05,000 --> 00:00:15,240
文档问答系统是一种常见的用LLM构建的复杂应用程序。

2
00:00:15,240 --> 00:00:23,886
给定一段可能从PDF文件、网页或某公司的内部文档库中提取的文本，

3
00:00:23,914 --> 00:00:33,460
你能否使用LLM回答关于这些文档内容的问题，帮助用户深入了解并获取他们需要的信息？

4
00:00:33,460 --> 00:00:39,300
这样的应用非常强大，因为它可以将大语言模型与完全没被训练的数据相结合。

5
00:00:39,300 --> 00:00:42,540
这使得它们可以灵活的适应你的应用场景。

6
00:00:42,540 --> 00:00:51,543
这也非常令人兴奋，因为我们将开始超越语言模型、Prompt和输出解析器，引入LangChain的更多关键组件，

7
00:00:51,557 --> 00:00:54,580
例如Embedding模型和向量存储（Vector Stores）。

8
00:00:54,580 --> 00:00:58,620
正如Andrew提到的，这是LangChain最受欢迎的链之一，希望你能对这个有兴趣。

9
00:00:58,620 --> 00:01:03,580
事实上，Embedding和向量存储是两种强大的前沿技术。

10
00:01:03,580 --> 00:01:08,300
如果你还没有了解过这些技术，那么绝对值得学习。

11
00:01:08,300 --> 00:01:10,060
那让我们开始吧！

12
00:01:10,060 --> 00:01:11,060
开始吧！

13
00:01:11,060 --> 00:01:16,300
首先，我们会像往常一样导入环境变量。

14
00:01:16,300 --> 00:01:20,060
现在我们要导入一些在构建这个链时需要用到的库。

15
00:01:20,060 --> 00:01:24,340
我们将导入RetrievalQA链，它可以帮助检索文档。

16
00:01:24,340 --> 00:01:28,100
我们将导入大家都喜欢的ChatOpenAI语言模型。

17
00:01:28,100 --> 00:01:29,700
我们要导入一个文档加载器，

18
00:01:29,700 --> 00:01:34,500
后面会用它加载一些专用数据，用来与语言模型结合使用。

19
00:01:34,500 --> 00:01:36,380
在这里我们要加载的是一个CSV文件，

20
00:01:36,380 --> 00:01:38,380
所以要导入CSVLoader。

21
00:01:38,380 --> 00:01:41,220
最后，我们要导入一个向量存储。

22
00:01:41,220 --> 00:01:48,900
有很多不同类型的向量存储，我们稍后会介绍它是什么，让我们先从向量存储DocArrayInMemorySearch开始。

23
00:01:48,900 --> 00:01:57,580
这个向量存储非常好用，因为它是内存存储，不需要连接任何外部数据库，所以很容易上手。

24
00:01:57,580 --> 00:02:04,500
我们还将导入"display"和"Markdown"，常用来在Jupyter Notebook中显示信息。

25
00:02:04,500 --> 00:02:10,500
我们提供了一个户外服装产品目录的CSV文件，会用它和语言模型结合使用。

26
00:02:10,500 --> 00:02:18,180
在这里，我们将初始化一个名叫CSVLoader的加载器（Loader），并为其指定文件路径。

27
00:02:18,180 --> 00:02:22,800
接下来，我们要导入一个叫VectorStoreIndexCreator的索引（Index）。

28
00:02:22,800 --> 00:02:25,980
借助它们，我们可以很方便地创建一个向量存储。

29
00:02:25,980 --> 00:02:33,940
如下所示，只需几行代码就可以创建。

30
00:02:33,940 --> 00:02:37,020
要创建向量存储，我们需要指定两件事。

31
00:02:37,020 --> 00:02:40,540
第一件事是我们要指定一个向量存储类。

32
00:02:40,540 --> 00:02:46,100
如前所述，我们将使用这个向量存储，因为它特别容易上手。

33
00:02:46,100 --> 00:02:51,820
第二件事是，创建后我们将调用文档加载器(Loader)，传入一个包含一个或多个加载器的列表。

34
00:02:51,820 --> 00:02:58,460
我们只需要一个加载器，也就是传入的这个。

35
00:02:58,460 --> 00:03:02,220
现在已经创建好了向量存储，我们可以开始提问了。

36
00:03:02,220 --> 00:03:06,900
稍后将会向你介绍这背后到底发生了什么。

37
00:03:06,900 --> 00:03:09,140
我们先定义一个包含查询内容的字符串变量"query"。（内容是：“请在表格中列出你所有带防晒功能的衬衫，并为这些衬衫写一份摘要。”）

38
00:03:09,140 --> 00:03:15,729
然后我们用"index.query"生成一个响应，并传入"query"变量。

39
00:03:16,340 --> 00:03:20,900
同样，稍后将会向你介绍这背后到底发生了什么

40
00:03:20,900 --> 00:03:23,100
现在，我们来等等看它返回什么结果。

41
00:03:30,380 --> 00:03:34,540
在完成后，我们可以看看到底返回了什么。

42
00:03:34,540 --> 00:03:40,780
我们得到了一个Markdown格式的表格，列出了所有具有防晒功能的衬衫的名称和描述。

43
00:03:40,780 --> 00:03:45,540
我们还得到了一个很好的小结，是语言模型为我们生成的。

44
00:03:45,540 --> 00:03:52,180
我们已经介绍了如何对文档中的内容进行问答，但底层到底是怎么实现的呢？

45
00:03:52,180 --> 00:03:54,340
首先让我们来梳理一下思路。

46
00:03:54,340 --> 00:03:59,900
想要使用语言模型，并且与大量文档相结合，存在一个关键问题：

47
00:03:59,900 --> 00:04:03,460
语言模型一次只能接收几千个单词。

48
00:04:03,460 --> 00:04:10,300
那么，如果我们有一个很大的文档，如何让语言模型对文档所有内容进行问答呢？

49
00:04:10,300 --> 00:04:14,460
这里就需要Embedding和向量存储发挥作用了。

50
00:04:14,460 --> 00:04:17,780
首先让我们谈谈Embedding。

51
00:04:17,780 --> 00:04:21,740
Embedding将一段文本转换成数字，用一组数字表示这段文本。

52
00:04:21,740 --> 00:04:28,000
这组数字捕捉了它所代表的文字片段的内容含义。

53
00:04:28,000 --> 00:04:31,920
内容相似的文本片段会有相似的向量值。

54
00:04:31,920 --> 00:04:35,300
这样我们就可以在向量空间中比较文本片段。

55
00:04:35,300 --> 00:04:38,940
在下面的例子中，我们可以看到有三个句子，

56
00:04:38,940 --> 00:04:43,220
前两个是关于宠物的，而第三个是关于汽车的。

57
00:04:43,220 --> 00:04:54,180
如果我们观察数值空间中的表示，可以看到当我们比较关于两个关于宠物的句子的向量时，它们相似度非常高。

58
00:04:54,180 --> 00:04:58,260
而如果我们将其中一个与汽车相关的那个句子向量进行比较，它们相似度很低。

59
00:04:58,260 --> 00:05:02,100
利用向量相似度可以让我们轻松地找出哪些文本片段相似，

60
00:05:02,101 --> 00:05:09,940
这对我们非常有用，因为我们可以利用这种技术从文档中找出跟问题相似的文本片段，一起传递给语言模型来帮助回答问题。

61
00:05:09,940 --> 00:05:13,020
接下来我们要介绍的组件是向量数据库。

62
00:05:13,020 --> 00:05:18,060
向量数据库是一种存储方法，可以存储我们在前面创建的那种矢量数字数组。

63
00:05:18,060 --> 00:05:23,780
往向量数据库中新建数据的方式，就是将文档拆分成块，每块生成Embedding，然后把Embedding和原始块一起存储到数据库中。

64
00:05:23,780 --> 00:05:33,429
当我们在处理大一点的文档时，首先要将其拆分成较小的文本块，

66
00:05:33,457 --> 00:05:37,900
因为可能无法将整个文档的内容都传给语言模型。

67
00:05:37,900 --> 00:05:43,300
所以需要把文档拆分成小块，这样每次就只用把最相关的几块内容传递给语言模型。

68
00:05:43,300 --> 00:05:48,940
然后，把每个文本块生成一个Embedding，然后将这些Embedding存储在向量数据库中。

69
00:05:48,940 --> 00:05:51,640
这就是我们创建索引时发生的事情。

70
00:05:51,640 --> 00:05:58,420
索引创建后，我们可以用它来找到与查询内容最相关的几个文本片段。

71
00:05:58,420 --> 00:06:02,220
当一个查询进来时，我们首先将查询的内容生成Embedding，得到一个数字数组。

72
00:06:02,220 --> 00:06:08,000
然后将这个数字数组与向量数据库中的所有向量进行比较，选择最相似的前若干个文本块。

73
00:06:08,000 --> 00:06:14,380
拿到这些文本块后，将这些文本块和原始的查询内容一起传递给语言模型，这样可以让语言模型根据检索出来的文档内容生成最终答案。

74
00:06:14,380 --> 00:06:17,780
在上面，我们用几行代码创建了这个链。

75
00:06:17,780 --> 00:06:19,180
这对于快速入门非常有帮助。

76
00:06:19,180 --> 00:06:25,260
好吧，现在让我们详细了解一下底层到底发生了什么。

77
00:06:25,260 --> 00:06:27,220
第一步与上面类似。

78
00:06:27,220 --> 00:06:36,560
我们将创建一个文档加载器，加载前面提到的产品描述的CSV文件，用做后面问答的文档数据。

79
00:06:36,560 --> 00:06:41,460
然后我们可以借助这个文档加载器中加载所有文档。

80
00:06:41,460 --> 00:06:49,740
如果查看单个文档，可以看到每个文档对应CSV文件中的一个产品。

81
00:06:49,740 --> 00:06:53,060
之前我们讨论过分块。

82
00:06:53,060 --> 00:06:57,100
因为这些文档已经很小了，所以这里我们不需要分块。

83
00:06:57,100 --> 00:07:01,140
可以直接生成Embedding。

84
00:07:01,140 --> 00:07:05,260
要生成Embedding，我们将使用OpenAIEmbedding类。

85
00:07:05,260 --> 00:07:08,220
让我们导入并初始化它。

86
00:07:08,221 --> 00:07:15,600
如果想看看这些Embedding是如何工作的，可以实际看看一段特定的文本时生成的Embedding是什么。

87
00:07:21,180 --> 00:07:26,200
让我们使用Embedding对象上的"embed_query"方法为特定文本生成Embedding。

88
00:07:26,200 --> 00:07:28,660
在这种情况下，句子是"嗨，我叫Harrison。"

89
00:07:31,629 --> 00:07:36,057
如果我们看一下这个Embedding，可以看到有超过一千个(1536)不同的元素。

90
00:07:42,020 --> 00:07:44,620
每个元素都是一个不同的数字。

91
00:07:44,620 --> 00:07:51,340
这些数字就是这段文字的Embedding，1536维向量数字。

92
00:07:51,340 --> 00:07:58,500
我们希望给刚刚加载的所有文本片段生成Embedding，并将它们存储在一个向量存储器中。

93
00:07:58,500 --> 00:08:03,820
通过在向量存储器上调用"from_documents"方法来实现这一点。

94
00:08:03,820 --> 00:08:12,260
这个方法需要一个文档列表、一个Embedding对象，然后我们将创建一个向量存储器。

95
00:08:12,260 --> 00:08:18,020
现在我们可以用这个向量存储器来找到与输入的查询内容类似的文本片段。

96
00:08:18,020 --> 00:08:23,060
让我们看一下查询的内容：“请推荐一件防晒衣”。

97
00:08:23,060 --> 00:08:29,100
在向量存储器上调用“similarity_search”方法并传入查询的内容，就可以查询到一个文档列表。

98
00:08:36,860 --> 00:08:47,400
它返回了四个文档，如果我们看第一个，可以看到它确实是一件关于防晒的衣服。

99
00:08:48,040 --> 00:08:53,060
那么我们如何利用这个来回答我们自己文档中的问题呢？

100
00:08:53,060 --> 00:08:57,100
首先，需要从这个向量存储器创建一个检索器（Retriever）。

101
00:08:57,100 --> 00:09:03,820
检索器是一个通用接口，这个接口定义了一个接收查询内容并返回相似文档的方法。

102
00:09:03,820 --> 00:09:11,260
实现检索器的方法有很多种，基于向量存储和Embedding的检索是其中的一种，除此之外还有其他方法，有简单的有复杂的。

103
00:09:11,260 --> 00:09:20,920
接下来，我们想要返回一个自然语言的回应，所以要导入一个语言模型，我们将使用ChatOpenAI。

104
00:09:20,920 --> 00:09:28,780
接下来我们将手动把检索出来的文档合并成一段文本，

105
00:09:28,780 --> 00:09:34,729
将所有文档中的内容连接起来，并将结果保存到一个变量中，

106
00:09:34,743 --> 00:09:48,980
然后我们会将这个变量的内容和一个问题一起传给LLM，问题内容是：“请用Markdown表格列出所有具有防晒功能的衬衫，并为这些衬衫写一份摘要。”

107
00:09:48,980 --> 00:09:55,780
如果我们在这里打印出返回的结果，可以看到一个完全符合要求的表格。

108
00:09:55,780 --> 00:09:59,900
所有这些步骤都可以用LangChain链来封装。

109
00:09:59,900 --> 00:10:02,980
在这里我们可以创建一个RetrievalQA链，

110
00:10:02,980 --> 00:10:06,860
这个链会对查询进行检索，然后在检索到的文档上进行问答。

111
00:10:06,860 --> 00:10:09,860
为了创建这个链，我们需要传入一些不同的东西。

112
00:10:09,860 --> 00:10:12,200
首先，我们要传入语言模型。

113
00:10:12,200 --> 00:10:15,260
这将用于最后的文本生成。

114
00:10:15,260 --> 00:10:17,660
接下来，我们要传入链类型。

115
00:10:17,660 --> 00:10:18,660
我们将使用"stuff"方法。

116
00:10:18,660 --> 00:10:25,380
这是最简单的方法，因为它只是在调用语言模型时将所有文档内容都一起放到上下文中。

117
00:10:25,380 --> 00:10:31,940
还有一些其他方法可以用来进行问答，我可能会在最后简单提一下，但不会详细讨论。

118
00:10:31,940 --> 00:10:34,460
第三，我们要传入一个检索器。

119
00:10:34,460 --> 00:10:38,660
我们在上面创建的检索器只是一个检索文档的接口，

120
00:10:38,660 --> 00:10:41,860
它将用于检索文档并将结果传递给语言模型。

121
00:10:41,860 --> 00:10:46,340
最后，把"verbose"参数设置为"True"，这样可以打印详细日志。

122
00:10:46,340 --> 00:10:50,929
现在我们可以创建一个查询，并把查询的内容传入链并运行。

123
00:11:08,460 --> 00:11:14,860
当我们得到返回结果后，可以再次使用disaplay和Markdown工具显示结果。

124
00:11:14,860 --> 00:11:20,940
建议你暂停视频，尝试不同的查询内容，并且运行看看结果。

125
00:11:20,940 --> 00:11:22,240
这就是详细操作方法。

126
00:11:22,240 --> 00:11:26,560
但如果你还记得一开始我们用这一行代码就可以轻松完成文档问答。

127
00:11:26,560 --> 00:11:30,260
这两种方式得到的结果是相同的。

128
00:11:30,260 --> 00:11:32,720
这也是LangChain有意思的一点。

129
00:11:32,720 --> 00:11:38,140
你可以用一行代码完成，也可以把它分成五个详细的步骤，可以查看每一步的详细结果。

130
00:11:38,140 --> 00:11:43,000
五个详细的步骤可以让你更具体地知道到底发生了什么。

131
00:11:43,000 --> 00:11:44,740
一句话总结就是很容易上手。

132
00:11:44,740 --> 00:11:48,420
由你自己决定如何使用。

133
00:11:48,420 --> 00:11:51,760
在创建索引时，也可以自定义索引。

134
00:11:51,760 --> 00:11:55,260
如果你还记得，我们手动创建索引时指定了一个Embedding。

135
00:11:55,260 --> 00:11:57,740
我们也可以在这里指定一个Embedding。

136
00:11:57,740 --> 00:12:01,860
这将使我们在生成Embedding时具有更大的灵活性。

137
00:12:01,860 --> 00:12:06,620
我们还可以将这里的向量存储替换为其他类型的向量存储。

138
00:12:06,620 --> 00:12:15,100
在手动创建向量存储时，跟创建索引一样也可以很灵活的定制。

139
00:12:15,100 --> 00:12:16,820
在这个Notebook中我们使用"stuff"方法。

140
00:12:16,820 --> 00:12:19,380
"stuff"方法非常好，因为它很简单。

141
00:12:19,380 --> 00:12:25,220
你只需把所有内容放到Prompt里，然后发送给语言模型，得到返回结果。

142
00:12:25,220 --> 00:12:27,780
这很容易理解发生了什么。

143
00:12:27,780 --> 00:12:30,500
非常便宜，效果也相当好。

144
00:12:30,500 --> 00:12:32,740
但这并不总是有效。

145
00:12:32,740 --> 00:12:39,340
如果你还记得，在Notebook中获取文档时，我们只得到了四个相对较小的文档。

146
00:12:39,340 --> 00:12:44,660
但如果你想在许多不同类型的分块上进行同样类型的问题回答呢？

147
00:12:44,660 --> 00:12:47,060
那么我们可以使用几种不同的方法。

148
00:12:47,060 --> 00:12:48,820
第一个是Map_reduce。

149
00:12:48,820 --> 00:12:55,343
基本就是对所有的分块，把每一块的内容连同问题一起传递给语言模型，得到一个独立返回结果，

151
00:12:55,344 --> 00:13:02,440
然后每一块得到的结果都合并在一起，再使用语言模型来对这些结果进行总结，得到最终答案。

152
00:13:02,440 --> 00:13:06,900
这真的很强大，因为它可以处理任意数量的文档。

153
00:13:06,900 --> 00:13:11,860
而且它也很强大，因为你可以并行处理，同时处理多个分块。

154
00:13:11,860 --> 00:13:13,660
但是这种方法需要更多的调用语言模型。

155
00:13:13,660 --> 00:13:19,060
而且它把所有文档都独立处理，这可能并不总是最理想的结果。

156
00:13:19,060 --> 00:13:24,260
另一种方法是"Refine"，也是用来处理多个文档的。

157
00:13:24,260 --> 00:13:25,580
但它实际上是迭代进行的。

158
00:13:25,580 --> 00:13:28,780
它基于前一个文档的答案。

159
00:13:28,780 --> 00:13:33,900
所以这对于需要整合信息，以及随着时间推移构建答案非常有用。

160
00:13:33,900 --> 00:13:36,580
但它通常会导致更长的答案，

161
00:13:36,580 --> 00:13:39,740
而且速度也不那么快，因为现在每一个文档无法被独立调用，

162
00:13:39,740 --> 00:13:43,380
必须依赖前面的结果。

163
00:13:43,380 --> 00:13:49,980
这意味着它通常需要更长的时间，而且调用次数与Map_reduce一样多。

164
00:13:49,980 --> 00:13:57,980
"Map_rerank"是一个相当有趣但还处于实验阶段的方法，对于每个文档，你只需对语言模型进行一次调用。

165
00:13:57,980 --> 00:14:00,300
另外还需要让它返回一个评分。

166
00:14:00,300 --> 00:14:02,620
然后选择最高分的结果。

167
00:14:02,620 --> 00:14:06,220
这依赖于语言模型知道分数应该是多少。

168
00:14:06,220 --> 00:14:12,580
所以你需要告诉给它指令：“嘿，如果与文档相关，分数应该很高”，并且需要具体优化那部分指令。

169
00:14:12,580 --> 00:14:15,140
与"Map_reduce"方法类似，所有调用都是独立的。

170
00:14:15,140 --> 00:14:16,380
所以你可以批量处理它们。

171
00:14:16,380 --> 00:14:18,220
而且速度相对较快。

172
00:14:18,220 --> 00:14:20,780
不过，你要调用很多次语言模型。

173
00:14:20,780 --> 00:14:22,740
所以会有点贵。

174
00:14:22,740 --> 00:14:29,220
这些方法中最常见的是"stuff"方法，我们在Notebook中用它把所有内容合并成一个文档。

175
00:14:29,220 --> 00:14:35,580
第二常见的是"Map_reduce"方法，它将这些块分别发送到语言模型。

176
00:14:35,580 --> 00:14:42,740
除了问答之外，像stuff、Map_reduce、refine和rerank这些方法，你也可用于其他链上。

177
00:14:42,740 --> 00:14:47,629
比如说"Map_reduce"链用来生成摘要，

178
00:14:47,630 --> 00:14:53,820
在你需要对一个长文档分段递归生成摘要时很常用。

179
00:14:53,820 --> 00:14:56,180
关于文档问答就讲到这里了。

180
00:14:56,180 --> 00:15:00,580
你可能已经注意到，这些链都可以做很多事情。

181
00:15:00,580 --> 00:15:01,891
接下来的部分，

182
00:15:01,892 --> 00:15:06,260
我们将介绍如何更好地理解这些链内部到底发生了什么。
