1
00:00:05,000 --> 00:00:07,000
当你与这些模型互动时，
When you interact with these models,

2
00:00:07,000 --> 00:00:12,160
正常情况下它们无法记住你之前说过的话或之前的对话，
naturally they don't remember what you say before or any of the previous conversation,

3
00:00:12,160 --> 00:00:17,680
这对构建像聊天机器人这样的应用来说是个问题，因为你希望与模型对话时它们能有记忆。
which is an issue when you're building some applications like Chatbot and you want to have a conversation with them.

4
00:00:17,680 --> 00:00:20,520
所以在这一堂课，我们将讨论记忆存储，
And so in this section, we'll cover memory,

5
00:00:20,520 --> 00:00:26,240
也就是如何记住前面的对话内容，并能将其输入到语言模型中，
which is basically how do you remember previous parts of the conversation and feed that into the language model,

6
00:00:26,240 --> 00:00:30,440
这样聊天机器人在与你互动时就能让对话更流畅。
so that they can have this conversational flow as you're interacting with them.

7
00:00:30,440 --> 00:00:36,040
LangChain针对复杂的记忆存储管理提供了多种选项。
Yep. So, LangChain offers multiple sophisticated options for managing these memories.

8
00:00:36,040 --> 00:00:38,040
让我们深入了解一下。
Let's jump in and take a look.

9
00:00:38,040 --> 00:00:44,240
首先，让我导入我的OpenAI API密钥，
So, let me start off by importing my OpenAI API key,

10
00:00:44,240 --> 00:00:47,440
然后让我导入一些我需要的工具。
and then let me import a few tools that I'll need.

11
00:00:47,440 --> 00:00:50,800
让我们以记忆存储为例子，
Let's use as the motivating example for memory,

12
00:00:50,800 --> 00:00:55,440
用LangChain管理聊天或聊天机器人对话。
using LangChain to manage a chat or a chatbot conversation.

13
00:00:55,440 --> 00:01:02,640
为此，我将把LLM设置为OpenAI的聊天对话模式，temperature为0。
So, to do that, I'm going to set the LLM as a chat interface of OpenAI with temperature equals 0.

14
00:01:02,640 --> 00:01:09,560
嗯，我将memory设置为ConversationBufferMemory的实例引用。
And, um, I'm going to use the memory as a ConversationBufferMemory.

15
00:01:09,560 --> 00:01:12,400
稍后你会明白这样做有什么意义。
And you'll see later what this means.

16
00:01:12,400 --> 00:01:15,800
嗯，我要创建一个对话链（ConversationChain）。
Um, and I'm going to build a conversation chain.

17
00:01:15,800 --> 00:01:18,040
在这个短课程的后面，
Again, later in this short course,

18
00:01:18,040 --> 00:01:23,320
Harrison会更深入地讲解LangChain中的链到底是什么。
Harrison will dive much more deeply into what exactly is a chain in LangChain.

19
00:01:23,320 --> 00:01:26,680
现在不用太担心语法细节。
So, don't worry too much about the details of the syntax for now.

20
00:01:26,680 --> 00:01:28,920
但这会构建一个基于LLM的聊天对话。
But this builds an LLM.

21
00:01:28,920 --> 00:01:32,120
如果我开始发送消息，
And if I start to have a conversation,

22
00:01:32,120 --> 00:01:36,840
通过"conversation.predict"函数，输入”嗨，我叫Andrew。“
"conversation.predict", give the input, "Hi, my name is Andrew."

23
00:01:36,840 --> 00:01:38,960
看看它会说什么。
Let's see what it says.

24
00:01:38,960 --> 00:01:40,400
“你好，Andrew。很高兴认识你。”
"Hello, Andrew. It's nice to meet you."

25
00:01:40,400 --> 00:01:41,440
对吧？然后继续。
Right? And so on.

26
00:01:41,440 --> 00:01:44,040
然后，假设我问它，
And then, let's say I ask it,

27
00:01:44,040 --> 00:01:46,640
“一加一等于几？”
"What is one plus one?"

28
00:01:46,640 --> 00:01:49,200
嗯，“一加一等于二。”
Um, one plus one is two.

29
00:01:49,200 --> 00:01:51,400
然后，再问一遍，
And then, ask it again,

30
00:01:51,400 --> 00:01:53,000
“你知道我的名字吗？”
"You know, what's my name?"

31
00:01:53,000 --> 00:01:54,920
“你的名字是Andrew，你之前提到过。”
"Your name is Andrew, as you mentioned earlier."

32
00:01:54,920 --> 00:01:58,680
嗯，有点嘲讽的意味在里面。但我没有证据：）
"Hmm, there's a lot of trace of sarcasm there. Not sure."

33
00:01:58,680 --> 00:02:05,640
如果你想了解LangChain运行时的更多细节，可以把这个"verbose"变量改成"True"，看看它实际在做什么。
And so if you want, you can change this "verbose" variable to "True", to see what LangChain is actually doing.

34
00:02:05,640 --> 00:02:08,840
当你输入“嗨，我叫Andrew。”后运行"conversation.predict"
When you run, um, predict, "Hi, my name is Andrew."

35
00:02:08,840 --> 00:02:11,400
这是LangChain生成的提示词（Prompt）。
This is the prompt that LangChain is generating.

36
00:02:11,400 --> 00:02:16,640
它说，"以下是人类和AI之间的友好对话，健谈的，"等等。
It says, "The following is a friendly conversation between a human and an AI as talkative," and so on.

37
00:02:16,640 --> 00:02:25,920
所以这是LangChain生成的提示词，让系统进行愉快友好的对话，并且必须要有回应，这是生成的内容。
So this is a prompt that LangChain has generated to have the system have a hopeful and friendly conversation and it has to say the conversation and here's the response.

38
00:02:25,920 --> 00:02:33,560
当你在向模型发送第二句话和第三句话时，
And when you execute this on the, um, second and third parts of the conversations,

40
00:02:33,560 --> 00:02:35,560
它会在提示词中保留这些信息：
it keeps the prompt as follows.

41
00:02:35,560 --> 00:02:39,480
注意到当我说：“我的名字是什么？”的时候，
And notice that by the time I'm uttering, "What is my name?"

42
00:02:39,480 --> 00:02:42,760
这是第三轮对话，也是我的第三次输入。
This is the third turn, that's my third input.

43
00:02:42,760 --> 00:02:47,400
它已经按下面的格式存储了当前对话的历史消息：
It has stored the current conversation as follows.

44
00:02:47,400 --> 00:02:48,600
“嗨，我叫Andrew。”
"Hi, my name is Andrew.

45
00:02:48,600 --> 00:02:49,840
“一加一等于几？”
What is one plus one?"

46
00:02:49,840 --> 00:02:56,760
随着时间的推移，这段对话的记忆或历史变得越来越长。
And so on. And so this memory or this history of a conversation gets longer and longer.

47
00:02:56,760 --> 00:03:01,880
实际上，在顶部，我用变量"memory"来保存这个记忆。
In fact, up on top, I had used the memory variable to store the memory.

48
00:03:01,880 --> 00:03:08,400
所以如果我打印"memory.buffer"，它已经存储了到目前为止对话中的所有消息。
So if I were to print "memory.buffer", it has stored the conversation so far.

49
00:03:08,400 --> 00:03:13,520
嗯，你也可以用"memory.load_memory_variables({})"将"memory"中的内容打印出来。
Um, you can also print this out, "memory.load_memory_variables".

50
00:03:13,520 --> 00:03:17,940
这对花括号实际上是一个空字典。
Um, the curly braces here is actually an empty dictionary.

51
00:03:17,940 --> 00:03:24,920
可以通过往这个花括号里面传入一些值来修改选项做一些高级的定制，但在本次课程中我们不打算进一步讨论。
There's some more advanced features that you can use with a more sophisticated input, but we won't talk about them in this short course.

52
00:03:24,920 --> 00:03:28,360
所以不用担心为什么这里有一个空的花括号。
So don't worry about why there's an empty curly braces here.

53
00:03:28,360 --> 00:03:33,440
这是LangChain到目前为止在对话记忆中所记住的内容。
But this is what LangChain has remembered in the memory of the conversation so far.

54
00:03:33,440 --> 00:03:38,120
这就是目前人类和AI之间的所有对话内容。
It's just everything that the AI or that the human has said.

55
00:03:38,120 --> 00:03:41,120
我建议你暂停视频，运行代码。
I encourage you to pause the video and run the code.

56
00:03:41,120 --> 00:03:48,813
LangChain提供了一个ConversationBufferMemory方法来临时存储对话记忆。
So the way that LangChain is storing the conversation is with this ConversationBufferMemory.

57
00:03:48,814 --> 00:03:54,960
要使用ConversationBufferMemory存储消息，可以往其中添加输入和输出。
in order to use the ConversationBufferMemory, to specify a couple of inputs and outputs.

58
00:03:54,960 --> 00:03:59,080
如果你想要往存储里面添加新内容，按照这样的方法做就好了。
This is how you add new things to the memory if you wish to do so explicitly.

59
00:03:59,080 --> 00:04:02,800
通过"memory.save_context"加入：“嗨“，”最近怎么样？”
"memory.save_context" says, "Hi, what's up?"

60
00:04:02,800 --> 00:04:08,840
我知道这对话内容平淡无奇，但我只是想举个简短的例子。
I know this is not the most exciting conversation, but I wanted to have a short example.

61
00:04:08,840 --> 00:04:15,220
嗯，有了这个，这就是记忆存储的状态。
Um, and with that, this is what the status of the memory is.

62
00:04:15,220 --> 00:04:21,240
让我再来打印一下记忆存储中的内容。
And once again, let me actually show the memory variables.

63
00:04:21,240 --> 00:04:27,240
现在，如果你想向记忆存储中添加更多数据，
Now, if you want to add additional data to the memory,

64
00:04:27,240 --> 00:04:29,320
你可以继续保存更多的上下文。
you can keep on saving additional context.

65
00:04:29,320 --> 00:04:33,680
聊天继续：“没什么，就这样”，“挺好的”。
So conversation goes on, not much, just hanging, cool.

66
00:04:33,680 --> 00:04:38,040
如果你把记忆中存储的信息打印出来，现在里面有更多的内容。
And if you print out the memory, you know, there's now more stuff in it.

67
00:04:38,040 --> 00:04:42,640
当你使用大语言模型进行聊天对话中时，
So when you use a large language model for a chat conversation,

68
00:04:42,640 --> 00:04:46,620
大语言模型自身实际上是无状态的。
um, the large language model itself is actually stateless.

69
00:04:46,620 --> 00:04:51,600
语言模型自身不会记住和你对话之间的历史消息。
The language model itself does not remember the conversation you've had so far.

70
00:04:51,600 --> 00:04:55,400
每个请求交互，每次调用API都是独立的。
And each transaction, each call to the API endpoint is independent.

71
00:04:55,400 --> 00:05:07,400
聊天机器人之所以看起来好像有记忆，是因为借助代码的帮助，提供历史消息作为和LLM对话的上下文。
And chatbots appear to have memory only because there's usually rapid code that provides the full conversation that's been had so far as context to the LLM.

72
00:05:07,400 --> 00:05:15,000
所以记忆存储可以明确地存储到目前为止的对话消息，比如“嗨，我叫Andrew。”
And so the memory can store explicitly the terms or the utterances so far, "Hi, my name is Andrew."

73
00:05:15,000 --> 00:05:16,680
“你好，很高兴认识你”等等。
"Hello, it's just nice to meet you," and so on.

74
00:05:16,680 --> 00:05:21,800
这个记忆存储被用作LLM的输入或额外上下文，
And this memory storage is used as input or additional context to the LLM,

75
00:05:21,800 --> 00:05:29,860
这样它在生成输出时，就可以基于之前所说过的会话内容，再生成新的会话，让你感觉它好像“记得”你说过的话。
so that it can generate an output as if it's just having the next conversational turn knowing what's been said before.

76
00:05:29,860 --> 00:05:33,640
随着对话变得越来越长，
And as the conversation becomes long,

77
00:05:33,640 --> 00:05:40,240
所需的记忆存储量也变得非常非常大，而向LLM发送大量令牌（Token）的成本也会增加，
the amounts of memory needed becomes really, really long and does the cost of sending a lot of tokens to the LLM,

78
00:05:40,240 --> 00:05:46,480
因为它通常根据需要处理的令牌数量收费。
which usually charges based on the number of tokens it needs to process, will also become more expensive.

79
00:05:46,480 --> 00:05:54,240
所以LangChain提供了几种便捷的记忆存储方案来存储对话消息和累积对话内容。
So LangChain provides several convenient kinds of memory to store and accumulate the conversation.

80
00:05:54,240 --> 00:05:57,900
到目前为止，我们一直在研究对话的记忆存储方案。
So far, we've been looking at the conversation buffer memory.

81
00:05:57,900 --> 00:06:00,360
现在让我们看看另一种类型的记忆存储方案：
Let's look at a different type of memory.

82
00:06:00,360 --> 00:06:09,800
ConversationBufferWindowMemory，保留窗口记忆，也就是仅保留最后若干轮对话消息。
I'm going to import the ConversationBufferWindowMemory, that only keeps a window of memory.

83
00:06:09,800 --> 00:06:14,280
如果我将传入ConversationBufferWindowMemory的k参数设置为1，
If I set memory to ConversationBufferWindowMemory with k equals 1,

84
00:06:14,280 --> 00:06:20,520
变量k等于1表示我只想记住最后一轮对话，
the variable k equals 1 specifies that I wanted to remember just one conversational exchange.

85
00:06:20,520 --> 00:06:25,360
也就是：我最后发出的一句话和聊天机器人的最后一句话。
That is, one utterance from me and one utterance from the chatbot.

86
00:06:25,360 --> 00:06:31,280
现在如果我让它保存上下文，“嗨，最近怎么样？”，“没什么，就这样。”
So now if I were to have it save context, "Hi, what's up? No much, just hanging."

87
00:06:31,280 --> 00:06:38,820
如果我查看"memory.load_variables"，它只记得最近的话语。
If I were to look at "memory.load_variables", it only remembers the most recent utterance.

88
00:06:38,820 --> 00:06:40,760
注意它已经丢掉了“嗨，最近怎么样？”
Notice it's dropped, "Hi, what's up?"

89
00:06:40,760 --> 00:06:45,440
它只是说，人类说：“没什么，就这样。”，AI说：“酷”
It's just saying, "Human says not much, just hanging" and the AI says, "Cool."

90
00:06:45,440 --> 00:06:47,920
这是因为k等于1。
So that's because k was equal to 1.

91
00:06:47,920 --> 00:06:56,160
这是一个很好的功能，因为它让你跟踪最近的几个对话。
So this is a nice feature because it lets you keep track of just the most recent few conversational terms.

92
00:06:56,160 --> 00:07:02,920
你在实际使用这个功能时，可能不会用k等于1，而是将k设置为一个较大的数字。
In practice, you probably won't use this with k equals 1, you use this with k set to a larger number.

93
00:07:02,920 --> 00:07:10,420
但是一样可以防止记忆存储量随着对话的进行而无限增长。
But still, this prevents the memory from growing without limit, as the conversation goes longer.

94
00:07:10,420 --> 00:07:25,580
所以如果我再来一次刚才的对话，我们会说，“嗨，我叫Andrew”，“1加1等于几？”
And so if I were to rerun the conversation that we had just now, we'll say, "Hi, my name is Andrew. What is 1 plus 1?"

95
00:07:25,580 --> 00:07:30,980
现在我问它：“我的名字是什么？”
And now I ask it, "What is my name?"

96
00:07:30,980 --> 00:07:37,240
因为k等于1，它只记得上一次的会话，关于1加1等于几？
Because k equals 1, it only remembers the last exchange versus what is 1 plus 1?

98
00:07:37,240 --> 00:07:41,260
答案是1加1等于2，但现在已经忘记了之前交流的内容，
The answer is 1 plus 1 equals 2, and it's forgotten this early exchange which is now,

99
00:07:41,260 --> 00:07:45,060
现在说：“抱歉，无法获取那些信息。”
now says, "Sorry, don't have access to that information."

100
00:07:45,060 --> 00:07:56,660
我建议你暂停视频，在左侧代码中将"verbose"参数设置为"True"，然后重新运行这个对话。
One thing I hope you will do is pause the video, change this to "True" in the code on the left, and rerun this conversation with verbose equals "True".

101
00:07:56,660 --> 00:08:00,540
然后你会看到实际运行时用到的提示词。
And then you will see the prompts actually used to generate this.

102
00:08:00,540 --> 00:08:07,840
希望你能看到，当你问LLM：“我的名字是什么？”时，
And hopefully you'll see that the memory, when you're calling the LLM on what is my name,

103
00:08:07,840 --> 00:08:15,920
在它的提示词中，已经丢失了前面有关名字的交流，所以现在它说不知道我的名字是什么。
that the memory has dropped this exchange where it learned what is my name, which is why it now says it doesn't know what is my name.

104
00:08:15,920 --> 00:08:28,320
使用ConversationalTokenBufferMemory，将限制保存在记忆存储的令牌数量。
With the ConversationalTokenBufferMemory, the memory will limit the number of tokens saved.

105
00:08:28,320 --> 00:08:38,580
由于很多LLM定价是基于令牌的，令牌的数量直接反映了LLM调用的成本。
And because a lot of LLM pricing is based on tokens, this maps more directly to the cost of the LLM calls.

106
00:08:38,580 --> 00:08:47,060
如果我设置"max_token_limit"为50，实际上让我插入一些消息。
So if I were to say the "max_token_limit" is equal to 50, and actually let me inject a few comments.

107
00:08:47,060 --> 00:08:51,140
比如说对话是，“AI是什么？”，“太棒了！”
So let's say the conversation is, "AI is what?", "Amazing!".

108
00:08:51,140 --> 00:08:53,020
“反向传播是什么？”, “美丽！”
"Backpropagation is what?", "Beautiful!".

109
00:08:53,020 --> 00:08:54,500
“聊天机器人是什么？”,“迷人！”
"Chatbot is what?", "Charming!".

110
00:08:54,500 --> 00:08:58,540
我用ABC作为所有这些对话单词的第一个字母。
I use ABC as the first letter of all of these conversational terms.

111
00:08:58,540 --> 00:09:02,620
这样我们可以记录什么时候说了什么。
We can keep track of, um, what was said when.

112
00:09:02,620 --> 00:09:08,620
如果我把令牌限制的值调的比较高，运行这段代码，它几乎可以包含整个对话。
If I run this with a high token limit, um, it has almost the whole conversation.

113
00:09:08,620 --> 00:09:16,300
如果我把令牌限制的值提高到100，现在它有整个从“AI是什么？”开始的对话。
If I increase the token limit to 100, it now has the whole conversation starting with "AI is what?".

114
00:09:16,300 --> 00:09:31,940
如果我减少值，那么它会删掉这个对话的最早的那部分消息，只保留最近对话的消息，并且保证总的消息内容长度不超过设置的令牌限制的值。
If I decrease it, then, you know, it chops off the earlier parts of this conversation to retain the number of tokens corresponding to the most recent exchanges, um, but subject to not exceeding the token limit.

116
00:09:31,940 --> 00:09:39,180
如果你想知道为什么我们需要指定一个LLM参数，那是因为不同的LLM使用不同的令牌计算方法。
And in case you're wondering why we needed to specify an LLM, is because different LLMs use different ways of counting tokens.

117
00:09:39,180 --> 00:09:46,500
所以这告诉它使用ChatOpenAI LLM使用的计算令牌的方法。
So this tells it to use the way of counting tokens that the, um, ChatOpenAI LLM uses.

118
00:09:46,500 --> 00:09:54,420
我建议你暂停视频，运行代码，尝试修改提示词，看看能否得到不同的输出。
I encourage you to pause the video and run the code, and also try modifying the prompt to see if you can get a different output.

119
00:09:54,420 --> 00:09:58,300
我想在这里说明的最后一种记忆存储类型是
Finally, there's one last type of memory I want to illustrate here,

120
00:09:58,300 --> 00:10:04,320
那就是ConversationSummaryBufferMemory。
which is the ConversationSummaryBufferMemory.

121
00:10:04,320 --> 00:10:15,040
这个想法是，与其将记忆的存储量限制在最近若干对话数量上，或限制在令牌数量上，
And the idea is, instead of limiting the memory to fixed number of tokens based on the most recent utterances or a fixed number of conversational exchanges,

122
00:10:15,040 --> 00:10:23,900
不如让LLM为所有历史消息生成摘要，在记忆中存储历史消息的摘要。
let's use an LLM to write a summary of the conversation so far, and let that be the memory.

123
00:10:23,900 --> 00:10:28,700
来举一个例子，我将创建一个关于某人日程安排的长字符串。
So here's an example where I'm going to create a long string with someone's schedule.

124
00:10:28,700 --> 00:10:33,720
比如说，早上8点与产品团队有一个会议，需要PowerPoint演示文稿，等等。
You know, there's meeting at 8AM with your product team, you need your PowerPoint presentation, and so on and so on.

125
00:10:33,720 --> 00:10:37,920
这是一个长字符串，说的是你的日程安排，
So this is a long string saying what's your schedule, you know,

126
00:10:37,920 --> 00:10:46,240
可能以中午在意大利餐厅与客户共进午餐结束，带上你的笔记本电脑，展示最新的LLM演示。
maybe ending with a noon lunch at the Italian restaurant with a customer, bring your laptop, show the latest LLM demo.

127
00:10:46,240 --> 00:10:52,600
那么，让我使用ConversationSummaryBufferMemory，
And so let me use a ConversationSummaryBufferMemory,

128
00:10:52,600 --> 00:10:57,880
在这种情况下，最大令牌限制为400，相当高的令牌限制。
um, with a max token limits of 400 in this case, pretty high token limit.

129
00:10:57,880 --> 00:11:10,143
我要加入一些对话内容，比如："你好"，"最近怎么样"，"没什么，就这样"，"嗯，酷。"
And I'm going to insert in a few conversational terms in which we start with, "Hello", "what's up". "Not much, just hanging", "uh, cool."

130
00:11:10,144 --> 00:11:13,440
然后是："今天的日程安排是什么？"
And then, "What is on the schedule today?"

131
00:11:13,440 --> 00:11:17,040
回答是，前面那个很长的日程安排字符串。
And the response is, you know, that long schedule.

132
00:11:17,040 --> 00:11:22,440
所以现在这个记忆存储里有很多文本。
So this memory now has quite a lot of text in it.

133
00:11:22,440 --> 00:11:37,200
事实上，让我们看一下记忆存储中的内容，它包含了所有的文本，因为400个令牌足以存储所有这些文本。
In fact, let's take a look, um, at the memory variables, it contains that entire, um, piece of text because 400 tokens was sufficient to store all this text.

134
00:11:37,200 --> 00:11:46,580
但是，如果我现在把最大令牌数限制减少到100个令牌，记住这里存储了整个对话历史。
But now, if I were to reduce the max_token_limit, say to 100 tokens, remember this stores the entire conversational history.

135
00:11:46,580 --> 00:11:57,640
如果我把令牌数限制减少到100，那么ConversationSummaryBufferMemory实际会调用LLM，
If I reduce the number of tokens to 100, then the ConversationSummaryBufferMemory has actually used an LLM,

136
00:11:57,640 --> 00:12:05,360
在这种情况下是OpenAI API，因为这就是我们设置的LLM，来生成当前所有会话内容的摘要。
the OpenAI endpoint in this case because that's what we have set the LLM to, to actually generate a summary of the conversation so far.

137
00:12:05,360 --> 00:12:12,840
所以摘要是：“人类和AI进行了闲聊，然后讨论了当天的日程安排，AI告诉人类早上有一个会议”，等等，
So the summary is, "The human and AI engage in small talk before discussing the day's schedule, and AI informs the human of a morning meeting, blah, blah, blah,

138
00:12:12,840 --> 00:12:17,920
嗯，还有一个与客户一起的午餐会议，客户对最新AI发展感兴趣。
um, and a lunch meeting with a customer interested in the latest AI developments."

139
00:12:17,920 --> 00:12:32,880
如果我们用这个LLM进行对话，然后创建一个ConversationChain，就像之前一样。
And so, if we were to have a conversation using this LLM, then create a conversation chain, same as before.

140
00:12:32,880 --> 00:12:40,840
那么，假设我们要问：“（给客户）演示什么比较好？”
And, um, let's say that we were to ask, you know, input what would be a good demo to show.

141
00:12:40,840 --> 00:12:43,000
我设置了“verbose”等于“True”。
Um, I set "variables" equals "True".

142
00:12:43,000 --> 00:12:44,720
这是提示词。
So here's the prompt.

143
00:12:44,720 --> 00:12:55,840
这是LLM知道的目前已经进行过的对话讨论，因为这里有对之前对话的总结。
The LLM thinks the current conversation has had this discussion so far, because that's the summary of the conversation.

144
00:12:55,840 --> 00:12:58,400
还有一点需要注意，
And just one note,

145
00:12:58,400 --> 00:13:06,720
如果你熟悉OpenAI Chat API，通常要设置一个特定的系统消息。
if you're familiar with the OpenAI chat API endpoint, there is a specific system message.

146
00:13:06,720 --> 00:13:14,360
在这个例子中，这并不是一个OpenAI Chat的系统消息，而是提示词中用来描述历史会话摘要的部分。
In this example, this is not using the official OpenAI system message, it's just including it as part of the prompt here.

147
00:13:14,360 --> 00:13:16,600
但是，它还是效果很好。
But, but it nonetheless works pretty well.

148
00:13:16,600 --> 00:13:24,760
有了这个提示词，基于客户对AI发展很有兴趣，LLM建议向客户演示我们最新的自然语言处理能力。
And given this prompt, you know, the LLM outputs base on customer interest in AI developments, so it's just showcasing our latest NLP capabilities.

149
00:13:24,760 --> 00:13:26,200
好的，这很酷。
Okay, that's cool.

150
00:13:26,200 --> 00:13:29,800
嗯，这个就是，
Um, well, it's, you know,

151
00:13:29,800 --> 00:13:34,240
一个让LLM给你的酷炫演示提供建议的例子，这可能会让你想：“如果我在见客户的时候，
making some suggestions to the cool demos and makes you think, if I was meeting a customer,

152
00:13:34,240 --> 00:13:42,760
我会说，哇，如果有开源框架可以帮我用LLM构建酷炫的NLP应用就好了！”
I would say, boy, if only there were open source framework available to help me build cool NLP applications using LLMs.

153
00:13:42,760 --> 00:13:44,920
嗯，这样优秀的项目正在推出。
Hmm, good things are launching.

154
00:13:44,920 --> 00:13:54,920
有趣的是，现在看看记忆存储中发生了什么。
Um, and the interesting thing is, if you now look at what has happened to the memory.

155
00:13:54,920 --> 00:14:04,240
注意到这里，它已经包含了最近的AI系统输出，
So notice that, um, here it has incorporated the most recent AI system output,

156
00:14:04,240 --> 00:14:10,920
而我问它的问题：“演示什么比较好？”，已经被归纳进了系统消息。
whereas my utterance asking it, what would be a good demo to show, has been incorporated into the system message.

157
00:14:10,920 --> 00:14:14,240
嗯，到目前为止，整个对话的总结。
Um, you know, the overall summary of the conversation so far.

158
00:14:14,240 --> 00:14:27,240
对于ConversationSummaryBufferMemory，它试图将消息的显性记忆保持在我们指定的令牌数上限。
With the ConversationSummaryBufferMemory, what it tries to do is keep the explicit storage of the messages up to the number of tokens we have specified as a limit.

159
00:14:27,240 --> 00:14:34,120
所以，这部分显性记忆，我们试图将其限制在100个令牌，因为这是我们前面指定的。
So, you know, this part, the explicit storage, we're trying to cap at 100 tokens because that's what we're asked for.

160
00:14:34,120 --> 00:14:40,680
然后，它会使用LLM生成摘要，就像这里看到的那样。
And then anything beyond that, it will use the LLM to generate a summary, which is what is seen up here.

161
00:14:40,680 --> 00:14:46,640
尽管我用聊天作为例子来说明这些不同的记忆存储方案，
And even though I've illustrated these different memories using a chat as a running example,

162
00:14:46,640 --> 00:14:49,800
这些记忆存储方案对其他应用程序也很有用，
these memories are useful for other applications too,

163
00:14:49,800 --> 00:14:54,760
比如你可能会不断收到新的文本片段或新的信息，
where you might keep on getting new snippets of text, or keep on getting new information,

164
00:14:54,760 --> 00:14:59,200
例如，如果你的系统需要经常上网检索内容，
such as if your system repeatedly goes online to search for facts,

165
00:14:59,200 --> 00:15:07,560
检索结果会存储在一个列表中，但你希望列表的存储总数保持在一个限定的范围内，而不是无限地增长。
but you want to keep the total memory used to store this growing list of facts as you know, capped and not growing arbitrarily long.

166
00:15:07,560 --> 00:15:10,360
我建议你暂停视频并运行代码。
I encourage you to pause the video and run the code.

167
00:15:10,360 --> 00:15:15,160
在这个视频中，你看到了几种类型的记忆存储方案，
In this video, you saw a few types of memory,

168
00:15:15,160 --> 00:15:26,520
包括基于对话次数或令牌数量限制的记忆存储方案，或者可以对超过特定令牌数的会话内容进行总结的方案。
including buffer memories that limits based on number of conversation exchanges or tokens or a memory that can summarize tokens above a certain limit.

169
00:15:26,520 --> 00:15:29,880
LangChain实际上还支持其他类型的记忆存储。
LangChain actually supports additional memory types as well.

170
00:15:29,880 --> 00:15:33,160
其中最强大的是向量数据存储。
One of the most powerful is vector data memory.

171
00:15:33,160 --> 00:15:39,040
如果你熟悉词嵌入（Embeddings）和文本嵌入，向量数据库实际上就是存储这些嵌入的。
If you're familiar with word embeddings and text embeddings, the vector database actually stores such embeddings.

172
00:15:39,040 --> 00:15:41,480
如果你不知道这是什么意思，不用担心。
If you don't know what that means, don't worry about it.

173
00:15:41,480 --> 00:15:43,320
Harrison稍后会解释。
Harrison will explain it later.

174
00:15:43,320 --> 00:15:51,120
它可以使用这种向量数据库来检索最相关的文本块。
And it can then retrieve the most relevant blocks of text using this type of vector database for its memory.

175
00:15:51,120 --> 00:15:54,480
LangChain还支持实体记忆存储，
And LangChain also supports entity memories,

176
00:15:54,480 --> 00:16:00,640
当你想记住关于特定人或其他实体的详细信息时，这是适用的，
which is applicable when you wanted to remember details about specific people or specific other entities,

177
00:16:00,640 --> 00:16:12,280
比如，如果你谈论一个特定的朋友，你可以让LangChain记住关于那个朋友的事实，这将以明确的方式成为一个实体。
such as if you talk about a specific friend, you can have LangChain remember facts about that friend, which would be an entity in an explicit way.

178
00:16:12,280 --> 00:16:14,600
当你使用LangChain实现应用程序时，
When you're implementing applications using LangChain,

179
00:16:14,600 --> 00:16:17,240
你还可以使用多种类型的记忆存储，
you can also use multiple types of memories,

180
00:16:17,240 --> 00:16:26,480
比如使用本视频中看到的某种对话记忆存储类型，再加上实体记忆存储来回忆个人。
such as using one of the types of conversation memory that you saw in this video, plus additionally, entity memory to recall individuals.

181
00:16:26,480 --> 00:16:35,160
这样，你可以记住对话的大致内容，以及明确记录对话中重要人物的重要事实。
So this way, you can remember maybe a summary of the conversation, plus an explicit way of storing important facts about important people in the conversation.

182
00:16:35,160 --> 00:16:38,000
当然，除了使用这些记忆存储类型，
And of course, in addition to using these memory types,

183
00:16:38,000 --> 00:16:45,920
开发者也经常将整个对话存储在传统数据库中，如键值存储（key-value store）或SQL数据库。
it's also not uncommon for developers to store the entire conversation in the conventional database, some sort of key-value store or SQL database.

184
00:16:45,920 --> 00:16:51,560
这样你可以回顾整个对话，进行审计或进一步改进系统。
So you could refer back to the whole conversation for auditing or for improving the system further.

185
00:16:51,560 --> 00:16:53,680
这就是记忆存储类型。
And so that's memory types.

186
00:16:53,680 --> 00:16:57,000
希望这些知识能有效地帮助你更好的构建自己的应用程序。
I hope you find this useful building your own applications.

187
00:16:57,000 --> 00:17:05,200
现在，让我们继续下一个视频，了解LangChain的关键构建模块，也就是链。
And now, let's go on to the next video to learn about the key building block of LangChain, namely, the chain.
