1
00:00:05,000 --> 00:00:07,000
当你与这些模型互动时，

2
00:00:07,000 --> 00:00:12,160
正常情况下它们无法记住你之前说过的话或之前的对话，

3
00:00:12,160 --> 00:00:17,680
这对构建像聊天机器人这样的应用来说是个问题，因为你希望与模型对话时它们能有记忆。

4
00:00:17,680 --> 00:00:20,520
所以在这一堂课，我们将讨论记忆存储，

5
00:00:20,520 --> 00:00:26,240
也就是如何记住前面的对话内容，并能将其输入到语言模型中，

6
00:00:26,240 --> 00:00:30,440
这样聊天机器人在与你互动时就能让对话更流畅。

7
00:00:30,440 --> 00:00:36,040
LangChain针对复杂的记忆存储管理提供了多种选项。

8
00:00:36,040 --> 00:00:38,040
让我们深入了解一下。

9
00:00:38,040 --> 00:00:44,240
首先，让我导入我的OpenAI API密钥，

10
00:00:44,240 --> 00:00:47,440
然后让我导入一些我需要的工具。

11
00:00:47,440 --> 00:00:50,800
让我们以记忆存储为例子，

12
00:00:50,800 --> 00:00:55,440
用LangChain管理聊天或聊天机器人对话。

13
00:00:55,440 --> 00:01:02,640
为此，我将把LLM设置为OpenAI的聊天对话模式，temperature为0。

14
00:01:02,640 --> 00:01:09,560
嗯，我将memory设置为ConversationBufferMemory的实例引用。

15
00:01:09,560 --> 00:01:12,400
稍后你会明白这样做有什么意义。

16
00:01:12,400 --> 00:01:15,800
嗯，我要创建一个对话链（ConversationChain）。

17
00:01:15,800 --> 00:01:18,040
在这个短课程的后面，

18
00:01:18,040 --> 00:01:23,320
Harrison会更深入地讲解LangChain中的链到底是什么。

19
00:01:23,320 --> 00:01:26,680
现在不用太担心语法细节。

20
00:01:26,680 --> 00:01:28,920
但这会构建一个基于LLM的聊天对话。

21
00:01:28,920 --> 00:01:32,120
如果我开始发送消息，

22
00:01:32,120 --> 00:01:36,840
通过"conversation.predict"函数，输入”嗨，我叫Andrew。“

23
00:01:36,840 --> 00:01:38,960
看看它会说什么。

24
00:01:38,960 --> 00:01:40,400
“你好，Andrew。很高兴认识你。”

25
00:01:40,400 --> 00:01:41,440
对吧？然后继续。

26
00:01:41,440 --> 00:01:44,040
然后，假设我问它，

27
00:01:44,040 --> 00:01:46,640
“一加一等于几？”

28
00:01:46,640 --> 00:01:49,200
嗯，“一加一等于二。”

29
00:01:49,200 --> 00:01:51,400
然后，再问一遍，

30
00:01:51,400 --> 00:01:53,000
“你知道我的名字吗？”

31
00:01:53,000 --> 00:01:54,920
“你的名字是Andrew，你之前提到过。”

32
00:01:54,920 --> 00:01:58,680
嗯，有点嘲讽的意味在里面。但我没有证据：）

33
00:01:58,680 --> 00:02:05,640
如果你想了解LangChain运行时的更多细节，可以把这个"verbose"变量改成"True"，看看它实际在做什么。

34
00:02:05,640 --> 00:02:08,840
当你输入“嗨，我叫Andrew。”后运行"conversation.predict"

35
00:02:08,840 --> 00:02:11,400
这是LangChain生成的提示词（Prompt）。

36
00:02:11,400 --> 00:02:16,640
它说，"以下是人类和AI之间的友好对话，健谈的，"等等。

37
00:02:16,640 --> 00:02:25,920
所以这是LangChain生成的提示词，让系统进行愉快友好的对话，并且必须要有回应，这是生成的内容。

38
00:02:25,920 --> 00:02:33,560
当你在向模型发送第二句话和第三句话时，

40
00:02:33,560 --> 00:02:35,560
它会在提示词中保留这些信息：

41
00:02:35,560 --> 00:02:39,480
注意到当我说：“我的名字是什么？”的时候，

42
00:02:39,480 --> 00:02:42,760
这是第三轮对话，也是我的第三次输入。

43
00:02:42,760 --> 00:02:47,400
它已经按下面的格式存储了当前对话的历史消息：

44
00:02:47,400 --> 00:02:48,600
“嗨，我叫Andrew。”

45
00:02:48,600 --> 00:02:49,840
“一加一等于几？”

46
00:02:49,840 --> 00:02:56,760
随着时间的推移，这段对话的记忆或历史变得越来越长。

47
00:02:56,760 --> 00:03:01,880
实际上，在顶部，我用变量"memory"来保存这个记忆。

48
00:03:01,880 --> 00:03:08,400
所以如果我打印"memory.buffer"，它已经存储了到目前为止对话中的所有消息。

49
00:03:08,400 --> 00:03:13,520
嗯，你也可以用"memory.load_memory_variables({})"将"memory"中的内容打印出来。

50
00:03:13,520 --> 00:03:17,940
这对花括号实际上是一个空字典。

51
00:03:17,940 --> 00:03:24,920
可以通过往这个花括号里面传入一些值来修改选项做一些高级的定制，但在本次课程中我们不打算进一步讨论。

52
00:03:24,920 --> 00:03:28,360
所以不用担心为什么这里有一个空的花括号。

53
00:03:28,360 --> 00:03:33,440
这是LangChain到目前为止在对话记忆中所记住的内容。

54
00:03:33,440 --> 00:03:38,120
这就是目前人类和AI之间的所有对话内容。

55
00:03:38,120 --> 00:03:41,120
我建议你暂停视频，运行代码。

56
00:03:41,120 --> 00:03:48,813
LangChain提供了一个ConversationBufferMemory方法来临时存储对话记忆。

57
00:03:48,814 --> 00:03:54,960
要使用ConversationBufferMemory存储消息，可以往其中添加输入和输出。

58
00:03:54,960 --> 00:03:59,080
如果你想要往存储里面添加新内容，按照这样的方法做就好了。

59
00:03:59,080 --> 00:04:02,800
通过"memory.save_context"加入：“嗨“，”最近怎么样？”

60
00:04:02,800 --> 00:04:08,840
我知道这对话内容平淡无奇，但我只是想举个简短的例子。

61
00:04:08,840 --> 00:04:15,220
嗯，有了这个，这就是记忆存储的状态。

62
00:04:15,220 --> 00:04:21,240
让我再来打印一下记忆存储中的内容。

63
00:04:21,240 --> 00:04:27,240
现在，如果你想向记忆存储中添加更多数据，

64
00:04:27,240 --> 00:04:29,320
你可以继续保存更多的上下文。

65
00:04:29,320 --> 00:04:33,680
聊天继续：“没什么，就这样”，“挺好的”。

66
00:04:33,680 --> 00:04:38,040
如果你把记忆中存储的信息打印出来，现在里面有更多的内容。

67
00:04:38,040 --> 00:04:42,640
当你使用大语言模型进行聊天对话中时，

68
00:04:42,640 --> 00:04:46,620
大语言模型自身实际上是无状态的。

69
00:04:46,620 --> 00:04:51,600
语言模型自身不会记住和你对话之间的历史消息。

70
00:04:51,600 --> 00:04:55,400
每个请求交互，每次调用API都是独立的。

71
00:04:55,400 --> 00:05:07,400
聊天机器人之所以看起来好像有记忆，是因为借助代码的帮助，提供历史消息作为和LLM对话的上下文。

72
00:05:07,400 --> 00:05:15,000
所以记忆存储可以明确地存储到目前为止的对话消息，比如“嗨，我叫Andrew。”

73
00:05:15,000 --> 00:05:16,680
“你好，很高兴认识你”等等。

74
00:05:16,680 --> 00:05:21,800
这个记忆存储被用作LLM的输入或额外上下文，

75
00:05:21,800 --> 00:05:29,860
这样它在生成输出时，就可以基于之前所说过的会话内容，再生成新的会话，让你感觉它好像“记得”你说过的话。

76
00:05:29,860 --> 00:05:33,640
随着对话变得越来越长，

77
00:05:33,640 --> 00:05:40,240
所需的记忆存储量也变得非常非常大，而向LLM发送大量令牌（Token）的成本也会增加，

78
00:05:40,240 --> 00:05:46,480
因为它通常根据需要处理的令牌数量收费。

79
00:05:46,480 --> 00:05:54,240
所以LangChain提供了几种便捷的记忆存储方案来存储对话消息和累积对话内容。

80
00:05:54,240 --> 00:05:57,900
到目前为止，我们一直在研究对话的记忆存储方案。

81
00:05:57,900 --> 00:06:00,360
现在让我们看看另一种类型的记忆存储方案：

82
00:06:00,360 --> 00:06:09,800
ConversationBufferWindowMemory，保留窗口记忆，也就是仅保留最后若干轮对话消息。

83
00:06:09,800 --> 00:06:14,280
如果我将传入ConversationBufferWindowMemory的k参数设置为1，

84
00:06:14,280 --> 00:06:20,520
变量k等于1表示我只想记住最后一轮对话，

85
00:06:20,520 --> 00:06:25,360
也就是：我最后发出的一句话和聊天机器人的最后一句话。

86
00:06:25,360 --> 00:06:31,280
现在如果我让它保存上下文，“嗨，最近怎么样？”，“没什么，就这样。”

87
00:06:31,280 --> 00:06:38,820
如果我查看"memory.load_variables"，它只记得最近的话语。

88
00:06:38,820 --> 00:06:40,760
注意它已经丢掉了“嗨，最近怎么样？”

89
00:06:40,760 --> 00:06:45,440
它只是说，人类说：“没什么，就这样。”，AI说：“酷”

90
00:06:45,440 --> 00:06:47,920
这是因为k等于1。

91
00:06:47,920 --> 00:06:56,160
这是一个很好的功能，因为它让你跟踪最近的几个对话。

92
00:06:56,160 --> 00:07:02,920
你在实际使用这个功能时，可能不会用k等于1，而是将k设置为一个较大的数字。

93
00:07:02,920 --> 00:07:10,420
但是一样可以防止记忆存储量随着对话的进行而无限增长。

94
00:07:10,420 --> 00:07:25,580
所以如果我再来一次刚才的对话，我们会说，“嗨，我叫Andrew”，“1加1等于几？”

95
00:07:25,580 --> 00:07:30,980
现在我问它：“我的名字是什么？”

96
00:07:30,980 --> 00:07:37,240
因为k等于1，它只记得上一次的会话，关于1加1等于几？

98
00:07:37,240 --> 00:07:41,260
答案是1加1等于2，但现在已经忘记了之前交流的内容，

99
00:07:41,260 --> 00:07:45,060
现在说：“抱歉，无法获取那些信息。”

100
00:07:45,060 --> 00:07:56,660
我建议你暂停视频，在左侧代码中将"verbose"参数设置为"True"，然后重新运行这个对话。

101
00:07:56,660 --> 00:08:00,540
然后你会看到实际运行时用到的提示词。

102
00:08:00,540 --> 00:08:07,840
希望你能看到，当你问LLM：“我的名字是什么？”时，

103
00:08:07,840 --> 00:08:15,920
在它的提示词中，已经丢失了前面有关名字的交流，所以现在它说不知道我的名字是什么。

104
00:08:15,920 --> 00:08:28,320
使用ConversationalTokenBufferMemory，将限制保存在记忆存储的令牌数量。

105
00:08:28,320 --> 00:08:38,580
由于很多LLM定价是基于令牌的，令牌的数量直接反映了LLM调用的成本。

106
00:08:38,580 --> 00:08:47,060
如果我设置"max_token_limit"为50，实际上让我插入一些消息。

107
00:08:47,060 --> 00:08:51,140
比如说对话是，“AI是什么？”，“太棒了！”

108
00:08:51,140 --> 00:08:53,020
“反向传播是什么？”, “美丽！”

109
00:08:53,020 --> 00:08:54,500
“聊天机器人是什么？”,“迷人！”

110
00:08:54,500 --> 00:08:58,540
我用ABC作为所有这些对话单词的第一个字母。

111
00:08:58,540 --> 00:09:02,620
这样我们可以记录什么时候说了什么。

112
00:09:02,620 --> 00:09:08,620
如果我把令牌限制的值调的比较高，运行这段代码，它几乎可以包含整个对话。

113
00:09:08,620 --> 00:09:16,300
如果我把令牌限制的值提高到100，现在它有整个从“AI是什么？”开始的对话。

114
00:09:16,300 --> 00:09:31,940
如果我减少值，那么它会删掉这个对话的最早的那部分消息，只保留最近对话的消息，并且保证总的消息内容长度不超过设置的令牌限制的值。

116
00:09:31,940 --> 00:09:39,180
如果你想知道为什么我们需要指定一个LLM参数，那是因为不同的LLM使用不同的令牌计算方法。

117
00:09:39,180 --> 00:09:46,500
所以这告诉它使用ChatOpenAI LLM使用的计算令牌的方法。

118
00:09:46,500 --> 00:09:54,420
我建议你暂停视频，运行代码，尝试修改提示词，看看能否得到不同的输出。

119
00:09:54,420 --> 00:09:58,300
我想在这里说明的最后一种记忆存储类型是

120
00:09:58,300 --> 00:10:04,320
那就是ConversationSummaryBufferMemory。

121
00:10:04,320 --> 00:10:15,040
这个想法是，与其将记忆的存储量限制在最近若干对话数量上，或限制在令牌数量上，

122
00:10:15,040 --> 00:10:23,900
不如让LLM为所有历史消息生成摘要，在记忆中存储历史消息的摘要。

123
00:10:23,900 --> 00:10:28,700
来举一个例子，我将创建一个关于某人日程安排的长字符串。

124
00:10:28,700 --> 00:10:33,720
比如说，早上8点与产品团队有一个会议，需要PowerPoint演示文稿，等等。

125
00:10:33,720 --> 00:10:37,920
这是一个长字符串，说的是你的日程安排，

126
00:10:37,920 --> 00:10:46,240
可能以中午在意大利餐厅与客户共进午餐结束，带上你的笔记本电脑，展示最新的LLM演示。

127
00:10:46,240 --> 00:10:52,600
那么，让我使用ConversationSummaryBufferMemory，

128
00:10:52,600 --> 00:10:57,880
在这种情况下，最大令牌限制为400，相当高的令牌限制。

129
00:10:57,880 --> 00:11:10,143
我要加入一些对话内容，比如："你好"，"最近怎么样"，"没什么，就这样"，"嗯，酷。"

130
00:11:10,144 --> 00:11:13,440
然后是："今天的日程安排是什么？"

131
00:11:13,440 --> 00:11:17,040
回答是，前面那个很长的日程安排字符串。

132
00:11:17,040 --> 00:11:22,440
所以现在这个记忆存储里有很多文本。

133
00:11:22,440 --> 00:11:37,200
事实上，让我们看一下记忆存储中的内容，它包含了所有的文本，因为400个令牌足以存储所有这些文本。

134
00:11:37,200 --> 00:11:46,580
但是，如果我现在把最大令牌数限制减少到100个令牌，记住这里存储了整个对话历史。

135
00:11:46,580 --> 00:11:57,640
如果我把令牌数限制减少到100，那么ConversationSummaryBufferMemory实际会调用LLM，

136
00:11:57,640 --> 00:12:05,360
在这种情况下是OpenAI API，因为这就是我们设置的LLM，来生成当前所有会话内容的摘要。

137
00:12:05,360 --> 00:12:12,840
所以摘要是：“人类和AI进行了闲聊，然后讨论了当天的日程安排，AI告诉人类早上有一个会议”，等等，

138
00:12:12,840 --> 00:12:17,920
嗯，还有一个与客户一起的午餐会议，客户对最新AI发展感兴趣。

139
00:12:17,920 --> 00:12:32,880
如果我们用这个LLM进行对话，然后创建一个ConversationChain，就像之前一样。

140
00:12:32,880 --> 00:12:40,840
那么，假设我们要问：“（给客户）演示什么比较好？”

141
00:12:40,840 --> 00:12:43,000
我设置了“verbose”等于“True”。

142
00:12:43,000 --> 00:12:44,720
这是提示词。

143
00:12:44,720 --> 00:12:55,840
这是LLM知道的目前已经进行过的对话讨论，因为这里有对之前对话的总结。

144
00:12:55,840 --> 00:12:58,400
还有一点需要注意，

145
00:12:58,400 --> 00:13:06,720
如果你熟悉OpenAI Chat API，通常要设置一个特定的系统消息。

146
00:13:06,720 --> 00:13:14,360
在这个例子中，这并不是一个OpenAI Chat的系统消息，而是提示词中用来描述历史会话摘要的部分。

147
00:13:14,360 --> 00:13:16,600
但是，它还是效果很好。

148
00:13:16,600 --> 00:13:24,760
有了这个提示词，基于客户对AI发展很有兴趣，LLM建议向客户演示我们最新的自然语言处理能力。

149
00:13:24,760 --> 00:13:26,200
好的，这很酷。

150
00:13:26,200 --> 00:13:29,800
嗯，这个就是，

151
00:13:29,800 --> 00:13:34,240
一个让LLM给你的酷炫演示提供建议的例子，这可能会让你想：“如果我在见客户的时候，

152
00:13:34,240 --> 00:13:42,760
我会说，哇，如果有开源框架可以帮我用LLM构建酷炫的NLP应用就好了！”

153
00:13:42,760 --> 00:13:44,920
嗯，这样优秀的项目正在推出。

154
00:13:44,920 --> 00:13:54,920
有趣的是，现在看看记忆存储中发生了什么。

155
00:13:54,920 --> 00:14:04,240
注意到这里，它已经包含了最近的AI系统输出，

156
00:14:04,240 --> 00:14:10,920
而我问它的问题：“演示什么比较好？”，已经被归纳进了系统消息。

157
00:14:10,920 --> 00:14:14,240
嗯，到目前为止，整个对话的总结。

158
00:14:14,240 --> 00:14:27,240
对于ConversationSummaryBufferMemory，它试图将消息的显性记忆保持在我们指定的令牌数上限。

159
00:14:27,240 --> 00:14:34,120
所以，这部分显性记忆，我们试图将其限制在100个令牌，因为这是我们前面指定的。

160
00:14:34,120 --> 00:14:40,680
然后，它会使用LLM生成摘要，就像这里看到的那样。

161
00:14:40,680 --> 00:14:46,640
尽管我用聊天作为例子来说明这些不同的记忆存储方案，

162
00:14:46,640 --> 00:14:49,800
这些记忆存储方案对其他应用程序也很有用，

163
00:14:49,800 --> 00:14:54,760
比如你可能会不断收到新的文本片段或新的信息，

164
00:14:54,760 --> 00:14:59,200
例如，如果你的系统需要经常上网检索内容，

165
00:14:59,200 --> 00:15:07,560
检索结果会存储在一个列表中，但你希望列表的存储总数保持在一个限定的范围内，而不是无限地增长。

166
00:15:07,560 --> 00:15:10,360
我建议你暂停视频并运行代码。

167
00:15:10,360 --> 00:15:15,160
在这个视频中，你看到了几种类型的记忆存储方案，

168
00:15:15,160 --> 00:15:26,520
包括基于对话次数或令牌数量限制的记忆存储方案，或者可以对超过特定令牌数的会话内容进行总结的方案。

169
00:15:26,520 --> 00:15:29,880
LangChain实际上还支持其他类型的记忆存储。

170
00:15:29,880 --> 00:15:33,160
其中最强大的是向量数据存储。

171
00:15:33,160 --> 00:15:39,040
如果你熟悉词嵌入（Embeddings）和文本嵌入，向量数据库实际上就是存储这些嵌入的。

172
00:15:39,040 --> 00:15:41,480
如果你不知道这是什么意思，不用担心。

173
00:15:41,480 --> 00:15:43,320
Harrison稍后会解释。

174
00:15:43,320 --> 00:15:51,120
它可以使用这种向量数据库来检索最相关的文本块。

175
00:15:51,120 --> 00:15:54,480
LangChain还支持实体记忆存储，

176
00:15:54,480 --> 00:16:00,640
当你想记住关于特定人或其他实体的详细信息时，这是适用的，

177
00:16:00,640 --> 00:16:12,280
比如，如果你谈论一个特定的朋友，你可以让LangChain记住关于那个朋友的事实，这将以明确的方式成为一个实体。

178
00:16:12,280 --> 00:16:14,600
当你使用LangChain实现应用程序时，

179
00:16:14,600 --> 00:16:17,240
你还可以使用多种类型的记忆存储，

180
00:16:17,240 --> 00:16:26,480
比如使用本视频中看到的某种对话记忆存储类型，再加上实体记忆存储来回忆个人。

181
00:16:26,480 --> 00:16:35,160
这样，你可以记住对话的大致内容，以及明确记录对话中重要人物的重要事实。

182
00:16:35,160 --> 00:16:38,000
当然，除了使用这些记忆存储类型，

183
00:16:38,000 --> 00:16:45,920
开发者也经常将整个对话存储在传统数据库中，如键值存储（key-value store）或SQL数据库。

184
00:16:45,920 --> 00:16:51,560
这样你可以回顾整个对话，进行审计或进一步改进系统。

185
00:16:51,560 --> 00:16:53,680
这就是记忆存储类型。

186
00:16:53,680 --> 00:16:57,000
希望这些知识能有效地帮助你更好的构建自己的应用程序。

187
00:16:57,000 --> 00:17:05,200
现在，让我们继续下一个视频，了解LangChain的关键构建模块，也就是链。
