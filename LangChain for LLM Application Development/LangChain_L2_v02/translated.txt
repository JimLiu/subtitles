当你与这些模型互动时， 正常情况下它们无法记住\N你之前说过的话或之前的对话， 这对构建像聊天机器人\N这样的应用来说是个问题，\N因为你希望与模型对话时它们能有记忆。 所以在这一堂课，我们将讨论记忆存储， 也就是如何记住前面的对话\N内容，并能将其输入到语言模型中， 这样聊天机器人在与你\N互动时就能让对话更流畅。 LangChain针对复杂的\N记忆存储管理提供了多种选项。 让我们深入了解一下。 首先，让我导入我的OpenAI API密钥， 然后让我导入一些我需要的工具。 让我们以记忆存储为例子， 用LangChain管理聊天或聊天机器人对话。 为此，我将把LLM设置为\NOpenAI的聊天对话模式，temperature为0。 嗯，我将memory设置为\NConversationBufferMemory的实例引用。 稍后你会明白这样做有什么意义。 嗯，我要创建一个\N对话链（ConversationChain）。 在这个短课程的后面， Harrison会更深入地讲解\NLangChain中的链到底是什么。 现在不用太担心语法细节。 但这会构建一个基于LLM的聊天对话。 如果我开始发送消息， 通过"conversation.predict"函数，\N输入”嗨，我叫Andrew。“ 看看它会说什么。 “你好，Andrew。很高兴认识你。” 对吧？然后继续。 然后，假设我问它， “一加一等于几？” 嗯，“一加一等于二。” 然后，再问一遍， “你知道我的名字吗？” “你的名字是Andrew，你之前提到过。” 嗯，有点嘲讽的意味\N在里面。但我没有证据：） 如果你想了解LangChain运行时的\N更多细节，可以把这个"verbose"变量\N改成"True"，看看它实际在做什么。 当你输入“嗨，我叫Andrew。”\N后运行"conversation.predict" 这是LangChain生成的提示词（Prompt）。 它说，"以下是人类和\NAI之间的友好对话，健谈的，"等等。 所以这是LangChain生成的提示词，\N让系统进行愉快友好的对话，\N并且必须要有回应，这是生成的内容。 当你在向模型发送第二句话和第三句话时，  它会在提示词中保留这些信息： 注意到当我说：“我的名字是什么？”的时候， 这是第三轮对话，也是我的第三次输入。 它已经按下面的格式存储\N了当前对话的历史消息： “嗨，我叫Andrew。” “一加一等于几？” 随着时间的推移，这段对话\N的记忆或历史变得越来越长。 实际上，在顶部，我用变量\N"memory"来保存这个记忆。 所以如果我打印"memory.buffer"，\N它已经存储了到目前为止对话中的所有消息。 嗯，你也可以用\N"memory.load_memory_variables({})"\N将"memory"中的内容打印出来。 这对花括号实际上是一个空字典。 可以通过往这个花括号里面\N传入一些值来修改选项做一些高级的定制，\N但在本次课程中我们不打算进一步讨论。 所以不用担心为什么这里有一个空的花括号。 这是LangChain到目前为止\N在对话记忆中所记住的内容。 这就是目前人类和AI之间的所有对话内容。 我建议你暂停视频，运行代码。 LangChain提供了一个\NConversationBufferMemory\N方法来临时存储对话记忆。 要使用ConversationBufferMemory\N存储消息，可以往其中添加输入和输出。 如果你想要往存储里面添加\N新内容，按照这样的方法做就好了。 通过"memory.save_context"加入：\N“嗨“，”最近怎么样？” 我知道这对话内容平淡无奇，\N但我只是想举个简短的例子。 嗯，有了这个，这就是记忆存储的状态。 让我再来打印一下记忆存储中的内容。 现在，如果你想向记忆存储中添加更多数据， 你可以继续保存更多的上下文。 聊天继续：“没什么，\N就这样”，“挺好的”。 如果你把记忆中存储的信息\N打印出来，现在里面有更多的内容。 当你使用大语言模型进行聊天对话中时， 大语言模型自身实际上是无状态的。 语言模型自身不会记住\N和你对话之间的历史消息。 每个请求交互，每次\N调用API都是独立的。 聊天机器人之所以看起来好像\N有记忆，是因为借助代码的帮助，提供\N历史消息作为和LLM对话的上下文。 所以记忆存储可以明确地存储到目前为止的\N对话消息，比如“嗨，我叫Andrew。” “你好，很高兴认识你”等等。 这个记忆存储被用作\NLLM的输入或额外上下文， 这样它在生成输出时，就可以基于之前\N所说过的会话内容，再生成新的会话，\N让你感觉它好像“记得”你说过的话。 随着对话变得越来越长， 所需的记忆存储量也变得非常\N非常大，而向LLM发送大量\N令牌（Token）的成本也会增加， 因为它通常根据需要处理的令牌数量收费。 所以LangChain提供了几种便捷的记忆\N存储方案来存储对话消息和累积对话内容。 到目前为止，我们一直\N在研究对话的记忆存储方案。 现在让我们看看另一种类型的记忆存储方案： ConversationBufferWindowMemory，\N保留窗口记忆，也就是\N仅保留最后若干轮对话消息。 如果我将传入\NConversationBufferWindowMemory\N的k参数设置为1， 变量k等于1表示我只想记住最后一轮对话， 也就是：我最后发出的一句话\N和聊天机器人的最后一句话。 现在如果我让它保存上下文，\N“嗨，最近怎么样？”，\N“没什么，就这样。” 如果我查看"memory.load_variables"，\N它只记得最近的话语。 注意它已经丢掉了\N“嗨，最近怎么样？” 它只是说，人类说：\N“没什么，就这样。”，\NAI说：“酷” 这是因为k等于1。 这是一个很好的功能，\N因为它让你跟踪最近的几个对话。 你在实际使用这个功能时，\N可能不会用k等于1，\N而是将k设置为一个较大的数字。 但是一样可以防止记忆存储量\N随着对话的进行而无限增长。 所以如果我再来一次刚才的对话，\N我们会说，“嗨，我叫Andrew”，\N“1加1等于几？” 现在我问它：“我的名字是什么？” 因为k等于1，它只记得上一次的\N会话，关于1加1等于几？  答案是1加1等于2，但现在\N已经忘记了之前交流的内容， 现在说：“抱歉，无法获取那些信息。” 我建议你暂停视频，在左侧代码中\N将"verbose"参数设置为"True"，\N然后重新运行这个对话。 然后你会看到实际运行时用到的提示词。 希望你能看到，\N当你问LLM：“我的名字是什么？”时， 在它的提示词中，已经丢失了\N前面有关名字的交流，所以\N现在它说不知道我的名字是什么。 使用ConversationalTokenBufferMemory，\N将限制保存在记忆存储的令牌数量。 由于很多LLM定价是基于令牌的，\N令牌的数量直接反映了LLM调用的成本。 如果我设置"max_token_limit"\N为50，实际上让我插入一些消息。 比如说对话是，“AI是什么？”，“太棒了！” “反向传播是什么？”, “美丽！” “聊天机器人是什么？”,“迷人！” 我用ABC作为所有\N这些对话单词的第一个字母。 这样我们可以记录\N什么时候说了什么。 如果我把令牌限制的值调的比较高，\N运行这段代码，它几乎可以包含整个对话。 如果我把令牌限制的值提高到100，\N现在它有整个从“AI是什么？”开始的对话。 如果我减少值，那么它会删掉这个\N对话的最早的那部分消息，只保留\N最近对话的消息，并且保证总的消息\N内容长度不超过设置的令牌限制的值。  如果你想知道为什么我们需要\N指定一个LLM参数，那是因为不同的\NLLM使用不同的令牌计算方法。 所以这告诉它使用ChatOpenAI \NLLM使用的计算令牌的方法。 我建议你暂停视频，\N运行代码，尝试修改提示词，\N看看能否得到不同的输出。 我想在这里说明的\N最后一种记忆存储类型是 那就是\NConversationSummaryBufferMemory。 这个想法是，与其将记忆的存储量限制在\N最近若干对话数量上，或限制在令牌数量上， 不如让LLM为所有历史消息生成摘要，\N在记忆中存储历史消息的摘要。 来举一个例子，我将创建一个\N关于某人日程安排的长字符串。 比如说，早上8点与产品\N团队有一个会议，需要\NPowerPoint演示文稿，等等。 这是一个长字符串，说的是你的日程安排， 可能以中午在意大利餐厅与\N客户共进午餐结束，带上你的笔记本\N电脑，展示最新的LLM演示。 那么，让我使用\NConversationSummaryBufferMemory， 在这种情况下，最大令牌限制\N为400，相当高的令牌限制。 我要加入一些对话内容，比如：\N"你好"，"最近怎么样"，\N"没什么，就这样"，"嗯，酷。" 然后是：\N"今天的日程安排是什么？" 回答是，前面那个很长的日程安排字符串。 所以现在这个记忆存储里有很多文本。 事实上，让我们看一下记忆存储中的\N内容，它包含了所有的文本，因为\N400个令牌足以存储所有这些文本。 但是，如果我现在把最大令牌数\N限制减少到100个令牌，\N记住这里存储了整个对话历史。 如果我把令牌数限制减少到100，\N那么ConversationSummaryBufferMemory\N实际会调用LLM， 在这种情况下是OpenAI API，\N因为这就是我们设置的LLM，\N来生成当前所有会话内容的摘要。 所以摘要是：“人类和AI进行了闲聊，\N然后讨论了当天的日程安排，AI告诉\N人类早上有一个会议”，等等， 嗯，还有一个与客户一起的午餐\N会议，客户对最新AI发展感兴趣。 如果我们用这个LLM进行对话，然后创建\N一个ConversationChain，就像之前一样。 那么，假设我们要问：\N“（给客户）演示什么比较好？” 我设置了“verbose”等于“True”。 这是提示词。 这是LLM知道的目前已经进行过的\N对话讨论，因为这里有对之前对话的总结。 还有一点需要注意， 如果你熟悉OpenAI Chat API，\N通常要设置一个特定的系统消息。 在这个例子中，这并不是一个\NOpenAI Chat的系统消息，而是\N提示词中用来描述历史会话摘要的部分。 但是，它还是效果很好。 有了这个提示词，基于客户\N对AI发展很有兴趣，LLM建议向客户\N演示我们最新的自然语言处理能力。 好的，这很酷。 嗯，这个就是， 一个让LLM给你的酷炫演示\N提供建议的例子，这可能\N会让你想：“如果我在见客户的时候， 我会说，哇，如果有\N开源框架可以帮我用\NLLM构建酷炫的NLP应用就好了！” 嗯，这样优秀的项目正在推出。 有趣的是，现在看看\N记忆存储中发生了什么。 注意到这里，它已经\N包含了最近的AI系统输出， 而我问它的问题：“演示什么比较好？”\N，已经被归纳进了系统消息。 嗯，到目前为止，整个对话的总结。 对于ConversationSummaryBufferMemory，\N它试图将消息的显性记忆保持在\N我们指定的令牌数上限。 所以，这部分显性记忆，\N我们试图将其限制在100个\N令牌，因为这是我们前面指定的。 然后，它会使用LLM生成\N摘要，就像这里看到的那样。 尽管我用聊天作为例子来\N说明这些不同的记忆存储方案， 这些记忆存储方案对\N其他应用程序也很有用， 比如你可能会不断收到\N新的文本片段或新的信息， 例如，如果你的系统\N需要经常上网检索内容， 检索结果会存储在一个列表中，\N但你希望列表的存储总数保持在一个\N限定的范围内，而不是无限地增长。 我建议你暂停视频并运行代码。 在这个视频中，你看到了\N几种类型的记忆存储方案， 包括基于对话次数或令牌数量限制的记忆\N存储方案，或者可以对超过特定令牌数\N的会话内容进行总结的方案。 LangChain实际上还\N支持其他类型的记忆存储。 其中最强大的是向量数据存储。 如果你熟悉词嵌入\N（Embeddings）和文本嵌入，向量\N数据库实际上就是存储这些嵌入的。 如果你不知道这是什么意思，不用担心。 Harrison稍后会解释。 它可以使用这种向量数据库\N来检索最相关的文本块。 LangChain还支持实体记忆存储， 当你想记住关于特定人或其他\N实体的详细信息时，这是适用的， 比如，如果你谈论一个特定的朋友，\N你可以让LangChain记住关于那个朋友\N的事实，这将以明确的方式成为一个实体。 当你使用LangChain实现应用程序时， 你还可以使用多种类型的记忆存储， 比如使用本视频中看到的\N某种对话记忆存储类型，再加上\N实体记忆存储来回忆个人。 这样，你可以记住对话的\N大致内容，以及明确记录对话\N中重要人物的重要事实。 当然，除了使用这些记忆存储类型， 开发者也经常将整个对话存储\N在传统数据库中，如键值存储\N（key-value store）或SQL数据库。 这样你可以回顾整个对话，\N进行审计或进一步改进系统。 这就是记忆存储类型。 希望这些知识能有效地\N帮助你更好的构建自己的应用程序。 现在，让我们继续下一个视频，了解\NLangChain的关键构建模块，也就是链。