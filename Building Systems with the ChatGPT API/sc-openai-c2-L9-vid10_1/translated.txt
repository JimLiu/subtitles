在上个视频中，你看到了如何评估LLM输出，在一个例子中，它给出了正确答案， 我们可以编写一个函数，明确地告诉我们LLM是否输出了正确的类别和产品列表。 但是，如果LLM用来生成文本，而且并不只有一种正确的文本呢？ 让我们看一下如何评估这种类型的LLM输出的方法。 这是我常用的一些辅助函数，给出了一个客户消息， 告诉我关于SmartX Pro手机和Fotoshop相机等的信息。 这里有一些工具可以帮助我得到助手的答案。 这基本上就是Isa在早期的视频中介绍的过程。 这就是助手的答案。 我确信我们有整个智能手机，SmartX Pro手机等等。 那么你如何评估这是不是一个好答案呢？ 看起来有很多可能的好答案。 评估这个的一种方法是编写一个评分标准，也就是一套评估这个答案在不同维度上的指南， 然后用它来决定你是否对这个答案满意。 让我给你演示一下如何做到这一点。 那么让我创建一个小的数据结构来存储客户消息以及产品信息。 这里我要指定一个提示（Prompt），用来评估助手答案，使用的是所谓的评分标准。 我稍后会解释这是什么意思。 但是这个提示的系统消息中说， 你是一个助手，评估客户服务代表回答用户问题的效果， 但是看看客户服务代理是如何使用这个生成的回答的。 那么这个回答就是我们在Notebook上方看到的。 那是助手的回答。 我们将在这个提示中指定数据。 客户的留言是什么？ 上下文是什么？ 也就是说，提供的产品和类别信息是什么？ 然后LLM的输出是什么？ 接下来这是一个评分标准。 我们希望LLM将事实内容和提交的答案与内容进行比较。 忽略语音风格、语法、标点的差异。 然后我们还想检查一些事情，比如，助手的回应是否只基于提供的上下文？ 答案是否包含上下文中没有提供的信息？ 回应和上下文之间有没有任何分歧？ 这就是所谓的评分标准，它规定了答案应该达到的正确程度，以便我们判断它是不是一个好答案。 最后我们想打印出“是”或“否”等等 现在如果我们要运行这个评估，这就是你得到的结果。 它说，助手的回应是基于提供的上下文。 在这种情况下，它似乎没有编造新信息。 没有分歧。 用户问了两个问题，回答了问题一和问题二。 它回答了两个问题。 我们会看这个输出，可能会得出这是一个相当好的回应的结论。 还有一点需要注意，这里我使用的是ChatGPT-3.5-Turbo模型进行评估。 为了更全面的评估，可以考虑使用GPT-4， 因为即使你在生产环境中用ChatGPT-3.5-Turbo并生成了大量文本， 如果你的评估并不是太频繁， 那么为了得到一个更严谨的评估结果，使用稍微昂贵一点GPT-4 API还是值得的。 我希望你们能从这里学到的一个设计模式是：当你可以指定一个评分标准时， 也就是说，列出一系列评估LLM生成结果的标准， 你实际上可以使用另一个API调用来评估从LLM获得的结果。 还有一种设计模式可能对某些应用程序有用，就是如果你能指定一个用来参考的理想标准答案。 这里我要指定一个测试示例，其中客户的消息是： “告诉我关于SmartX Pro 手机的信息，”等等。 这里有一个理想的答案。 如果你有一个高水准的人类客服专家，能对客户问题撰写非常专业的回复。 专家回复的当然是一个很好的答案。SmartX Pro 手机，等等。 它后面还有很多有用的信息。 现在，期望LLM生成这样的答案是不现实的。 在经典的自然语言处理技术中， 有一些传统的度量标准，用于衡量LLM输出是否与人类专家撰写的结果相似。 例如，有一种叫做BLEU分数，B-L-E-U，你可以在网上搜索了解更多。 它们可以衡量一段文字与另一段文字的相似程度。 但事实证明，还有一个更好的方法，就是你可以使用一个提示（Prompt），我将在这里指定， 让LLM去比较，由AI自动生成的客服回复，与前面展示的人类客服专家的理想答案之间的相似度。  这是我们可以使用的提示，我们将使用LLM并让它充当一个助理， 通过比较自动生成的回复和理想的由人类专家编写的回复，评估客服代表回答用户问题的能力。  所以我们将给它提供数据，也就是客户的请求是什么， 专家编写的理想答案是什么，然后我们的LLM实际上输出了什么。 这个评分标准来自OpenAI的开源评估框架， 这是一个非常棒的框架，其中包含了许多由OpenAI开发人员和广大开源社区志愿者贡献的评估方法。 事实上，如果你愿意，你也可以为该框架贡献一个评估方法，以帮助其他人评估他们的大型语言模型输出。 在这个评分标准中，我们要求LLM对输入的内容与专家写的理想答案进行比较， 可以忽略风格、语法、标点的差异。 你可以随时暂停视频，详细阅读这个评分标准。 但关键是我们要求它进行比较并输出一个从A到E的分数， 并根据提交的内容是否包含在专家内容中并完全一致， 或者提交的内容是否超越了专家内容，但完全与之一致。  这可能意味着它产生了幻觉或编造了一些额外的事实。 提交的内容包含了所有专家内容的细节， 无论是否存在分歧，或者内容是否有所不同，但从事实性的角度来看，这些差异并不重要。 而且LLM会选择其中最合适的描述。 这就是我们刚才得到的内容。 我认为这是一个相当不错的内容， 但现在让我们看看它在将得到的内容与测试集进行比较时的想法。 哦，看起来它得了个A。 它认为提交的内容是专家内容的子集，并且与之完全一致。 我觉得这个说法很对。 这个LLM生成的答案比上面长篇大论的专家答案要短得多，但希望它是一致的。 再次说明，我在这个例子中使用的是 GPT-3.5-turbo，但为了更严谨的评估， 在你自己的应用中使用 GPT-4 可能更有意义。 现在让我们尝试一些完全不同的东西。 我将用一段完全不同的测试答案。 “生活就像一盒巧克力”，这句话出自一部名为《阿甘正传》的电影。 如果我们用它来测试，得到的结果是 D 并得出提交的答案“生活就像一盒巧克力”与专家答案之间存在分歧的结论。 所以它正确地判断这是一个非常糟糕的答案。 就是这样。 我希望你从这个视频中学到两个设计模式。 首先，即使没有专家提供的理想答案， 如果你能编写一个评分标准，你就可以用一个LLM来评估另一个LLM的输出。 其次，如果你有专家提供的理想答案， 那么这可以帮助你的LLM更好地比较某一段内容是否与专家提供的理想答案相似。 我希望这能帮助你评估你的LLM系统的输出， 以便在开发过程中和系统运行时，你可以对获得的响应进行持续的监控，并利用这些工具来持续评估和提升你的系统性能。 