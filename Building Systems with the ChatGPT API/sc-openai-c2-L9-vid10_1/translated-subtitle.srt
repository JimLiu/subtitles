1
00:00:04,986 --> 00:00:12,642
在上个视频中，你看到了如何评估LLM输出，在一个例子中，它给出了正确答案，

2
00:00:12,643 --> 00:00:21,280
我们可以编写一个函数，明确地告诉我们LLM是否输出了正确的类别和产品列表。

3
00:00:21,280 --> 00:00:26,400
但是，如果LLM用来生成文本，而且并不只有一种正确的文本呢？

4
00:00:26,400 --> 00:00:31,840
让我们看一下如何评估这种类型的LLM输出的方法。

5
00:00:31,840 --> 00:00:36,286
这是我常用的一些辅助函数，给出了一个客户消息，

6
00:00:36,287 --> 00:00:40,240
告诉我关于SmartX Pro手机和Fotoshop相机等的信息。

7
00:00:40,240 --> 00:00:44,600
这里有一些工具可以帮助我得到助手的答案。

8
00:00:44,600 --> 00:00:48,480
这基本上就是Isa在早期的视频中介绍的过程。

9
00:00:48,480 --> 00:00:50,040
这就是助手的答案。

10
00:00:50,040 --> 00:00:58,880
我确信我们有整个智能手机，SmartX Pro手机等等。

11
00:00:58,880 --> 00:01:04,520
那么你如何评估这是不是一个好答案呢？

12
00:01:04,520 --> 00:01:07,920
看起来有很多可能的好答案。

13
00:01:07,920 --> 00:01:18,799
评估这个的一种方法是编写一个评分标准，也就是一套评估这个答案在不同维度上的指南，

14
00:01:18,800 --> 00:01:22,600
然后用它来决定你是否对这个答案满意。

15
00:01:22,600 --> 00:01:26,080
让我给你演示一下如何做到这一点。

16
00:01:26,080 --> 00:01:31,520
那么让我创建一个小的数据结构来存储客户消息以及产品信息。

17
00:01:31,520 --> 00:01:39,240
这里我要指定一个提示（Prompt），用来评估助手答案，使用的是所谓的评分标准。

18
00:01:39,240 --> 00:01:41,260
我稍后会解释这是什么意思。

19
00:01:41,260 --> 00:01:43,743
但是这个提示的系统消息中说，

20
00:01:43,744 --> 00:01:48,300
你是一个助手，评估客户服务代表回答用户问题的效果，

21
00:01:48,301 --> 00:01:52,200
但是看看客户服务代理是如何使用这个生成的回答的。

22
00:01:52,200 --> 00:01:55,880
那么这个回答就是我们在Notebook上方看到的。

23
00:01:55,880 --> 00:01:58,480
那是助手的回答。

24
00:01:58,480 --> 00:02:01,760
我们将在这个提示中指定数据。

25
00:02:01,760 --> 00:02:03,600
客户的留言是什么？

26
00:02:03,600 --> 00:02:05,120
上下文是什么？

27
00:02:05,120 --> 00:02:08,760
也就是说，提供的产品和类别信息是什么？

28
00:02:08,760 --> 00:02:11,240
然后LLM的输出是什么？

29
00:02:11,240 --> 00:02:12,600
接下来这是一个评分标准。

30
00:02:12,600 --> 00:02:16,360
我们希望LLM将事实内容和提交的答案与内容进行比较。

31
00:02:16,360 --> 00:02:19,200
忽略语音风格、语法、标点的差异。

32
00:02:19,200 --> 00:02:25,200
然后我们还想检查一些事情，比如，助手的回应是否只基于提供的上下文？

33
00:02:25,200 --> 00:02:28,800
答案是否包含上下文中没有提供的信息？

34
00:02:28,800 --> 00:02:31,980
回应和上下文之间有没有任何分歧？

35
00:02:31,980 --> 00:02:42,800
这就是所谓的评分标准，它规定了答案应该达到的正确程度，以便我们判断它是不是一个好答案。

36
00:02:42,800 --> 00:02:47,371
最后我们想打印出“是”或“否”等等

37
00:02:50,280 --> 00:02:58,986
现在如果我们要运行这个评估，这就是你得到的结果。

38
00:02:59,520 --> 00:03:03,200
它说，助手的回应是基于提供的上下文。

39
00:03:03,200 --> 00:03:06,560
在这种情况下，它似乎没有编造新信息。

40
00:03:06,560 --> 00:03:07,560
没有分歧。

41
00:03:07,560 --> 00:03:10,920
用户问了两个问题，回答了问题一和问题二。

42
00:03:10,920 --> 00:03:14,120
它回答了两个问题。

43
00:03:14,120 --> 00:03:20,200
我们会看这个输出，可能会得出这是一个相当好的回应的结论。

44
00:03:20,200 --> 00:03:28,800
还有一点需要注意，这里我使用的是ChatGPT-3.5-Turbo模型进行评估。

45
00:03:28,800 --> 00:03:33,529
为了更全面的评估，可以考虑使用GPT-4，

46
00:03:33,530 --> 00:03:39,486
因为即使你在生产环境中用ChatGPT-3.5-Turbo并生成了大量文本，

47
00:03:39,487 --> 00:03:43,786
如果你的评估并不是太频繁，

48
00:03:43,787 --> 00:03:55,280
那么为了得到一个更严谨的评估结果，使用稍微昂贵一点GPT-4 API还是值得的。

49
00:03:55,280 --> 00:03:59,956
我希望你们能从这里学到的一个设计模式是：当你可以指定一个评分标准时，

50
00:03:59,957 --> 00:04:06,100
也就是说，列出一系列评估LLM生成结果的标准，

51
00:04:06,101 --> 00:04:12,400
你实际上可以使用另一个API调用来评估从LLM获得的结果。

52
00:04:12,400 --> 00:04:22,980
还有一种设计模式可能对某些应用程序有用，就是如果你能指定一个用来参考的理想标准答案。

53
00:04:22,980 --> 00:04:27,429
这里我要指定一个测试示例，其中客户的消息是：

54
00:04:27,430 --> 00:04:31,080
“告诉我关于SmartX Pro 手机的信息，”等等。

55
00:04:31,080 --> 00:04:32,800
这里有一个理想的答案。

56
00:04:32,800 --> 00:04:39,200
如果你有一个高水准的人类客服专家，能对客户问题撰写非常专业的回复。

57
00:04:39,200 --> 00:04:44,160
专家回复的当然是一个很好的答案。SmartX Pro 手机，等等。

58
00:04:44,160 --> 00:04:47,880
它后面还有很多有用的信息。

59
00:04:47,880 --> 00:04:54,600
现在，期望LLM生成这样的答案是不现实的。

60
00:04:54,600 --> 00:04:57,543
在经典的自然语言处理技术中，

61
00:04:57,544 --> 00:05:06,280
有一些传统的度量标准，用于衡量LLM输出是否与人类专家撰写的结果相似。

62
00:05:06,280 --> 00:05:12,320
例如，有一种叫做BLEU分数，B-L-E-U，你可以在网上搜索了解更多。

63
00:05:12,320 --> 00:05:17,680
它们可以衡量一段文字与另一段文字的相似程度。

64
00:05:17,680 --> 00:05:26,885
但事实证明，还有一个更好的方法，就是你可以使用一个提示（Prompt），我将在这里指定，

65
00:05:26,886 --> 00:05:41,280
让LLM去比较，由AI自动生成的客服回复，与前面展示的人类客服专家的理想答案之间的相似度。

67
00:05:41,280 --> 00:05:47,170
这是我们可以使用的提示，我们将使用LLM并让它充当一个助理，

68
00:05:47,171 --> 00:05:58,360
通过比较自动生成的回复和理想的由人类专家编写的回复，评估客服代表回答用户问题的能力。

70
00:05:58,360 --> 00:06:02,900
所以我们将给它提供数据，也就是客户的请求是什么，

71
00:06:02,901 --> 00:06:09,280
专家编写的理想答案是什么，然后我们的LLM实际上输出了什么。

72
00:06:09,280 --> 00:06:13,857
这个评分标准来自OpenAI的开源评估框架，

73
00:06:13,858 --> 00:06:23,320
这是一个非常棒的框架，其中包含了许多由OpenAI开发人员和广大开源社区志愿者贡献的评估方法。

74
00:06:23,320 --> 00:06:32,840
事实上，如果你愿意，你也可以为该框架贡献一个评估方法，以帮助其他人评估他们的大型语言模型输出。

75
00:06:32,840 --> 00:06:39,499
在这个评分标准中，我们要求LLM对输入的内容与专家写的理想答案进行比较，

76
00:06:39,500 --> 00:06:42,629
可以忽略风格、语法、标点的差异。

77
00:06:42,630 --> 00:06:46,840
你可以随时暂停视频，详细阅读这个评分标准。

78
00:06:46,840 --> 00:06:53,557
但关键是我们要求它进行比较并输出一个从A到E的分数，

79
00:06:53,558 --> 00:06:59,499
并根据提交的内容是否包含在专家内容中并完全一致，

80
00:06:59,500 --> 00:07:05,400
或者提交的内容是否超越了专家内容，但完全与之一致。

82
00:07:05,400 --> 00:07:08,560
这可能意味着它产生了幻觉或编造了一些额外的事实。

83
00:07:08,560 --> 00:07:13,729
提交的内容包含了所有专家内容的细节，

84
00:07:13,730 --> 00:07:22,840
无论是否存在分歧，或者内容是否有所不同，但从事实性的角度来看，这些差异并不重要。

85
00:07:22,840 --> 00:07:27,040
而且LLM会选择其中最合适的描述。

86
00:07:27,040 --> 00:07:29,480
这就是我们刚才得到的内容。

87
00:07:29,480 --> 00:07:31,000
我认为这是一个相当不错的内容，

88
00:07:31,001 --> 00:07:35,920
但现在让我们看看它在将得到的内容与测试集进行比较时的想法。

89
00:07:35,920 --> 00:07:37,729
哦，看起来它得了个A。

90
00:07:37,730 --> 00:07:44,400
它认为提交的内容是专家内容的子集，并且与之完全一致。

91
00:07:44,400 --> 00:07:45,400
我觉得这个说法很对。

92
00:07:45,400 --> 00:07:51,280
这个LLM生成的答案比上面长篇大论的专家答案要短得多，但希望它是一致的。

93
00:07:51,280 --> 00:07:59,000
再次说明，我在这个例子中使用的是 GPT-3.5-turbo，但为了更严谨的评估，

94
00:07:59,000 --> 00:08:04,400
在你自己的应用中使用 GPT-4 可能更有意义。

95
00:08:04,400 --> 00:08:06,160
现在让我们尝试一些完全不同的东西。

96
00:08:06,160 --> 00:08:10,400
我将用一段完全不同的测试答案。

97
00:08:10,400 --> 00:08:15,520
“生活就像一盒巧克力”，这句话出自一部名为《阿甘正传》的电影。

98
00:08:15,520 --> 00:08:20,214
如果我们用它来测试，得到的结果是 D

99
00:08:20,215 --> 00:08:28,320
并得出提交的答案“生活就像一盒巧克力”与专家答案之间存在分歧的结论。

100
00:08:28,320 --> 00:08:32,120
所以它正确地判断这是一个非常糟糕的答案。

101
00:08:32,120 --> 00:08:33,320
就是这样。

102
00:08:33,320 --> 00:08:38,080
我希望你从这个视频中学到两个设计模式。

103
00:08:38,081 --> 00:08:42,300
首先，即使没有专家提供的理想答案，

104
00:08:42,301 --> 00:08:48,720
如果你能编写一个评分标准，你就可以用一个LLM来评估另一个LLM的输出。

105
00:08:48,720 --> 00:08:53,029
其次，如果你有专家提供的理想答案，

106
00:08:53,030 --> 00:09:03,280
那么这可以帮助你的LLM更好地比较某一段内容是否与专家提供的理想答案相似。

107
00:09:03,280 --> 00:09:09,343
我希望这能帮助你评估你的LLM系统的输出，

108
00:09:09,344 --> 00:09:26,720
以便在开发过程中和系统运行时，你可以对获得的响应进行持续的监控，并利用这些工具来持续评估和提升你的系统性能。
