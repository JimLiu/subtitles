在这个视频中，我们将重点关注检查系统生成的输出。
在向用户展示之前检查输出结果，对于确保
提供给用户或在自动化流程中使用的内容的质量、相关性和安全性是很重要的。
我们将学习如何使用审查API，但这次是针对输出内容，
以及如何在显示之前使用额外的Prompt，让模型评估输出质量。
让我们深入了解这些示例。
之前我们讨论审查API的背景是检查输入内容。
现在让我们在检查输出的背景下重新审视它。
审查API还可以用于过滤和审查系统生成的内容。
所以这里有一个例子。
这里有一个生成给用户的输出内容。
使用审查API的方法跟之前视频介绍的方法一样。
让我们看看这个输出是否被标记。
正如您所看到的，这个输出没有被标记，并且在所有类别中得分都很低，
这对于给定的回应是有道理的。
通常，检查输出也很重要。
例如，如果您为敏感受众创建聊天机器人，您可以使用较低的阈值来标记输出。
总的来说，如果审查输出内容后，表明内容被标记，
您可以采取适当的行动，比如返回一个备用答案，或重新生成一个新的结果。
请注意，随着模型的改进，返回某种有害内容的概率会越来越低。
另一种检查输出的方法是直接询问模型自己
对生成的是否令人满意，是否符合您定义的某种标准。
实现的方式是：将模型输出的内容配合适当的Prompt提交给模型来评估，
要求模型评估输出的质量。
您可以用各种不同的方式来实现这一点。
让我们看一个例子。
我们的系统消息是：
“您是一个助手，评估客户服务代表的回答是否充分解答了客户的问题
并验证助手引用的所有产品信息中的事实是否正确。
产品信息以及用户和客户服务代表的消息将由三个反引号进行分割。
请用Y或N字符回答，不要加标点符号。
如果输出充分回答了问题，并且回应正确使用了产品信息，则为Y，否则为N。
仅输出一个字母。”
你还可以为此使用一种思维链推理Prompt。
对于模型来说，一步验证这两个可能有点困难。
你可以试着玩玩这个。
你还可以添加其他类型的指南。
你可以问，给一个类似于考试或评分论文的评分标准。
你可以使用这种格式说：
“这是否使用了符合我们品牌指南的友好语气？”
如果这对你来说非常重要，也许可以概述一下你的品牌指南。
接下来让我们添加客户留言。
这是用来生成这条回复的初始留言。
然后让我们粘贴产品信息。
这是我们在之前为了生成这条留言，获取的所有产品的产品信息。
现在我们来定义对比。
"Customer Message"是客户留言，产品信息，然后是代理返回结果，
这是我们从前面的单元格得到的对客户的返回结果。
让我们把这个格式化成一个消息列表，看看从模型中得到什么样的结果。
模型说：“是的”。产品信息是正确的，问题已经得到了充分的回答。
嗯，总的来说，对于这类评估任务，
我认为使用更高级的模型会更好，因为它们在推理方面更强大。
比如说GPT-4。
我们再试一个例子。
这个回答是：“生活就像一盒巧克力。”
让我们把信息加入到输出检查中。
模型已经判断出这个回答没有充分回答问题，或者没有使用检索到的信息。
“这个问题，它是否正确使用了检索到的信息？”
如果你想确保模型没有产生幻觉（Hallucination），这是一个好的Prompt，
幻觉是指编造一些不真实的事情。
现在可以随意暂停视频，尝试一些你自己的客户留言、回复，
并添加产品信息以测试这个功能是如何工作的。
如您所见，模型可以对生成输出的质量提供反馈。
您可以根据这个反馈来决定是将输出展示给用户还是生成新的内容。
您甚至可以尝试针对每个用户查询生成多个模型的结果，
然后让模型选择最佳的一个展示给用户。
有很多不同的事情可以尝试。
总的来说，使用审核 API 检查输出是个好习惯。
但是，虽然让模型评估自己的输出对于即时反馈可能有用，
以确保在极少数情况下返回结果的质量，
但我认为大多数时候这可能是不必要的，特别是如果您使用的是更先进的模型，如 GPT-4。
实际上，我还没有看到多少人在正式产品中这样做。
这也会增加系统的延迟和成本，
因为您必须等待模型的额外调用。
这也要消耗额外的Token。
如果对于您的应用产品来说，错误率低于0.0000001%非常重要，
那么您可以尝试这种方法。
但总的来说，我并不建议您在实践中这样做。
在下一个视频中，
我们将整合在评估输入部分、处理部分和检查输出部分所学到的所有内容，来构建一个端到端的系统。