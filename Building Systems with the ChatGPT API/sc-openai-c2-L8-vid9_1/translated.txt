在前几个视频中，Isa展示了\N如何使用大型语言模型（LLM）来构建应用， 从评估输入到处理输入，\N然后在向用户展示输出之前进行最终的输出检查。 在你构建了这样一个系统之后，\N你怎么知道它的运行情况呢？ 甚至当你部署并让用户使用它时， 你如何跟踪它的表现，发现不足之处，\N并继续提高系统答案的质量呢？ 在这个视频中，我想和大家\N分享一些评估LLM输出的最佳实践。 我特别想和你分享\N构建这些系统的感觉是什么样的。 你在这个视频中听我讲述的内容 和你在更传统的机器学习，监督学习\N应用中可能看到的内容之间的一个关键区别是 因为你可以快速构建这样的应用程序，\N所以评估它的方法通常不会从测试数据集开始。 相反，你通常最终会逐渐建立一套测试例子。 让我向你解释我这么说的原因。 你还记得第二个视频中的这个图表，它展示了\N”如何通过基于提示的开发加快模型开发“\N的核心部分， 从可能需要几个月到\N仅仅几分钟、几小时或者最多几天。 在传统的监督学习方法中，如果你需要收集，\N比如说，10,000个标记了的数据示例， 那么收集另外1,000个\N测试示例的增量成本并不算高。 所以在传统的监督学习环境中， 收集训练数据集、收集开发数据集、\N保留交叉验证数据集和测试数据集是很常见的， 然后在整个开发过程中随时要使用这些数据。 但是，如果你使用Prompt-based AI，\N你只要几分钟内就可以写好一个提示\（Prompt），N并在几个小时内使其工作， 那么如果你必须暂停很长时间\N去收集1000个测试示例，就会觉得非常麻烦， 因为你现在完全不需要训练数据\N就可以让它正常工作。 所以在使用LLM构建应用程序时，\N通常会有这种感觉： 首先，你会在少量的例子\N上调整提示（Prompt）， 可能是一到三到五个例子，\N尝试找到一个适用于它们的提示。 然后，在系统调试过程中，\N你偶尔会遇到一些棘手的案例。 提示或者算法在这些案例上不起作用。 在这种情况下，你可以把这些额外的一两个\N或三五个例子加入到你正在测试的数据集中， 慢慢的收集更多棘手的例子\N形成一个开发数据集。 最后，你添加到开发数据集的例子足够多了， 以至于每次对提示修改后，要手动\N把数据集的例子挨个运行一遍都有点麻烦了。 然后你可以开始制定指标来衡量\N这些例子的运行情况，比如说平均准确率如何。 而这个过程中，如果在任何时候\N你觉得系统运行得足够好了， 你可以就此停止，不需要再进行下一个步骤。 事实上，有很多应用可能只进行\N到第一或第二步，运行得也很好。 现在，如果你手工收集的\N用来评估模型的数据集， 还不能让你对系统的表现有足够的信心， 那么你可能需要进行下一步，\N收集随机抽样的数据集来调整模型。 这将继续作为一个开发数据集\N或保留交叉验证数据集， 因为继续调整你的提示\N以适应数据集合是很常见的。 只有当你需要对系统的表现\N做很高精准度的评估时， 你才需要收集和使用一个保留测试数据集，\N通常在调整模型时，你是不应该使用\N这个保留测试集的（甚至看都不要看一眼）。 所以第四步在某种程度上更重要，\N比如说： 如果你的系统正确率为91%，\N你想调整它使正确率达到92%或93%。 那么你确实需要更多的例子\N来衡量91%和93%之间的准确率差异。 只有在你真的需要一个\N公正、无偏的估计来评估系统的表现时， 你才需要在开发数据集之外\N再收集一个保留测试数据集。 有一个重要的注意事项： 我见过很多大型语言模型的应用，\N其中如果给出的答案不太准确，\N并没有实质性的危害风险。  但显然对于任何高风险的应用，如果存在\N偏见风险或不恰当的输出对某人造成伤害， 那么收集一个测试集来严格评估你的系统的表现， 确保在使用之前它能做正确的事情，\N这就变得更加重要了。 但是，举个例子，如果你只是\N用它来为自己阅读的文章做总结，\N而不是给别人看， 那么可能造成的危害风险就相对较小 你可以在这个过程的早期就停止，\N而不用花费第四和第五点的成本，\N收集更大的数据集来评估你的算法。 所以在这个例子中，让我从常用的辅助函数开始。 使用一个utils函数\N来获取产品和类别的列表。 所以在计算机和笔记本电脑类别中，\N有一个计算机和笔记本电脑的列表，\N还有智能手机和配件类别。 这里有一个智能手机和配件的\N列表，以及其他类别的列表。 现在假设一个任务是：\N“根据用户输入，比如我有预算限制要买什么电视 来检索相关的类别和产品，\N以便我们有正确的信息来回答用户的查询。” 那么这里有一个提示。 如果需要的话请随时暂停视频，\N以便你可以详细阅读这个Prompt的详细信息。 但是提示中指定了一组说明，\N并且实际上给了语言模型一个好的输出示例。 这有时被称为少量示例（Few-shot）\N或者从技术上说是单示例提示（One-shot prompting） 因为我们实际上使用用户消息\N和系统消息给它一个好的输出示例。 如果有人说我想要最贵的电脑，\N那我们就返回所有电脑，因为我们没有价格信息。 现在让我们在客户留言上使用这个提示：\N“如果我预算有限，我可以买哪台电视？” 所以我们把提示、customer_msg_0\N和产品类别都传递给它。 这是我们在上面使用utils函数\N检索到的信息。 这里列出了与此查询相关的信息，\N即电视和整个影院系统类别下的信息。 这是一份看起来相关的电视\N和整个影院系统列表。 要查看提示的效果如何，\N可以在第二个提示上进行评估。 有人说我需要一个智能手机充电器。 看起来它正确地获取了这些数据，\N如何成为智能手机配件并列出相关产品。 还有另一个： “那么你们有哪些电脑？” 希望你能找到一份电脑列表。 所以这里我有三个提示。 如果你是第一次开发这个提示， 这样的一两个或三个例子是相当合理的 并不断调整提示，直到它给出适当的输出 直到提示能够根据客户的请求，\N检索出所有提示所需的相关产品和类别， 就像这个例子中的所有三个提示一样。 如果提示缺少一些产品之类的东西， 那么我们可能会回去修改几次提示，\N直到它在这三个提示上都正确。 在将系统调整到这个程度之后，\N你可能会开始测试你的系统， 也许将其发送给内部测试用户\N或尝试自己使用它，然后运行一段时间\N看看会发生什么。 有时候你会遇到一个它无法解决的提示。 这里有一个提示的例子。 告诉我关于SmartX Pro手机\N和Fotoshop相机，还有你们有哪些电视。 当我在这个提示上运行它时，\N看起来它输出了正确的数据， 但它也输出了一堆额外的垃圾文字。 这使得要这段字符串很难\N被解析成Python对象列表。 我们不喜欢它输出这些额外的废话。 当你遇到一个系统处理失败的例子时， 通常的做法就是记下\N这是一个有点棘手的例子。 那么让我们把这个加入\N到我们要测试系统的例子集合里。 如果你继续运行系统一段时间，\N也许它能处理那些例子。 我们确实调整了三个示例的提示，\N它可能会在很多例子上都能工作。 但是碰巧，你可能会遇到\N另一个它产生错误的例子。 这个customer_msg_4也导致系统\N在最后输出了一堆我们不想要的垃圾文本。 试图帮助我们得到所有这些\N额外的文本，但实际上我们并不需要这些。 在这一点上，你可能已经\N在数百个例子上运行了这个提示， 也许你有测试用户，但你只需要\N拿那些棘手的例子，它做得不好的那些。 现在我有这一组5个例子，从0到4编号， 有这一组5个例子，\N你可以用来进一步微调提示。 在这两个例子中，LLM \N都输出了一堆我们不想要的额外垃圾文本。 经过一点尝试和错误，\N你可能会决定修改提示如下： 这里有一个新版本的提示，叫做Prompt v2。 但我们在这里做的是，我们在提示中添加了：\N“不要输出任何不是 JSON 格式的额外文本”。 就是强调一下：请不要输出JSON之外的内容。 并添加了第2个例子，\N使用用户和助手消息进行\N少量示例提示（Few-shot prompting）， 用户询问最便宜的电脑。 在这两个少量示例提示的例子中， 我们向系统展示了一个示例，\N这个示例只返回 JSON 格式的结果。 这是我们刚刚添加到提示中的额外内容： 不要输出任何不是JSON格式的额外文本。 我们使用few_shot_user1, \Nfew_shot_assistant1, few_shot_user2 \N和 few_shot_assistant2， 给它两个这样的少量示例提示。 那么让我按shift enter找到那个提示。 如果你回头手动重新运行这个提示，\N对所有五个用户输入示例， 包括之前给出了错误输出的这个， 你会发现它现在给出了正确的输出。 如果你回头重新运行这个新提示，\N这是提示版本v2， 在那个客户留言示例上，\N之前的版本在JSON输出之后还有额外垃圾文本，\N导致输出错误。 新版本生成了更好的输出结果。 我不会在这里做，但我鼓励你暂停视频，\N然后自己重新运行customer_msg_4，\N还有这个提示v2， 看看它是否也能生成正确的输出。 希望它能，我觉得应该可以。 当然，当你修改提示时， 也有必要做一些回归测试，\N以确保在修复提示3和4的错误输出时， 没有破坏提示0的输出。 现在你可以看出，如果我必须复制粘贴\N五个提示，customer_msg_0、1、2、3和4\N到我的Jupyter Notebook， 运行它们，然后手动查看它们，\N看看它们是否都放在了正确的类别和产品中， 你可以这么做。 我可以看着这个说：\N是的，类别是电视和家庭影院系统产品。 嗯，看起来你把它们都找到了。 但实际上，手动做这个有点痛苦， 需要用眼睛仔细检查输出结果，\N确保这确实是正确的输出。 当你调整的开发集不再只有少数几个例子时， 自动化测试流程就变得很有用了。 这里有10个例子，\N我指定了10条客户留言。 这是客户的留言，\N我有预算可以买什么电视？ 还有什么是理想的答案？ 把这个当作测试数据集里的正确答案， 或者我应该说开发数据集，\N因为我们实际上是在调整这个。 我们在这里收集了10个例子，\N索引编号从0到9， 最后一个是：\N“如果用户说我想要热水浴缸时间机器，\N我们真的没有相关产品，非常抱歉。” 理想的答案是空集。 现在，如果你想自动评估 某个提示在这10个例子中的某个例子上\N的运行结果，这里有一个函数可以实现。 这是一个有点长的函数，如果你\N愿意的话，可以暂停视频并仔细阅读。 让我演示一下它实际上在做什么。 让我打印出索引是0的客户留言。 客户留言：“如果我预算有限，\N我可以买哪台电视？” 我们也来打印一下\N这个留言理想的答案是什么。 所以理想的答案是根据提示\N列出所有我们希望获取的电视。 现在让我调用这个提示，这是在\N这个客户消息上使用该用户产品\N和类别信息的提示符v2。 我们打印出来。 然后我们将调用eval。 我们将调用\Neval_responsive_with_ideal函数，\N看看返回结果与理想答案的匹配程度如何。 在这种情况下，\N它确实输出了我们想要的类别，\N也输出了整个产品列表。 所以这给你一个1.0的分数。 再给你们举个例子，\N事实证明我知道它在第7个例子上出错了。 所以如果我把这个从0改成7\N然后运行，这就是它得到的。 哦，让我把这个也更新成7。 在这个客户留言下面，\N这是理想的答案，它应该输出\N在”游戏机和配件“分类下面。 这是游戏机和配件。 但是这里的回应有三个输出，\N实际上应该有一、二、三、四、五个输出。 它缺少了一些产品。 如果我现在要调整提示的话， 我会用一个for循环来遍历\N所有10个来自开发数据集的示例，\N我们逐个获取客户留言， 得到理想答案，正确答案，调用函数\N得到返回结果，并对结果进行评估， 然后你知道，累积平均。 让我运行一下这个。 这个运行起来需要一段时间，\N但当它运行完毕时，这就是结果。 我们正在浏览10个示例。 看起来第7个示例是错误的。 所以在10个示例的测试中，\N正确的比例是90%。 因此，如果你调整提示，\N可以重新运行以查看正确百分比是上升还是下降。 你刚才在这个笔记本中看到的是\N完成这个项目符号列表中的第一、二、三步。 这已经提供了一个相当好的\N开发数据集，包括10个示例，\N用于调整和验证提示是否有效。 如果你需要更高的严谨性， 那么你现在已经有了需要的软件，\N可以收集一个随机抽样的示例数据集，\N比如100个示例及其理想输出。 甚至可以做的更好，用一个\N你在调整提示时完全没有测试过的保留测试集，\N以保证严谨性。 但对于很多应用来说，做到第三点就足够了， 像我刚才在Jupyter Notebook\N做的这些，你也可以应用， 帮你快速将系统准确率优化到很好。 再次强调，如果你正在开发\N对安全性要求很高的应用或者 可能存在实质性伤害风险的应用， 那么负责任的做法当然是\N在任何地方使用它之前，进行\N大规模的测试集验证以严格验证其准确性。  就是这样。 我发现，使用提示构建应用程序\N的工作流程与使用监督学习构建应用程序\N的工作流程非常不同，迭代的步伐感觉快了很多。  如果你还没有尝试过这种方法， 你可能会对只用几个精心策划的棘手例子\N构建的评估方法的效果感到惊讶。 你可能会认为，10个例子对于几乎所有事情\N来说，这在统计上都是不成立的， 但你可能会在实际使用这个程序时惊讶地发现，\N将一小部分棘手的例子加入到你的开发集中，  可能会在帮助你和你的团队得到一套\N有效的提示和有效的系统方面，效果出奇的好。 在这个视频中， 输出可以定量地进行评估，因为有一个期望的输出，\N你可以判断它是否生成了期望的输出。 那么下一个视频中， 让我们一起看看在那种\N没有标准答案的情况下，如何评估输出。