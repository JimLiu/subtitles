1
00:00:05,000 --> 00:00:10,074
在前几个视频中，Isa展示了如何使用大型语言模型（LLM）来构建应用，

2
00:00:10,075 --> 00:00:19,000
从评估输入到处理输入，然后在向用户展示输出之前进行最终的输出检查。

3
00:00:19,000 --> 00:00:22,880
在你构建了这样一个系统之后，你怎么知道它的运行情况呢？

4
00:00:22,880 --> 00:00:26,516
甚至当你部署并让用户使用它时，

5
00:00:26,517 --> 00:00:35,000
你如何跟踪它的表现，发现不足之处，并继续提高系统答案的质量呢？

6
00:00:35,000 --> 00:00:41,380
在这个视频中，我想和大家分享一些评估LLM输出的最佳实践。

7
00:00:41,380 --> 00:00:46,440
我特别想和你分享构建这些系统的感觉是什么样的。

8
00:00:46,440 --> 00:00:49,848
你在这个视频中听我讲述的内容

9
00:00:49,849 --> 00:00:55,550
和你在更传统的机器学习，监督学习应用中可能看到的内容之间的一个关键区别是

10
00:00:55,551 --> 00:01:03,520
因为你可以快速构建这样的应用程序，所以评估它的方法通常不会从测试数据集开始。

11
00:01:03,520 --> 00:01:08,840
相反，你通常最终会逐渐建立一套测试例子。

12
00:01:08,840 --> 00:01:10,880
让我向你解释我这么说的原因。

13
00:01:10,880 --> 00:01:18,386
你还记得第二个视频中的这个图表，它展示了”如何通过基于提示的开发加快模型开发“的核心部分，

14
00:01:18,387 --> 00:01:25,640
从可能需要几个月到仅仅几分钟、几小时或者最多几天。

15
00:01:25,641 --> 00:01:32,515
在传统的监督学习方法中，如果你需要收集，比如说，10,000个标记了的数据示例，

16
00:01:32,516 --> 00:01:38,279
那么收集另外1,000个测试示例的增量成本并不算高。

17
00:01:38,280 --> 00:01:41,224
所以在传统的监督学习环境中，

18
00:01:41,225 --> 00:01:48,674
收集训练数据集、收集开发数据集、保留交叉验证数据集和测试数据集是很常见的，

19
00:01:48,675 --> 00:01:52,760
然后在整个开发过程中随时要使用这些数据。

20
00:01:52,760 --> 00:01:57,857
但是，如果你使用Prompt-based AI，你只要几分钟内就可以写好一个提示（Prompt），并在几个小时内使其工作，

21
00:01:57,858 --> 00:02:05,075
那么如果你必须暂停很长时间去收集1000个测试示例，就会觉得非常麻烦，

22
00:02:05,076 --> 00:02:09,560
因为你现在完全不需要训练数据就可以让它正常工作。

23
00:02:09,560 --> 00:02:14,880
所以在使用LLM构建应用程序时，通常会有这种感觉：

24
00:02:14,880 --> 00:02:18,928
首先，你会在少量的例子上调整提示（Prompt），

25
00:02:18,929 --> 00:02:25,120
可能是一到三到五个例子，尝试找到一个适用于它们的提示。

26
00:02:25,120 --> 00:02:32,560
然后，在系统调试过程中，你偶尔会遇到一些棘手的案例。

27
00:02:32,560 --> 00:02:35,720
提示或者算法在这些案例上不起作用。

28
00:02:35,720 --> 00:02:42,634
在这种情况下，你可以把这些额外的一两个或三五个例子加入到你正在测试的数据集中，

29
00:02:42,635 --> 00:02:46,240
慢慢的收集更多棘手的例子形成一个开发数据集。

30
00:02:46,240 --> 00:02:53,908
最后，你添加到开发数据集的例子足够多了，

31
00:02:53,909 --> 00:03:00,480
以至于每次对提示修改后，要手动把数据集的例子挨个运行一遍都有点麻烦了。

32
00:03:00,480 --> 00:03:08,040
然后你可以开始制定指标来衡量这些例子的运行情况，比如说平均准确率如何。

33
00:03:08,040 --> 00:03:16,843
而这个过程中，如果在任何时候你觉得系统运行得足够好了，

34
00:03:16,876 --> 00:03:19,880
你可以就此停止，不需要再进行下一个步骤。

35
00:03:19,880 --> 00:03:30,440
事实上，有很多应用可能只进行到第一或第二步，运行得也很好。

36
00:03:30,440 --> 00:03:37,029
现在，如果你手工收集的用来评估模型的数据集，

37
00:03:37,030 --> 00:03:40,843
还不能让你对系统的表现有足够的信心，

38
00:03:40,844 --> 00:03:49,760
那么你可能需要进行下一步，收集随机抽样的数据集来调整模型。

39
00:03:49,760 --> 00:03:54,057
这将继续作为一个开发数据集或保留交叉验证数据集，

40
00:03:54,100 --> 00:04:00,120
因为继续调整你的提示以适应数据集合是很常见的。

41
00:04:00,120 --> 00:04:05,680
只有当你需要对系统的表现做很高精准度的评估时，

42
00:04:05,680 --> 00:04:14,520
你才需要收集和使用一个保留测试数据集，通常在调整模型时，你是不应该使用这个保留测试集的（甚至看都不要看一眼）。

43
00:04:14,520 --> 00:04:18,500
所以第四步在某种程度上更重要，比如说：

44
00:04:18,501 --> 00:04:28,920
如果你的系统正确率为91%，你想调整它使正确率达到92%或93%。

45
00:04:28,920 --> 00:04:36,800
那么你确实需要更多的例子来衡量91%和93%之间的准确率差异。

46
00:04:36,800 --> 00:04:42,480
只有在你真的需要一个公正、无偏的估计来评估系统的表现时，

47
00:04:42,480 --> 00:04:47,680
你才需要在开发数据集之外再收集一个保留测试数据集。

48
00:04:47,680 --> 00:04:49,100
有一个重要的注意事项：

49
00:04:49,120 --> 00:04:59,080
我见过很多大型语言模型的应用，其中如果给出的答案不太准确，并没有实质性的危害风险。

51
00:04:59,080 --> 00:05:08,013
但显然对于任何高风险的应用，如果存在偏见风险或不恰当的输出对某人造成伤害，

52
00:05:08,014 --> 00:05:13,486
那么收集一个测试集来严格评估你的系统的表现，

53
00:05:13,487 --> 00:05:19,040
确保在使用之前它能做正确的事情，这就变得更加重要了。

54
00:05:19,040 --> 00:05:26,114
但是，举个例子，如果你只是用它来为自己阅读的文章做总结，而不是给别人看，

55
00:05:26,115 --> 00:05:28,614
那么可能造成的危害风险就相对较小

56
00:05:28,615 --> 00:05:38,480
你可以在这个过程的早期就停止，而不用花费第四和第五点的成本，收集更大的数据集来评估你的算法。

57
00:05:38,480 --> 00:05:48,600
所以在这个例子中，让我从常用的辅助函数开始。

58
00:05:48,600 --> 00:05:53,320
使用一个utils函数来获取产品和类别的列表。

59
00:05:53,320 --> 00:06:00,800
所以在计算机和笔记本电脑类别中，有一个计算机和笔记本电脑的列表，还有智能手机和配件类别。

60
00:06:00,800 --> 00:06:05,757
这里有一个智能手机和配件的列表，以及其他类别的列表。

61
00:06:12,040 --> 00:06:26,028
现在假设一个任务是：“根据用户输入，比如我有预算限制要买什么电视

62
00:06:26,029 --> 00:06:35,080
来检索相关的类别和产品，以便我们有正确的信息来回答用户的查询。”

63
00:06:35,080 --> 00:06:36,080
那么这里有一个提示。

64
00:06:36,080 --> 00:06:39,520
如果需要的话请随时暂停视频，以便你可以详细阅读这个Prompt的详细信息。

65
00:06:39,520 --> 00:06:47,360
但是提示中指定了一组说明，并且实际上给了语言模型一个好的输出示例。

66
00:06:47,360 --> 00:06:50,500
这有时被称为少量示例（Few-shot）或者从技术上说是单示例提示（One-shot prompting）

67
00:06:50,501 --> 00:06:56,640
因为我们实际上使用用户消息和系统消息给它一个好的输出示例。

68
00:06:56,640 --> 00:07:03,800
如果有人说我想要最贵的电脑，那我们就返回所有电脑，因为我们没有价格信息。

69
00:07:03,800 --> 00:07:15,560
现在让我们在客户留言上使用这个提示：“如果我预算有限，我可以买哪台电视？”

70
00:07:15,560 --> 00:07:22,760
所以我们把提示、customer_msg_0和产品类别都传递给它。

71
00:07:22,760 --> 00:07:26,880
这是我们在上面使用utils函数检索到的信息。

72
00:07:26,880 --> 00:07:34,220
这里列出了与此查询相关的信息，即电视和整个影院系统类别下的信息。

73
00:07:34,220 --> 00:07:38,220
这是一份看起来相关的电视和整个影院系统列表。

74
00:07:38,220 --> 00:07:43,920
要查看提示的效果如何，可以在第二个提示上进行评估。

75
00:07:43,920 --> 00:07:47,480
有人说我需要一个智能手机充电器。

76
00:07:47,480 --> 00:07:56,400
看起来它正确地获取了这些数据，如何成为智能手机配件并列出相关产品。

77
00:07:56,400 --> 00:08:00,680
还有另一个：

78
00:08:00,680 --> 00:08:03,080
“那么你们有哪些电脑？”

79
00:08:03,080 --> 00:08:06,620
希望你能找到一份电脑列表。

80
00:08:06,620 --> 00:08:08,960
所以这里我有三个提示。

81
00:08:08,960 --> 00:08:12,700
如果你是第一次开发这个提示，

82
00:08:12,701 --> 00:08:18,286
这样的一两个或三个例子是相当合理的

83
00:08:18,329 --> 00:08:22,686
并不断调整提示，直到它给出适当的输出

84
00:08:22,687 --> 00:08:30,200
直到提示能够根据客户的请求，检索出所有提示所需的相关产品和类别，

85
00:08:30,400 --> 00:08:32,414
就像这个例子中的所有三个提示一样。

86
00:08:34,960 --> 00:08:38,486
如果提示缺少一些产品之类的东西，

87
00:08:38,487 --> 00:08:44,300
那么我们可能会回去修改几次提示，直到它在这三个提示上都正确。

88
00:08:45,400 --> 00:08:53,199
在将系统调整到这个程度之后，你可能会开始测试你的系统，

89
00:08:53,200 --> 00:09:00,840
也许将其发送给内部测试用户或尝试自己使用它，然后运行一段时间看看会发生什么。

90
00:09:00,840 --> 00:09:06,120
有时候你会遇到一个它无法解决的提示。

91
00:09:06,120 --> 00:09:07,840
这里有一个提示的例子。

92
00:09:07,840 --> 00:09:12,120
告诉我关于SmartX Pro手机和Fotoshop相机，还有你们有哪些电视。

93
00:09:12,120 --> 00:09:16,500
当我在这个提示上运行它时，看起来它输出了正确的数据，

94
00:09:16,501 --> 00:09:20,320
但它也输出了一堆额外的垃圾文字。

95
00:09:20,320 --> 00:09:26,820
这使得要这段字符串很难被解析成Python对象列表。

96
00:09:26,820 --> 00:09:30,080
我们不喜欢它输出这些额外的废话。

97
00:09:30,080 --> 00:09:34,786
当你遇到一个系统处理失败的例子时，

98
00:09:34,787 --> 00:09:39,280
通常的做法就是记下这是一个有点棘手的例子。

99
00:09:39,280 --> 00:09:45,040
那么让我们把这个加入到我们要测试系统的例子集合里。

100
00:09:45,040 --> 00:09:49,720
如果你继续运行系统一段时间，也许它能处理那些例子。

101
00:09:49,720 --> 00:09:53,640
我们确实调整了三个示例的提示，它可能会在很多例子上都能工作。

102
00:09:53,640 --> 00:09:58,960
但是碰巧，你可能会遇到另一个它产生错误的例子。

103
00:09:58,960 --> 00:10:07,920
这个customer_msg_4也导致系统在最后输出了一堆我们不想要的垃圾文本。

104
00:10:07,920 --> 00:10:13,680
试图帮助我们得到所有这些额外的文本，但实际上我们并不需要这些。

105
00:10:13,680 --> 00:10:17,914
在这一点上，你可能已经在数百个例子上运行了这个提示，

106
00:10:17,915 --> 00:10:23,920
也许你有测试用户，但你只需要拿那些棘手的例子，它做得不好的那些。

107
00:10:23,920 --> 00:10:28,671
现在我有这一组5个例子，从0到4编号，

108
00:10:28,672 --> 00:10:34,440
有这一组5个例子，你可以用来进一步微调提示。

109
00:10:34,440 --> 00:10:44,560
在这两个例子中，LLM 都输出了一堆我们不想要的额外垃圾文本。

110
00:10:44,560 --> 00:10:51,400
经过一点尝试和错误，你可能会决定修改提示如下：

111
00:10:51,400 --> 00:10:54,640
这里有一个新版本的提示，叫做Prompt v2。

112
00:10:54,640 --> 00:11:00,556
但我们在这里做的是，我们在提示中添加了：“不要输出任何不是 JSON 格式的额外文本”。

113
00:11:00,557 --> 00:11:03,640
就是强调一下：请不要输出JSON之外的内容。

114
00:11:03,640 --> 00:11:09,680
并添加了第2个例子，使用用户和助手消息进行少量示例提示（Few-shot prompting），

115
00:11:09,680 --> 00:11:12,600
用户询问最便宜的电脑。

116
00:11:12,600 --> 00:11:14,857
在这两个少量示例提示的例子中，

117
00:11:14,858 --> 00:11:21,040
我们向系统展示了一个示例，这个示例只返回 JSON 格式的结果。

118
00:11:21,040 --> 00:11:23,414
这是我们刚刚添加到提示中的额外内容：

119
00:11:23,415 --> 00:11:26,000
不要输出任何不是JSON格式的额外文本。

120
00:11:26,000 --> 00:11:31,342
我们使用few_shot_user1, few_shot_assistant1, few_shot_user2 和 few_shot_assistant2，

121
00:11:31,343 --> 00:11:35,640
给它两个这样的少量示例提示。

122
00:11:35,640 --> 00:11:39,160
那么让我按shift enter找到那个提示。

123
00:11:39,160 --> 00:11:44,056
如果你回头手动重新运行这个提示，对所有五个用户输入示例，

124
00:11:44,057 --> 00:11:47,357
包括之前给出了错误输出的这个，

125
00:11:47,358 --> 00:11:51,280
你会发现它现在给出了正确的输出。

126
00:11:51,280 --> 00:11:55,943
如果你回头重新运行这个新提示，这是提示版本v2，

127
00:11:55,944 --> 00:12:03,670
在那个客户留言示例上，之前的版本在JSON输出之后还有额外垃圾文本，导致输出错误。

128
00:12:03,671 --> 00:12:08,880
新版本生成了更好的输出结果。

129
00:12:08,880 --> 00:12:16,442
我不会在这里做，但我鼓励你暂停视频，然后自己重新运行customer_msg_4，还有这个提示v2，

130
00:12:16,443 --> 00:12:20,280
看看它是否也能生成正确的输出。

131
00:12:20,280 --> 00:12:24,600
希望它能，我觉得应该可以。

132
00:12:24,600 --> 00:12:26,957
当然，当你修改提示时，

133
00:12:26,958 --> 00:12:36,760
也有必要做一些回归测试，以确保在修复提示3和4的错误输出时，

134
00:12:36,760 --> 00:12:41,480
没有破坏提示0的输出。

135
00:12:41,480 --> 00:12:50,570
现在你可以看出，如果我必须复制粘贴五个提示，customer_msg_0、1、2、3和4到我的Jupyter Notebook，

136
00:12:50,571 --> 00:12:55,813
运行它们，然后手动查看它们，看看它们是否都放在了正确的类别和产品中，

137
00:12:55,814 --> 00:12:57,320
你可以这么做。

138
00:12:57,320 --> 00:13:00,080
我可以看着这个说：是的，类别是电视和家庭影院系统产品。

139
00:13:00,080 --> 00:13:02,160
嗯，看起来你把它们都找到了。

140
00:13:02,160 --> 00:13:04,657
但实际上，手动做这个有点痛苦，

141
00:13:04,658 --> 00:13:12,280
需要用眼睛仔细检查输出结果，确保这确实是正确的输出。

142
00:13:12,280 --> 00:13:19,199
当你调整的开发集不再只有少数几个例子时，

143
00:13:19,200 --> 00:13:27,200
自动化测试流程就变得很有用了。

144
00:13:27,200 --> 00:13:35,780
这里有10个例子，我指定了10条客户留言。

145
00:13:35,780 --> 00:13:39,260
这是客户的留言，我有预算可以买什么电视？

146
00:13:39,260 --> 00:13:41,880
还有什么是理想的答案？

147
00:13:41,880 --> 00:13:45,429
把这个当作测试数据集里的正确答案，

148
00:13:45,430 --> 00:13:49,240
或者我应该说开发数据集，因为我们实际上是在调整这个。

149
00:13:49,240 --> 00:13:54,700
我们在这里收集了10个例子，索引编号从0到9，

150
00:13:54,701 --> 00:14:03,720
最后一个是：“如果用户说我想要热水浴缸时间机器，我们真的没有相关产品，非常抱歉。”

151
00:14:03,720 --> 00:14:07,040
理想的答案是空集。

152
00:14:07,040 --> 00:14:13,343
现在，如果你想自动评估

153
00:14:13,344 --> 00:14:19,920
某个提示在这10个例子中的某个例子上的运行结果，这里有一个函数可以实现。

154
00:14:19,920 --> 00:14:24,280
这是一个有点长的函数，如果你愿意的话，可以暂停视频并仔细阅读。

155
00:14:24,280 --> 00:14:28,020
让我演示一下它实际上在做什么。

156
00:14:28,020 --> 00:14:32,440
让我打印出索引是0的客户留言。

157
00:14:32,440 --> 00:14:38,080
客户留言：“如果我预算有限，我可以买哪台电视？”

158
00:14:38,080 --> 00:14:42,280
我们也来打印一下这个留言理想的答案是什么。

159
00:14:42,280 --> 00:14:49,400
所以理想的答案是根据提示列出所有我们希望获取的电视。

160
00:14:49,400 --> 00:14:56,920
现在让我调用这个提示，这是在这个客户消息上使用该用户产品和类别信息的提示符v2。

161
00:14:56,920 --> 00:14:57,920
我们打印出来。

162
00:14:57,920 --> 00:15:01,680
然后我们将调用eval。

163
00:15:01,680 --> 00:15:09,520
我们将调用eval_responsive_with_ideal函数，看看返回结果与理想答案的匹配程度如何。

164
00:15:09,520 --> 00:15:16,460
在这种情况下，它确实输出了我们想要的类别，也输出了整个产品列表。

165
00:15:16,460 --> 00:15:21,800
所以这给你一个1.0的分数。

166
00:15:21,800 --> 00:15:28,280
再给你们举个例子，事实证明我知道它在第7个例子上出错了。

167
00:15:28,280 --> 00:15:36,720
所以如果我把这个从0改成7然后运行，这就是它得到的。

168
00:15:36,720 --> 00:15:42,300
哦，让我把这个也更新成7。

169
00:15:42,300 --> 00:15:50,520
在这个客户留言下面，这是理想的答案，它应该输出在”游戏机和配件“分类下面。

170
00:15:50,520 --> 00:15:52,600
这是游戏机和配件。

171
00:15:52,600 --> 00:16:01,720
但是这里的回应有三个输出，实际上应该有一、二、三、四、五个输出。

172
00:16:01,720 --> 00:16:04,480
它缺少了一些产品。

173
00:16:04,480 --> 00:16:07,357
如果我现在要调整提示的话，

174
00:16:07,358 --> 00:16:18,970
我会用一个for循环来遍历所有10个来自开发数据集的示例，我们逐个获取客户留言，

175
00:16:18,971 --> 00:16:25,800
得到理想答案，正确答案，调用函数得到返回结果，并对结果进行评估，

176
00:16:25,800 --> 00:16:27,880
然后你知道，累积平均。

177
00:16:27,880 --> 00:16:32,200
让我运行一下这个。

178
00:16:32,200 --> 00:16:35,960
这个运行起来需要一段时间，但当它运行完毕时，这就是结果。

179
00:16:35,960 --> 00:16:38,320
我们正在浏览10个示例。

180
00:16:38,320 --> 00:16:41,240
看起来第7个示例是错误的。

181
00:16:41,240 --> 00:16:46,120
所以在10个示例的测试中，正确的比例是90%。

182
00:16:46,120 --> 00:16:53,760
因此，如果你调整提示，可以重新运行以查看正确百分比是上升还是下降。

183
00:16:53,760 --> 00:17:00,480
你刚才在这个笔记本中看到的是完成这个项目符号列表中的第一、二、三步。

184
00:17:00,480 --> 00:17:09,060
这已经提供了一个相当好的开发数据集，包括10个示例，用于调整和验证提示是否有效。

185
00:17:09,060 --> 00:17:11,243
如果你需要更高的严谨性，

186
00:17:11,244 --> 00:17:20,020
那么你现在已经有了需要的软件，可以收集一个随机抽样的示例数据集，比如100个示例及其理想输出。

187
00:17:20,020 --> 00:17:25,800
甚至可以做的更好，用一个你在调整提示时完全没有测试过的保留测试集，以保证严谨性。

188
00:17:25,800 --> 00:17:29,714
但对于很多应用来说，做到第三点就足够了，

189
00:17:29,715 --> 00:17:35,929
像我刚才在Jupyter Notebook做的这些，你也可以应用，

190
00:17:36,440 --> 00:17:39,840
帮你快速将系统准确率优化到很好。

191
00:17:39,840 --> 00:17:46,600
再次强调，如果你正在开发对安全性要求很高的应用或者

192
00:17:46,600 --> 00:17:50,900
可能存在实质性伤害风险的应用，

193
00:17:50,901 --> 00:17:59,340
那么负责任的做法当然是在任何地方使用它之前，进行大规模的测试集验证以严格验证其准确性。

195
00:17:59,340 --> 00:18:00,340
就是这样。

196
00:18:00,340 --> 00:18:12,860
我发现，使用提示构建应用程序的工作流程与使用监督学习构建应用程序的工作流程非常不同，迭代的步伐感觉快了很多。

198
00:18:12,860 --> 00:18:14,757
如果你还没有尝试过这种方法，

199
00:18:14,758 --> 00:18:22,380
你可能会对只用几个精心策划的棘手例子构建的评估方法的效果感到惊讶。

200
00:18:22,380 --> 00:18:27,400
你可能会认为，10个例子对于几乎所有事情来说，这在统计上都是不成立的，

201
00:18:27,401 --> 00:18:36,643
但你可能会在实际使用这个程序时惊讶地发现，将一小部分棘手的例子加入到你的开发集中，

203
00:18:36,644 --> 00:18:43,500
可能会在帮助你和你的团队得到一套有效的提示和有效的系统方面，效果出奇的好。

204
00:18:43,500 --> 00:18:45,029
在这个视频中，

205
00:18:45,030 --> 00:18:55,500
输出可以定量地进行评估，因为有一个期望的输出，你可以判断它是否生成了期望的输出。

206
00:18:55,500 --> 00:18:56,229
那么下一个视频中，

207
00:18:56,230 --> 00:19:04,900
让我们一起看看在那种没有标准答案的情况下，如何评估输出。
