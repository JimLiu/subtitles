1
00:05:00,000 --> 00:00:11,080
在这第一个视频中，我想和大家分享一下大型语言模型（LLM）的概述，它们是如何工作的。

3
00:00:11,080 --> 00:00:16,480
我们将深入了解它们的训练过程，以及诸如分词器之类的细节，以及这些细节如何影响

4
00:00:16,480 --> 00:00:20,080
在提示LLM时的输出结果。

5
00:00:20,080 --> 00:00:25,800
我们还将了解LLM的聊天格式，这是一种既可以指定系统消息，

6
00:00:25,800 --> 00:00:31,360
也可以指定用户消息的方法，了解您可以利用这种功能做什么。

7
00:00:31,360 --> 00:00:32,680
让我们来看看。

8
00:00:32,680 --> 00:00:36,280
首先，大型语言模型是如何工作的？

9
00:00:36,280 --> 00:00:41,080
您可能已经熟悉了文本生成过程，您可以给出一个提示，比如“我喜欢吃”，

10
00:00:41,080 --> 00:00:47,360
然后让LLM填充在这个提示下可能的补全内容。

11
00:00:47,360 --> 00:00:52,560
它可能会说“百吉饼加奶油奶酪”或“我妈妈做的肉饼”或“和朋友一起在外面（吃）”。

12
00:00:52,560 --> 00:00:55,320
但是模型是如何学会这个的呢？

13
00:00:55,320 --> 00:01:00,800
实际上，训练LLM的主要工具是监督学习。

14
00:01:00,800 --> 00:01:07,320
在监督学习中，计算机使用带标签的训练数据学习输入/输出或X/Y映射。

16
00:01:07,320 --> 00:01:12,480
所以，例如，如果你使用监督学习来学习对餐厅评价

17
00:01:12,480 --> 00:01:17,000
进行好评/差评分类，你可能会收集这样的训练集，其中像

18
00:01:17,000 --> 00:01:23,320
“熏牛肉三明治很棒”被标记为好评，依此类推。

19
00:01:23,320 --> 00:01:28,320
而“服务慢，食物一般”是差评，“伯爵灰茶非常棒”

20
00:01:28,320 --> 00:01:30,440
带有正面标签。

21
00:01:30,440 --> 00:01:35,960
顺便说一下，Isa和我都出生在英国，所以我们都喜欢我们的伯爵茶。

22
00:01:35,960 --> 00:01:41,740
因此，监督学习的过程通常是获得带标签的数据，然后

23
00:01:41,740 --> 00:01:47,240
在数据上训练一个模型，训练完成后，您可以部署并调用该模型，

24
00:01:47,240 --> 00:01:50,920
给它一个新的餐厅评论，比如"我吃过的最好的披萨"。

25
00:01:50,920 --> 00:01:54,320
希望您能输出这是一个积极的情感。

26
00:01:54,320 --> 00:02:00,280
事实证明，监督学习是训练大型语言的核心构建模块模型。

28
00:02:00,280 --> 00:02:05,840
具体来说，可以通过使用监督学习来反复

29
00:02:05,840 --> 00:02:08,160
预测下一个单词来构建大型语言模型。

30
00:02:08,160 --> 00:02:14,740
假设在您的训练集中有很多文本数据，您必须将句子"我最喜欢的

31
00:02:14,740 --> 00:02:17,480
食物是百吉饼配上奶油奶酪和熏鲑鱼"。

32
00:02:17,480 --> 00:02:23,480
那么这个句子就会变成一个训练示例序列，给定一个句子

33
00:02:23,480 --> 00:02:28,800
片段"我最喜欢的食物是"，如果你想预测下一个单词，这种情况下是

34
00:02:28,800 --> 00:02:36,520
"百吉饼"，或者给定句子片段或句子前缀"我最喜欢的食物是百吉饼"，

35
00:02:36,520 --> 00:02:41,120
接下来的单词就是"配上"，依此类推。

36
00:02:41,120 --> 00:02:45,600
而且，给定一个包含数千亿甚至更多单词的大型训练集，

37
00:02:45,600 --> 00:02:51,520
你就可以创建一个庞大的训练集，从一句话的一部分开始

38
00:02:51,520 --> 00:02:56,600
或者一段文字的一部分，反复让语言模型学会预测

39
00:02:56,600 --> 00:02:58,440
下一个单词是什么。

40
00:02:58,440 --> 00:03:04,820
所以今天有两种主要类型的大型语言模型。

41
00:03:04,820 --> 00:03:12,520
第一种是基础LLM，第二种是越来越多地被使用的指令调优LLM（Instruction Tuned LLM）。

43
00:03:12,520 --> 00:03:18,200
所以基础LLM会根据文本训练数据反复预测下一个单词。

44
00:03:18,200 --> 00:03:22,720
因此，如果我给它一个提示“从前有一个独角兽”，那么它可能会通过反复地

45
00:03:22,720 --> 00:03:27,200
一次预测一个单词，编写一个关于独角兽的故事，

46
00:03:27,200 --> 00:03:30,480
讲述独角兽与所有独角兽朋友一起生活在一个神奇的森林里。

47
00:03:30,480 --> 00:03:36,600
现在的一个缺点是，如果你给它一个提示“法国的首都是什么？”

49
00:03:36,600 --> 00:03:40,600
在互联网上很有可能存在关于法国的一系列测验问题。

50
00:03:40,600 --> 00:03:42,920
所以它可能会用“法国最大的城市是什么？”来完成这个问题，

51
00:03:42,920 --> 00:03:46,400
“法国的人口是多少？”等等。

52
00:03:46,400 --> 00:03:50,880
但是你真正想要的是让它告诉你法国的首都是什么，

53
00:03:50,880 --> 00:03:54,360
而不是列出所有这些问题。

54
00:03:54,360 --> 00:03:58,800
所以，指令调优LLM尝试遵循指令，并希望能正确回答出，

55
00:03:58,800 --> 00:04:02,160
法国的首都是巴黎。

56
00:04:02,160 --> 00:04:06,120
如何从基础LLM转变为指令调优LLM？

57
00:04:06,120 --> 00:04:11,760
这就是训练一个类似ChatGPT的指令调优LLM的过程。

58
00:04:11,760 --> 00:04:16,080
首先，你需要在大量数据上训练一个基础LLM，可能是数千亿甚至

59
00:04:16,080 --> 00:04:22,080
更多的词汇，这个过程可能需要在大型超级计算系统上进行数月。

60
00:04:22,080 --> 00:04:28,480
在训练了基础LLM之后，你可以通过在一小部分的例子上微调模型

61
00:04:28,480 --> 00:04:35,820
来进一步训练它，这些例子的输出遵循输入的指令。

62
00:04:35,820 --> 00:04:42,600
所以，例如，你可以请负责数据标注的承包商帮助你编写很多指令的示例，

63
00:04:42,600 --> 00:04:48,040
以及如何对这些指令高质量的回应，这样就形成了一套训练集，可以进行

64
00:04:48,040 --> 00:04:55,080
额外的微调，使其在尝试遵循指令的情况下，学会预测下一个词是什么。

66
00:04:55,080 --> 00:05:01,520
在此之后，为了提高LLM输出质量的常见方法是获得人类

67
00:05:01,520 --> 00:05:07,520
对许多不同LLM输出质量的评分，例如输出是否

68
00:05:07,520 --> 00:05:14,120
有帮助、诚实和无害，然后您可以进一步调整LLM以提高

69
00:05:14,120 --> 00:05:18,000
生成更高评分输出的概率。

70
00:05:18,000 --> 00:05:24,600
最常用的技术是RLHF，即来自人类反馈的强化学习。

72
00:05:24,600 --> 00:05:29,800
而训练基础LLM可能需要几个月的时间，从基础LLM

73
00:05:29,800 --> 00:05:36,680
到指令调优LLM的过程可能只需要几天时间，数据集规模

74
00:05:36,680 --> 00:05:40,360
和计算资源都要小得多。

75
00:05:40,360 --> 00:05:42,560
所以这就是你如何使用LLM的方法。

76
00:05:42,560 --> 00:05:45,560
我将导入一些库。

77
00:05:45,560 --> 00:05:48,840
我将在这里加载我的OpenAI密钥。

78
00:05:48,840 --> 00:05:52,620
稍后在这个视频中，我会更详细地介绍这个。

79
00:05:52,620 --> 00:05:57,580
这里有一个辅助函数，用于根据提示获取补全（completion）。

80
00:05:57,580 --> 00:06:04,640
如果您还没有在计算机上安装OpenAI软件包，您可能需要运行pip

81
00:06:04,640 --> 00:06:09,960
安装OpenAI，但我已经在这里安装了，所以我不会运行那个。

82
00:06:09,960 --> 00:06:20,280
让我按shift enter运行这些，现在我可以设置response=get_completion

83
00:06:20,280 --> 00:06:34,820
“法国的首都是什么？”，希望它能给我一个好结果。

84
00:06:34,820 --> 00:06:42,620
现在关于大型语言模型的描述，到目前为止，我谈论的是

85
00:06:42,620 --> 00:06:47,940
一次预测一个单词，但实际上还有一个更重要的技术细节。

86
00:06:47,940 --> 00:06:54,980
如果你让它“把单词lollipop中的字母倒过来”。

87
00:06:54,980 --> 00:07:00,380
这看起来像是一个简单的任务，也许一个四岁的孩子就能完成这个任务，但如果你

88
00:07:00,380 --> 00:07:09,120
让ChatGPT去做这个，它实际上输出的是一种有点乱七八糟的东西。

89
00:07:09,120 --> 00:07:14,100
这不是L-O-L-L-I-P-O-P，这不是lollipop的字母倒过来。

90
00:07:14,100 --> 00:07:19,620
那么为什么ChatGPT无法完成看似相对简单的任务呢。

91
00:07:19,620 --> 00:07:24,300
事实证明，大型语言模型的工作方式还有一个更重要的细节，

92
00:07:24,300 --> 00:07:30,220
那就是它实际上不是反复预测下一个单词，而是反复预测

93
00:07:30,220 --> 00:07:36,720
下一个标记（Token），而一个LLM实际上会做的是，它会接收一系列字符，比如

94
00:07:36,720 --> 00:07:43,260
“learning new things is fun”，并将字符组合在一起形成代表常见

95
00:07:43,260 --> 00:07:46,580
字符序列的标记。

96
00:07:46,580 --> 00:07:52,660
所以在这里“learning new things is fun”，每个都是相当常见的词，所以每个标记

97
00:07:52,660 --> 00:07:57,980
对应一个词或一个词的空格或感叹号。

98
00:07:57,980 --> 00:08:03,540
但是，如果你给它一些不太常用的词作为输入，比如“Prompting

99
00:08:03,540 --> 00:08:10,340
is a powerful developer tool.”对于开发者来说，Prompting这个词在英语中还不是那么常见

100
00:08:10,340 --> 00:08:14,900
但肯定越来越受欢迎，所以“Prompting”实际上被分解成三个

101
00:08:14,900 --> 00:08:21,340
标记，分别是"prompt"、"pt"和"ing"，因为这三个是常见的字母序列

102
00:08:21,340 --> 00:08:27,220
如果你给它一个“棒棒糖（lollipop）”这个词，分词器实际上把这个

103
00:08:27,220 --> 00:08:34,300
分成三个标记"l"、"o"和"ipop"，因为ChatGPT没有看到单独的

104
00:08:34,300 --> 00:08:40,200
字母，而是看到了这三个标记，所以要正确地按相反的顺序打印

105
00:08:40,200 --> 00:08:43,420
出这些字母就更困难了。

106
00:08:43,420 --> 00:08:52,540
所以这里有一个技巧，如果我在这些字母之间加上破折号，

107
00:08:52,540 --> 00:08:56,780
空格也可以，或者其他东西也可以，然后把字母和棒棒糖拿过来

108
00:08:56,780 --> 00:09:03,100
反过来，它实际上做得更好，这个LOLLIPOP，原因是

109
00:09:03,100 --> 00:09:08,740
如果你用破折号把lollipop的字母隔开，它会把每一个

110
00:09:08,740 --> 00:09:14,140
这些字符分成一个个的标记，让它更容易看到单独的

111
00:09:14,140 --> 00:09:17,140
字母，然后按相反的顺序打印出来。

112
00:09:17,140 --> 00:09:23,180
所以如果你想用ChatGPT玩像wordle或scrap这样的单词游戏，

113
00:09:23,180 --> 00:09:29,460
这个巧妙的技巧有助于更好地看到单词的各个字母。

114
00:09:29,460 --> 00:09:34,700
对于英语，一个标记大致平均对应四个字符，

115
00:09:34,700 --> 00:09:40,980
或者大约三分之二个单词，所以不同的大型语言模型通常会有不同的

116
00:09:40,980 --> 00:09:45,780
关于它可以接受的输入加输出标记的数量限制。

117
00:09:45,780 --> 00:09:50,700
输入通常被称为上下文（context），输出通常被称为补全（completion）

118
00:09:50,700 --> 00:09:56,700
例如，最常用的ChatGPT模型GPT-3.5 Turbo在输入和输出中的限制

119
00:09:56,700 --> 00:10:01,700
大约是4000个标记。

120
00:10:01,700 --> 00:10:05,660
所以，如果你尝试输入一个超过这个长度的上下文，它实际上会抛出

121
00:10:05,660 --> 00:10:08,540
一个异常或产生一个错误。

122
00:10:08,540 --> 00:10:17,460
接下来，我想和你分享另一种使用LLM API的强大方法，它涉及指定

123
00:10:17,460 --> 00:10:22,580
独立的系统（system）、用户（user）和助手（assistant）消息。

124
00:10:22,580 --> 00:10:29,900
让我给你展示一个例子，然后我们可以更详细地解释它实际上在做什么。

125
00:10:29,900 --> 00:10:35,660
这里有一个新的辅助函数，叫做从消息中获取补全，当我们提示这个LLM时

126
00:10:35,660 --> 00:10:39,460
我们将给它提供多条消息。

127
00:10:39,460 --> 00:10:42,500
这是一个你可以做的例子。

128
00:10:42,500 --> 00:10:49,860
首先，我将以系统的角色指定一条信息，以便这个助手信息

129
00:10:49,860 --> 00:10:55,500
系统信息的内容是，你是一个以苏斯博士的风格回应的助手。

131
00:10:56,500 --> 00:11:00,700
然后我将指定一个用户信息，所以第二条信息的角色是用户，

132
00:11:00,700 --> 00:11:07,180
而这个内容是“给我写一首关于快乐胡萝卜的非常短的诗。”

133
00:11:07,180 --> 00:11:12,340
所以让我们运行这个，温度（Temperature）参数设置为1，我实际上永远不知道会出现什么

134
00:11:12,340 --> 00:11:14,740
但好吧，这是一首很酷的诗。

135
00:11:14,740 --> 00:11:19,580
“哦，我看到的这根胡萝卜是多么快乐”，而且它实际上运行得相当好。

136
00:11:19,580 --> 00:11:22,420
好吧，干得好ChatGPT。

137
00:11:22,420 --> 00:11:30,380
所以在这个例子中，系统消息指定了你想要的

138
00:11:30,380 --> 00:11:35,500
大语言模型的整体语言风格，而用户消息是一个具体的指令，

139
00:11:35,500 --> 00:11:41,900
你希望在系统中执行这个更高级别的行为消息。

141
00:11:41,900 --> 00:11:46,120
这里有一个说明它是如何工作的例子。

142
00:11:46,120 --> 00:11:51,700
所以这就是聊天格式的工作原理。

143
00:11:51,700 --> 00:11:57,100
系统消息设定了大型语言模型或

144
00:11:57,100 --> 00:12:03,140
助手的整体行为风格，然后当你给出用户消息，比如说“给我

145
00:12:03,140 --> 00:12:10,860
讲个笑话或者写首诗”，它会根据你在用户消息中要求的内容，

146
00:12:10,860 --> 00:12:19,220
以及在系统消息中设定的整体行为风格，输出一个合适的回应。

148
00:12:19,220 --> 00:12:24,500
顺便说一下，虽然我没有在这里展示，但如果你想在多轮对话中

149
00:12:24,500 --> 00:12:33,140
使用这个功能，你也可以用这种消息格式输入助手消息，让ChatGPT

150
00:12:33,140 --> 00:12:41,820
了解它之前说过什么，如果你想根据之前的对话继续对话的话。

152
00:12:41,820 --> 00:12:45,180
但是这里有更多的例子。

153
00:12:45,180 --> 00:12:52,340
如果你想设置语言风格，告诉它输出一句话，那么在

154
00:12:52,340 --> 00:12:58,460
系统消息中，我可以说你的所有回答都必须是一句话长，当我执行

155
00:12:58,460 --> 00:13:03,540
这个操作时，它输出的一句话不再是诗歌，也不是苏斯博士的风格，但这

156
00:13:03,540 --> 00:13:10,060
是一句关于快乐胡萝卜的故事。

157
00:13:10,060 --> 00:13:17,620
如果我们想同时指定风格和长度，那么我可以使用系统消息，

158
00:13:17,620 --> 00:13:22,020
在系统回应中说，以苏斯博士的风格，你的所有句子都必须是一句话长，

159
00:13:22,020 --> 00:13:32,860
现在这生成了一个很好的一句话诗歌，总是微笑，从不可怕。

160
00:13:32,860 --> 00:13:35,740
我喜欢这是一首非常快乐的诗。

161
00:13:35,740 --> 00:13:42,780
然后最后，只是为了好玩，如果你在使用LLM，想知道有多少个tokens

162
00:13:42,780 --> 00:13:49,020
你在这里使用了一个稍微复杂一点的辅助函数。

163
00:13:49,020 --> 00:13:57,500
它从OpenAI API获取响应，然后使用响应中的其他值

164
00:13:57,500 --> 00:14:05,060
告诉你在你的API调用中使用了多少“prompt tokens”、“completion tokens”和“total tokens”。

166
00:14:05,060 --> 00:14:19,260
让我定义一下，如果我现在运行这个，这是响应，这是一个关于我们使用了多少个

167
00:14:19,260 --> 00:14:20,620
tokens的计数。

168
00:14:20,620 --> 00:14:27,980
所以这个输出有55个tokens，而提示输入有37个tokens，所以这个

169
00:14:27,980 --> 00:14:31,740
一共使用了92个tokens。

170
00:14:31,740 --> 00:14:38,740
当我在实践中使用LLM模型时，我实际上并不太担心我使用的tokens数量。

172
00:14:38,740 --> 00:14:42,420
也许在一个情况下，检查tokens数量是值得的，那就是如果你担心

173
00:14:42,420 --> 00:14:48,100
用户可能给你太长的输入，超过了4000个左右的tokens

174
00:14:48,100 --> 00:14:52,940
ChatGPT的限制，在这种情况下，你可以仔细检查一下tokens数量并截断

175
00:14:52,940 --> 00:14:58,380
它以确保你在大型语言模型的输入tokens限制范围内。

176
00:14:58,380 --> 00:15:04,780
现在我想和你分享如何使用大型语言模型的另一个技巧。

177
00:15:04,780 --> 00:15:10,900
调用OpenAI API需要使用与免费或付费账户绑定的API密钥，

178
00:15:10,900 --> 00:15:17,940
因此许多开发者会像这样将API密钥以纯文本形式写入他们的

179
00:15:17,940 --> 00:15:26,460
Jupyter Notebook，这是一种使用API密钥的不太安全的方式，我不建议

180
00:15:26,460 --> 00:15:32,580
你使用，因为这样太容易与他人共享这个Notebook或将其检查到

181
00:15:32,580 --> 00:15:38,700
GitHub或其他地方，从而泄露你的API密钥给其他人。

182
00:15:38,700 --> 00:15:46,140
相比之下，你在 Jupyter Notebook中看到我做的是这段代码，我使用

183
00:15:46,140 --> 00:15:52,900
一个库 dotenv，然后运行这个命令 load dotenv find dotenv 来读取一个本地

184
00:15:52,900 --> 00:15:58,660
文件，叫做 .env，里面包含了我的密钥。

185
00:15:58,660 --> 00:16:04,420
所以通过这段代码片段，我在本地存储了一个名为 .env 的文件，其中包含

186
00:16:04,420 --> 00:16:13,140
我的 API 密钥，然后将其加载到操作系统的环境变量中，然后 os.getenv，

187
00:16:13,140 --> 00:16:21,180
OPENAI_API_KEY将其存储到这个变量中，在整个过程中，我不需要将

188
00:16:21,180 --> 00:16:27,020
API 密钥以明文形式输入到我的 Jupyter Notebook中。

189
00:16:27,020 --> 00:16:33,380
所以这是一个相对更安全、更好的访问 API 密钥的方法，实际上

190
00:16:33,380 --> 00:16:39,420
这是一种用于存储来自许多不同在线服务的不同 API 密钥的通用方法

191
00:16:39,420 --> 00:16:44,180
你可能想要使用并从你的 Jupyter Notebook中调用。

192
00:16:44,180 --> 00:16:54,220
我认为，提示（prompting）正在革新AI应用开发的程度，仍然被低估了。

194
00:16:54,220 --> 00:16:58,900
在传统的监督式机器学习工作流中，就像我刚才提到的餐厅评论好评/差评分类

195
00:16:58,900 --> 00:17:03,340
的例子，如果你想建立一个分类器来

196
00:17:03,340 --> 00:17:07,900
分类餐厅评论的正面或负面情感，你首先需要获得一堆标签数据，

197
00:17:07,900 --> 00:17:13,200
也许是几百个例子，这可能需要几周，也许一个月。然后，

198
00:17:13,200 --> 00:17:18,620
你会在数据上训练一个模型，获取一个合适的开源模型，

199
00:17:18,620 --> 00:17:25,540
对模型进行调优，评估模型，这可能需要几天、几周甚至几个月的时间。接下来，

200
00:17:25,540 --> 00:17:31,140
您可能需要找到一个云服务来部署它，然后将模型上传到云端，

201
00:17:31,140 --> 00:17:34,940
然后运行模型，最后才能调用你的模型。再说，这种情况并不少见，

202
00:17:34,940 --> 00:17:39,820
这可能需要一个团队几个月的时间才能完成。

203
00:17:39,820 --> 00:17:46,900
与基于提示的机器学习相比，当你有一个文本应用程序时，你可以指定

204
00:17:46,900 --> 00:17:51,780
一个提示，这可能需要几分钟甚至几个小时，如果你需要迭代几次才能得到

205
00:17:51,780 --> 00:17:59,880
一个有效的提示，然后在几个小时甚至最多几天内，但坦率地说，更多的是几个小时

206
00:17:59,880 --> 00:18:05,880
你可以使用API调用来运行这个程序，并开始调用模型，一旦

207
00:18:05,880 --> 00:18:11,480
你完成了这个过程，可能只需要几分钟或几个小时，你就可以开始调用模型并

208
00:18:11,480 --> 00:18:17,280
开始进行推断，所以有些应用程序过去可能需要我花费六个月甚至一

209
00:18:17,280 --> 00:18:22,840
年的时间来构建，现在你可以在几分钟或几小时内，甚至很少的几天内

210
00:18:22,840 --> 00:18:29,120
使用提示来构建，这正在彻底改变可以快速构建的AI应用程序。

211
00:18:29,120 --> 00:18:34,680
一个重要的注意事项是，这适用于许多非结构化数据应用，包括特别是

212
00:18:34,680 --> 00:18:40,600
文本应用程序，也许越来越多的视觉应用程序，尽管视觉技术

213
00:18:40,600 --> 00:18:44,520
目前还不够成熟，但它正在朝这个方向发展。

214
00:18:44,520 --> 00:18:49,520
这个方法对于结构化数据应用（即机器学习）并不十分适用，

215
00:18:49,520 --> 00:18:54,840
在Excel表格中有大量数值的表格数据应用上，

216
00:18:54,840 --> 00:19:01,120
但对于适用的应用来说，AI组件可以快速构建的事实，

217
00:19:01,120 --> 00:19:07,480
正在改变整个系统可能构建的工作流程。

218
00:19:07,480 --> 00:19:11,080
构建整个系统可能仍然需要几天或几周的时间，但至少这部分

219
00:19:11,080 --> 00:19:16,880
可以更快地完成，所以让我们继续下一个视频，Isa将展示

220
00:19:16,880 --> 00:19:24,400
如何使用这些组件来评估客户服务助手的输入，

221
00:19:24,400 --> 00:19:29,040
这将是一个更大示例的一部分，您将在本课程中看到

222
00:19:29,040 --> 00:19:33,760
如何为在线零售商构建一个客户服务助手。
