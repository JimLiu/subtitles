[Script Info]

Title: sc-openai-c2-L1-vid2_1
ScriptType: v4.00+
WrapStyle: 0
Collisions: Reverse
PlayResX: 384
PlayResY: 288
Timer: 100.0000
ScaledBorderAndShadow: no
Last Style Storage: Default
Video Aspect Ratio: 0
Video Zoom: 6
Video Position: 0

[V4+ Styles]
Format: Name,Fontname,Fontsize,PrimaryColour,SecondaryColour,OutlineColour,BackColour,Bold,Italic,Underline,StrikeOut,ScaleX,ScaleY,Spacing,Angle,BorderStyle,Outline,Shadow,Alignment,MarginL,MarginR,MarginV,Encoding
Style: Default,LXGW WenKai,16,&H0080FFFF,&H000000FF,&H00000000,&H00000000,-1,0,0,0,100,100,0,0,1,2,2,2,1,1,6,1
Style: Secondary,Helvetica,10,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,-1,0,0,0,100,100,0,0,1,2,2,2,1,1,6,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 9,0:00:05.0,0:00:10.0,Secondary,,0,0,0,,{\an7\fs10\pos(9,11)\fad(300,1000)}{\1c&H00FFFFFF&\2c&H0000FF&}翻译：{\1c&H80FFFF&\2c&H0000FF&}宝玉 + GPT-4
Dialogue: 1,0:05:00.0,0:00:10.8,Secondary,,0,0,0,,In this first video, I'd like to share with you an overview of how LLMs, large language
Dialogue: 1,0:00:10.8,0:00:11.8,Secondary,,0,0,0,,models work.
Dialogue: 1,0:00:11.8,0:00:16.48,Secondary,,0,0,0,,We'll go into how they are trained, as well as details like the tokenIsa and how that
Dialogue: 1,0:00:16.48,0:00:20.8,Secondary,,0,0,0,,can affect the output of when you prompt an LLM.
Dialogue: 1,0:00:20.8,0:00:25.80,Secondary,,0,0,0,,And we'll also take a look at the chat format for LLMs, which is a way of specifying both
Dialogue: 1,0:00:25.80,0:00:31.36,Secondary,,0,0,0,,system as well as user messages and understand what you can do with that capability.
Dialogue: 1,0:00:31.36,0:00:32.68,Secondary,,0,0,0,,Let's take a look.
Dialogue: 1,0:00:32.68,0:00:36.28,Secondary,,0,0,0,,First, how does a large language model work?
Dialogue: 1,0:00:36.28,0:00:41.8,Secondary,,0,0,0,,You're probably familiar with the text generation process where you can give a prompt, "I love
Dialogue: 1,0:00:41.8,0:00:47.36,Secondary,,0,0,0,,eating" and ask an LLM to fill in what the things are likely completions given this prompt.
Dialogue: 1,0:00:47.36,0:00:52.56,Secondary,,0,0,0,,And it may say "bagels with cream cheese" or "my mother's meatloaf" or "out with friends".
Dialogue: 1,0:00:52.56,0:00:55.32,Secondary,,0,0,0,,But how did the model learn to do this?
Dialogue: 1,0:00:55.32,0:01:00.80,Secondary,,0,0,0,,The main tool used to train an LLM is actually supervised learning.
Dialogue: 1,0:01:00.80,0:01:06.32,Secondary,,0,0,0,,In supervised learning, a computer learns an input/output or X/Y mapping using labeled
Dialogue: 1,0:01:06.32,0:01:07.32,Secondary,,0,0,0,,training data.
Dialogue: 1,0:01:07.32,0:01:12.48,Secondary,,0,0,0,,So, for example, if you're using supervised learning to learn to classify the sentiment
Dialogue: 1,0:01:12.48,0:01:17.0,Secondary,,0,0,0,,of restaurant reviews, you might collect a training set like this, where a review like
Dialogue: 1,0:01:17.0,0:01:23.32,Secondary,,0,0,0,,"the pastrami sandwich was great" is labeled as a positive sentiment review and so on.
Dialogue: 1,0:01:23.32,0:01:28.32,Secondary,,0,0,0,,And "service was slow, the food was so-so" is negative and "the earl grey tea was fantastic"
Dialogue: 1,0:01:28.32,0:01:30.44,Secondary,,0,0,0,,has a positive label.
Dialogue: 1,0:01:30.44,0:01:35.96,Secondary,,0,0,0,,By the way, both Isa and I were born in the UK and so both of us like our earl grey tea.
Dialogue: 1,0:01:35.96,0:01:41.74,Secondary,,0,0,0,,And so the process for supervised learning is typically to get labeled data and then
Dialogue: 1,0:01:41.74,0:01:47.24,Secondary,,0,0,0,,train a model on data and after training, you can then deploy and call the model and
Dialogue: 1,0:01:47.24,0:01:50.92,Secondary,,0,0,0,,give it a new restaurant review like "best pizza I've ever had".
Dialogue: 1,0:01:50.92,0:01:54.32,Secondary,,0,0,0,,You hopefully output that that has a positive sentiment.
Dialogue: 1,0:01:54.32,0:01:59.28,Secondary,,0,0,0,,It turns out that supervised learning is a core building block for training large language
Dialogue: 1,0:01:59.28,0:02:00.28,Secondary,,0,0,0,,models.
Dialogue: 1,0:02:00.28,0:02:05.84,Secondary,,0,0,0,,Specifically, a large language model can be built by using supervised learning to repeatedly
Dialogue: 1,0:02:05.84,0:02:08.16,Secondary,,0,0,0,,predict the next word.
Dialogue: 1,0:02:08.16,0:02:14.74,Secondary,,0,0,0,,Let's say that in your training set of a lot of text data, you have to sentence "my favorite
Dialogue: 1,0:02:14.74,0:02:17.48,Secondary,,0,0,0,,food is a bagel with cream cheese and mozz".
Dialogue: 1,0:02:17.48,0:02:23.48,Secondary,,0,0,0,,Then this sentence is turned into a sequence of training examples where given a sentence
Dialogue: 1,0:02:23.48,0:02:28.80,Secondary,,0,0,0,,fragment "my favorite food is a" if you want to predict the next word in this case was
Dialogue: 1,0:02:28.80,0:02:36.52,Secondary,,0,0,0,,"bagel" or given the sentence fragment or sentence prefix "my favorite food is a bagel",
Dialogue: 1,0:02:36.52,0:02:41.12,Secondary,,0,0,0,,the next word in this case would be "with" and so on.
Dialogue: 1,0:02:41.12,0:02:45.60,Secondary,,0,0,0,,And given a large training set of hundreds of billions or sometimes even more words,
Dialogue: 1,0:02:45.60,0:02:51.52,Secondary,,0,0,0,,you can then create a massive training set where you can start off with part of a sentence
Dialogue: 1,0:02:51.52,0:02:56.60,Secondary,,0,0,0,,or part of a piece of text and repeatedly ask the language model to learn to predict
Dialogue: 1,0:02:56.60,0:02:58.44,Secondary,,0,0,0,,what is the next word.
Dialogue: 1,0:02:58.44,0:03:04.82,Secondary,,0,0,0,,So today there are broadly two major types of large language models.
Dialogue: 1,0:03:04.82,0:03:10.72,Secondary,,0,0,0,,The first is a base LLM and the second which is what is increasingly used is the instruction
Dialogue: 1,0:03:10.72,0:03:12.52,Secondary,,0,0,0,,Tuned LLM.
Dialogue: 1,0:03:12.52,0:03:18.20,Secondary,,0,0,0,,So the base LLM repeatedly predicts the next word based on text training data.
Dialogue: 1,0:03:18.20,0:03:22.72,Secondary,,0,0,0,,And so if I give it a prompt "once upon a time there was a unicorn" then it may by repeatedly
Dialogue: 1,0:03:22.72,0:03:27.20,Secondary,,0,0,0,,predicting one word at a time come up with a completion that tells a story about a unicorn
Dialogue: 1,0:03:27.20,0:03:30.48,Secondary,,0,0,0,,living in a magical forest with all unicorn friends.
Dialogue: 1,0:03:30.48,0:03:35.60,Secondary,,0,0,0,,Now a downside of this is that if you were to prompt it with "what is the capital of
Dialogue: 1,0:03:35.60,0:03:36.60,Secondary,,0,0,0,,France?"
Dialogue: 1,0:03:36.60,0:03:40.60,Secondary,,0,0,0,,quite plausible that on the internet there might be a list of quiz questions about France.
Dialogue: 1,0:03:40.60,0:03:42.92,Secondary,,0,0,0,,So it may complete this with "what is France's largest city?"
Dialogue: 1,0:03:42.92,0:03:46.40,Secondary,,0,0,0,,"what is France's population?" and so on.
Dialogue: 1,0:03:46.40,0:03:50.88,Secondary,,0,0,0,,But what you really want is you want it to tell you what is the capital of France probably
Dialogue: 1,0:03:50.88,0:03:54.36,Secondary,,0,0,0,,rather than list all these questions.
Dialogue: 1,0:03:54.36,0:03:58.80,Secondary,,0,0,0,,So the Instruction Tuned LLM instead tries to follow instructions and will hopefully
Dialogue: 1,0:03:58.80,0:04:02.16,Secondary,,0,0,0,,say the capital of France is Paris.
Dialogue: 1,0:04:02.16,0:04:06.12,Secondary,,0,0,0,,How do you go from a base LLM to an Instruction Tuned LLM?
Dialogue: 1,0:04:06.12,0:04:11.76,Secondary,,0,0,0,,This is what the process of training an Instruction Tuned LLM like ChatGPT looks like.
Dialogue: 1,0:04:11.76,0:04:16.8,Secondary,,0,0,0,,You first train a base LLM on a lot of data so hundreds of billions of words maybe even
Dialogue: 1,0:04:16.8,0:04:22.8,Secondary,,0,0,0,,more and this is a process that can take months on a large supercomputing system.
Dialogue: 1,0:04:22.8,0:04:28.48,Secondary,,0,0,0,,After you've trained the base LLM you would then further train the model by fine tuning
Dialogue: 1,0:04:28.48,0:04:35.82,Secondary,,0,0,0,,it on a smaller set of examples where the output follows an input instruction.
Dialogue: 1,0:04:35.82,0:04:42.60,Secondary,,0,0,0,,And so for example you may have contractors help you write a lot of examples of an instruction
Dialogue: 1,0:04:42.60,0:04:48.4,Secondary,,0,0,0,,and then a good response to an instruction and that creates a training set to carry out
Dialogue: 1,0:04:48.4,0:04:51.92,Secondary,,0,0,0,,this additional fine tuning so that it learns to predict what is the next word if it's trying
Dialogue: 1,0:04:51.92,0:04:55.8,Secondary,,0,0,0,,to follow an instruction.
Dialogue: 1,0:04:55.8,0:05:01.52,Secondary,,0,0,0,,After that to improve the quality of the LLM's output a common process now is to obtain human
Dialogue: 1,0:05:01.52,0:05:07.52,Secondary,,0,0,0,,ratings of the quality of many different LLM outputs on criteria such as whether the output
Dialogue: 1,0:05:07.52,0:05:14.12,Secondary,,0,0,0,,is helpful, honest, and harmless and you can then further tune the LLM to increase the
Dialogue: 1,0:05:14.12,0:05:18.0,Secondary,,0,0,0,,probability of its generating the more highly rated outputs.
Dialogue: 1,0:05:18.0,0:05:22.8,Secondary,,0,0,0,,And the most common technique to do this is RLHF which stands for reinforcement learning
Dialogue: 1,0:05:22.8,0:05:24.60,Secondary,,0,0,0,,from human feedback.
Dialogue: 1,0:05:24.60,0:05:29.80,Secondary,,0,0,0,,And whereas training the base LLM can take months the process of going from the base
Dialogue: 1,0:05:29.80,0:05:36.68,Secondary,,0,0,0,,LLM to the instruction tune LLM can be done in maybe days on a much more modest size data
Dialogue: 1,0:05:36.68,0:05:40.36,Secondary,,0,0,0,,set and much more modest size computational resources.
Dialogue: 1,0:05:40.36,0:05:42.56,Secondary,,0,0,0,,So this is how you would use an LLM.
Dialogue: 1,0:05:42.56,0:05:45.56,Secondary,,0,0,0,,I'm going to import a few libraries.
Dialogue: 1,0:05:45.56,0:05:48.84,Secondary,,0,0,0,,I'm going to load my OpenAI key here.
Dialogue: 1,0:05:48.84,0:05:52.62,Secondary,,0,0,0,,I'll say a little bit more about this later in this video.
Dialogue: 1,0:05:52.62,0:05:57.58,Secondary,,0,0,0,,And here's a helper function to get a completion given a prompt.
Dialogue: 1,0:05:57.58,0:06:04.64,Secondary,,0,0,0,,If you have not yet installed the OpenAI package on your computer you might have to run pip
Dialogue: 1,0:06:04.64,0:06:09.96,Secondary,,0,0,0,,install OpenAI but I already have it installed here so I won't run that.
Dialogue: 1,0:06:09.96,0:06:20.28,Secondary,,0,0,0,,And let me hit shift enter to run these and now I can set response equals get completion
Dialogue: 1,0:06:20.28,0:06:34.82,Secondary,,0,0,0,,"what is the capital of France?" and hopefully it will give me a good result.
Dialogue: 1,0:06:34.82,0:06:42.62,Secondary,,0,0,0,,Now about now in the description of the large language model so far I talked about it as
Dialogue: 1,0:06:42.62,0:06:47.94,Secondary,,0,0,0,,predicting one word at a time but there's actually one more important technical detail.
Dialogue: 1,0:06:47.94,0:06:54.98,Secondary,,0,0,0,,If you were to tell it "Take the letters in the word lollipop and reverse them".
Dialogue: 1,0:06:54.98,0:07:00.38,Secondary,,0,0,0,,This seems like an easy task maybe like a four year old could do this task but if you
Dialogue: 1,0:07:00.38,0:07:09.12,Secondary,,0,0,0,,were to ask ChatGPT to do this it actually outputs a somewhat garbled whatever this is.
Dialogue: 1,0:07:09.12,0:07:14.10,Secondary,,0,0,0,,This is not L-O-L-L-I-P-O-P this is not lollipop's letters reversed.
Dialogue: 1,0:07:14.10,0:07:19.62,Secondary,,0,0,0,,So why is ChatGPT unable to do what seems like a relatively simple task.
Dialogue: 1,0:07:19.62,0:07:24.30,Secondary,,0,0,0,,It turns out that there's one more important detail for how a large language model works
Dialogue: 1,0:07:24.30,0:07:30.22,Secondary,,0,0,0,,which is it doesn't actually repeatedly predict the next word it instead repeatedly predicts
Dialogue: 1,0:07:30.22,0:07:36.72,Secondary,,0,0,0,,the next token and what an LLM actually does is it will take a sequence of characters like
Dialogue: 1,0:07:36.72,0:07:43.26,Secondary,,0,0,0,,"learning new things is fun" and group the characters together to form tokens that comprise commonly
Dialogue: 1,0:07:43.26,0:07:46.58,Secondary,,0,0,0,,occurring sequences of characters.
Dialogue: 1,0:07:46.58,0:07:52.66,Secondary,,0,0,0,,So here "learning new things is fun" each of them is a fairly common word and so each token
Dialogue: 1,0:07:52.66,0:07:57.98,Secondary,,0,0,0,,corresponds to one word or one word in a space or an exclamation mark.
Dialogue: 1,0:07:57.98,0:08:03.54,Secondary,,0,0,0,,But if you were to give it input with some somewhat less frequently used words like "Prompting
Dialogue: 1,0:08:03.54,0:08:10.34,Secondary,,0,0,0,,is powerful developer tool." the word "Prompting" is still not that common in the English language
Dialogue: 1,0:08:10.34,0:08:14.90,Secondary,,0,0,0,,but certainly gaining in popularity and so "Prompting" is actually broken down to three
Dialogue: 1,0:08:14.90,0:08:21.34,Secondary,,0,0,0,,tokens with prompt, pt and ing because those three are commonly occurring sequences of
Dialogue: 1,0:08:21.34,0:08:27.22,Secondary,,0,0,0,,letters and if you were to give it the word lollipop the tokenIsa actually breaks this
Dialogue: 1,0:08:27.22,0:08:34.30,Secondary,,0,0,0,,down into three tokens "l" and "o" and "ipop" and because ChatGPT isn't seeing the individual
Dialogue: 1,0:08:34.30,0:08:40.20,Secondary,,0,0,0,,letters is instead seeing these three tokens it's more difficult for it to correctly print
Dialogue: 1,0:08:40.20,0:08:43.42,Secondary,,0,0,0,,out these letters in reverse order.
Dialogue: 1,0:08:43.42,0:08:52.54,Secondary,,0,0,0,,So here's a trick you can use to fix this if I were to add dashes between these letters
Dialogue: 1,0:08:52.54,0:08:56.78,Secondary,,0,0,0,,and spaces would work too or other things would work too and take the letters and lollipop
Dialogue: 1,0:08:56.78,0:09:03.10,Secondary,,0,0,0,,and reverse them then it actually does a much better job this LOLLIPOP and the reason for
Dialogue: 1,0:09:03.10,0:09:08.74,Secondary,,0,0,0,,that is if you pass it lollipop with dashes in between the letters it tokenizes each of
Dialogue: 1,0:09:08.74,0:09:14.14,Secondary,,0,0,0,,these characters into an individual token making it easier for it to see the individual
Dialogue: 1,0:09:14.14,0:09:17.14,Secondary,,0,0,0,,letters and print them out in reverse order.
Dialogue: 1,0:09:17.14,0:09:23.18,Secondary,,0,0,0,,So if you ever want to use ChatGPT to play a word game like wordle or scrap or something
Dialogue: 1,0:09:23.18,0:09:29.46,Secondary,,0,0,0,,this nifty trick helps it to better see the individual letters of the words.
Dialogue: 1,0:09:29.46,0:09:34.70,Secondary,,0,0,0,,For the English language one token roughly on average corresponds to about four characters
Dialogue: 1,0:09:34.70,0:09:40.98,Secondary,,0,0,0,,or about three quarters of a word and so different large language models will often have different
Dialogue: 1,0:09:40.98,0:09:45.78,Secondary,,0,0,0,,limits on the number of input plus output tokens it can accept.
Dialogue: 1,0:09:45.78,0:09:50.70,Secondary,,0,0,0,,The input is often called the context and the output is often called the completion
Dialogue: 1,0:09:50.70,0:09:56.70,Secondary,,0,0,0,,and the model GPT 3.5 turbo for example the most commonly used ChatGPT model has a limit
Dialogue: 1,0:09:56.70,0:10:01.70,Secondary,,0,0,0,,of roughly 4000 tokens in the input plus output.
Dialogue: 1,0:10:01.70,0:10:05.66,Secondary,,0,0,0,,So if you try to feed it an input context that's much longer than this so actually throw
Dialogue: 1,0:10:05.66,0:10:08.54,Secondary,,0,0,0,,an exception or generate an error.
Dialogue: 1,0:10:08.54,0:10:17.46,Secondary,,0,0,0,,Next I want to share with you another powerful way to use an LLM API which involves specifying
Dialogue: 1,0:10:17.46,0:10:22.58,Secondary,,0,0,0,,separate system user and assistant messages.
Dialogue: 1,0:10:22.58,0:10:29.90,Secondary,,0,0,0,,Let me show you an example then we can explain in more detail what it's actually doing.
Dialogue: 1,0:10:29.90,0:10:35.66,Secondary,,0,0,0,,Here's a new helper function called get completion from messages and when we prompt this LLM
Dialogue: 1,0:10:35.66,0:10:39.46,Secondary,,0,0,0,,we are going to give it multiple messages.
Dialogue: 1,0:10:39.46,0:10:42.50,Secondary,,0,0,0,,Here's an example of what you can do.
Dialogue: 1,0:10:42.50,0:10:49.86,Secondary,,0,0,0,,I'm going to specify first a message in the role of a system so this assistant message
Dialogue: 1,0:10:49.86,0:10:54.50,Secondary,,0,0,0,,and the content of the system message is you're an assistant who responds in the style of
Dialogue: 1,0:10:54.50,0:10:55.50,Secondary,,0,0,0,,Dr. Seuss.
Dialogue: 1,0:10:56.50,0:11:00.70,Secondary,,0,0,0,,Then I'm going to specify a user message so the role of the second message is role
Dialogue: 1,0:11:00.70,0:11:07.18,Secondary,,0,0,0,,user and the content of this is write me a very short poem about a happy carrot.
Dialogue: 1,0:11:07.18,0:11:12.34,Secondary,,0,0,0,,And so let's run that and with temperature equals one I actually never know what's going
Dialogue: 1,0:11:12.34,0:11:14.74,Secondary,,0,0,0,,to come out but okay that's a cool poem.
Dialogue: 1,0:11:14.74,0:11:19.58,Secondary,,0,0,0,,"Oh how jolly is this carrot that I see" and it actually runs pretty well.
Dialogue: 1,0:11:19.58,0:11:22.42,Secondary,,0,0,0,,All right well done ChatGPT.
Dialogue: 1,0:11:22.42,0:11:30.38,Secondary,,0,0,0,,And so in this example the system message specifies the overall tone of what you want
Dialogue: 1,0:11:30.38,0:11:35.50,Secondary,,0,0,0,,the large language model to do and the user message is a specific instruction that you
Dialogue: 1,0:11:35.50,0:11:40.90,Secondary,,0,0,0,,wanted to carry out given this higher level behavior that was specified in the system
Dialogue: 1,0:11:40.90,0:11:41.90,Secondary,,0,0,0,,message.
Dialogue: 1,0:11:41.90,0:11:46.12,Secondary,,0,0,0,,Here's an illustration of how it all works.
Dialogue: 1,0:11:46.12,0:11:51.70,Secondary,,0,0,0,,So this is how the chat format works.
Dialogue: 1,0:11:51.70,0:11:57.10,Secondary,,0,0,0,,The system message sets the overall tone of behavior of the large language model or the
Dialogue: 1,0:11:57.10,0:12:03.14,Secondary,,0,0,0,,assistant and then when you give the user message such as maybe such as a tell me a
Dialogue: 1,0:12:03.14,0:12:10.86,Secondary,,0,0,0,,joke or write me a poem it will then output an appropriate response following what you
Dialogue: 1,0:12:10.86,0:12:16.54,Secondary,,0,0,0,,asked for in the user message and consistent with the overall behavior set in the system
Dialogue: 1,0:12:16.54,0:12:19.22,Secondary,,0,0,0,,message.
Dialogue: 1,0:12:19.22,0:12:24.50,Secondary,,0,0,0,,And by the way although I'm not illustrating it here if you want to use this in a multi-term
Dialogue: 1,0:12:24.50,0:12:33.14,Secondary,,0,0,0,,conversation you can also input assistant messages in this messages format to let ChatGPT
Dialogue: 1,0:12:33.14,0:12:38.86,Secondary,,0,0,0,,know what it had previously said if you wanted to continue the conversation based on things
Dialogue: 1,0:12:38.86,0:12:41.82,Secondary,,0,0,0,,that had previously said as well.
Dialogue: 1,0:12:41.82,0:12:45.18,Secondary,,0,0,0,,But here are a few more examples.
Dialogue: 1,0:12:45.18,0:12:52.34,Secondary,,0,0,0,,If you want to set the tone to tell it to have a one sentence long output then in the
Dialogue: 1,0:12:52.34,0:12:58.46,Secondary,,0,0,0,,system message I can say all your responses must be one sentence long and when I execute
Dialogue: 1,0:12:58.46,0:13:03.54,Secondary,,0,0,0,,this it outputs a single sentence is no longer a poem not in the style of Dr. Seuss but this
Dialogue: 1,0:13:03.54,0:13:10.6,Secondary,,0,0,0,,is a single sentence that's a story about the happy carrot.
Dialogue: 1,0:13:10.6,0:13:17.62,Secondary,,0,0,0,,And if we want to combine both specify the style and the length then I can use the system
Dialogue: 1,0:13:17.62,0:13:22.2,Secondary,,0,0,0,,message to say in the system response to style Dr. Seuss all your sentences must be one sentence
Dialogue: 1,0:13:22.2,0:13:32.86,Secondary,,0,0,0,,long and now this generates a nice one sentence poem that was always smiling and never scary.
Dialogue: 1,0:13:32.86,0:13:35.74,Secondary,,0,0,0,,I like that that's a very happy poem.
Dialogue: 1,0:13:35.74,0:13:42.78,Secondary,,0,0,0,,And then lastly just for fun if you are using an LLM and you want to know how many tokens
Dialogue: 1,0:13:42.78,0:13:49.2,Secondary,,0,0,0,,are you using here's a helper function that is a little bit more sophisticated in that
Dialogue: 1,0:13:49.2,0:13:57.50,Secondary,,0,0,0,,it gets a response from the OpenAI API endpoint and then it uses other values in the response
Dialogue: 1,0:13:57.50,0:14:03.26,Secondary,,0,0,0,,to tell you how many prompt tokens completion tokens and total tokens were used in your
Dialogue: 1,0:14:03.26,0:14:05.6,Secondary,,0,0,0,,API call.
Dialogue: 1,0:14:05.6,0:14:19.26,Secondary,,0,0,0,,Let me define that and if I run this now here's the response and here is a count of how many
Dialogue: 1,0:14:19.26,0:14:20.62,Secondary,,0,0,0,,tokens we use.
Dialogue: 1,0:14:20.62,0:14:27.98,Secondary,,0,0,0,,So this output which had 55 tokens whereas the prompt input had 37 tokens so this used
Dialogue: 1,0:14:27.98,0:14:31.74,Secondary,,0,0,0,,up 92 tokens altogether.
Dialogue: 1,0:14:31.74,0:14:36.26,Secondary,,0,0,0,,When I'm using LLM models in practice I don't worry that much frankly about the number of
Dialogue: 1,0:14:36.26,0:14:38.74,Secondary,,0,0,0,,tokens I'm using.
Dialogue: 1,0:14:38.74,0:14:42.42,Secondary,,0,0,0,,Maybe one case where it might be worth checking the number of tokens is if you're worried
Dialogue: 1,0:14:42.42,0:14:48.10,Secondary,,0,0,0,,that the user might have given you too long an input that exceeds the 4000 or so token
Dialogue: 1,0:14:48.10,0:14:52.94,Secondary,,0,0,0,,limits of ChatGPT in which case you could double check how many tokens it was and truncate
Dialogue: 1,0:14:52.94,0:14:58.38,Secondary,,0,0,0,,it to make sure you're staying within the input token limits of the large language model.
Dialogue: 1,0:14:58.38,0:15:04.78,Secondary,,0,0,0,,Now I want to share with you one more tip for how to use a large language model.
Dialogue: 1,0:15:04.78,0:15:10.90,Secondary,,0,0,0,,Calling the OpenAI API requires using an API key that's tied to either a free or a
Dialogue: 1,0:15:10.90,0:15:17.94,Secondary,,0,0,0,,paid account and so many developers will write the API key in plain text like this into their
Dialogue: 1,0:15:17.94,0:15:26.46,Secondary,,0,0,0,,Jupyter notebook and this is a less secure way of using API keys that I would not recommend
Dialogue: 1,0:15:26.46,0:15:32.58,Secondary,,0,0,0,,you use because it's just too easy to share this notebook with someone else or check this
Dialogue: 1,0:15:32.58,0:15:38.70,Secondary,,0,0,0,,into GitHub or something and thus end up leaking your API key to someone else.
Dialogue: 1,0:15:38.70,0:15:46.14,Secondary,,0,0,0,,In contrast what you saw me do in the Jupyter notebook was this piece of code where I use
Dialogue: 1,0:15:46.14,0:15:52.90,Secondary,,0,0,0,,a library dotenv and then run this command load dotenv find dotenv to read a local
Dialogue: 1,0:15:52.90,0:15:58.66,Secondary,,0,0,0,,file which is called dotenv that contains my secret key.
Dialogue: 1,0:15:58.66,0:16:04.42,Secondary,,0,0,0,,And so with this code snippet I have locally stored a file called dotenv that contains
Dialogue: 1,0:16:04.42,0:16:13.14,Secondary,,0,0,0,,my API key and this loads it into the operating systems environmental variable and then os.get
Dialogue: 1,0:16:13.14,0:16:21.18,Secondary,,0,0,0,,env, OpenAI API key stores it into this variable and in this whole process I don't ever have
Dialogue: 1,0:16:21.18,0:16:27.2,Secondary,,0,0,0,,to enter the API key in plain text in unencrypted plain text into my Jupyter notebook.
Dialogue: 1,0:16:27.2,0:16:33.38,Secondary,,0,0,0,,So this is a relatively more secure and a better way to access the API key and in fact
Dialogue: 1,0:16:33.38,0:16:39.42,Secondary,,0,0,0,,this is a general method for storing different API keys from lots of different online services
Dialogue: 1,0:16:39.42,0:16:44.18,Secondary,,0,0,0,,that you might want to use and call from your Jupyter notebook.
Dialogue: 1,0:16:44.18,0:16:51.58,Secondary,,0,0,0,,The- I think the degree to which prompting is revolutionizing AI application development
Dialogue: 1,0:16:51.58,0:16:54.22,Secondary,,0,0,0,,is still underappreciated.
Dialogue: 1,0:16:54.22,0:16:58.90,Secondary,,0,0,0,,In the traditional supervised machine learning workflow like the restaurant review sentiment
Dialogue: 1,0:16:58.90,0:17:03.34,Secondary,,0,0,0,,classification example that I touched on just now, if you want to build a classifier to
Dialogue: 1,0:17:03.34,0:17:07.90,Secondary,,0,0,0,,classify restaurant review positive or negative sentiments you at first get a bunch of label
Dialogue: 1,0:17:07.90,0:17:13.20,Secondary,,0,0,0,,data maybe hundreds of examples this might take I don't know weeks maybe a month then
Dialogue: 1,0:17:13.20,0:17:18.62,Secondary,,0,0,0,,you would train a model on data and getting an appropriate open source model tuning on
Dialogue: 1,0:17:18.62,0:17:25.54,Secondary,,0,0,0,,the model evaluating it that might take days weeks maybe even a few months and then you
Dialogue: 1,0:17:25.54,0:17:31.14,Secondary,,0,0,0,,might have to find a cloud service to deploy it and then get your model uploaded to the
Dialogue: 1,0:17:31.14,0:17:34.94,Secondary,,0,0,0,,cloud and then run the model and finally be able to call your model and again not
Dialogue: 1,0:17:34.94,0:17:39.82,Secondary,,0,0,0,,uncommon for this to take a team a few months to get working.
Dialogue: 1,0:17:39.82,0:17:46.90,Secondary,,0,0,0,,In contrast with prompting based machine learning when you have a text application you can specify
Dialogue: 1,0:17:46.90,0:17:51.78,Secondary,,0,0,0,,a prompt this can take minutes maybe hours if you need to iterate a few times to get
Dialogue: 1,0:17:51.78,0:17:59.88,Secondary,,0,0,0,,an effective prompt and then in hours maybe at most days but frankly more often hours
Dialogue: 1,0:17:59.88,0:18:05.88,Secondary,,0,0,0,,you can have this running using API calls and start making calls to the model and once
Dialogue: 1,0:18:05.88,0:18:11.48,Secondary,,0,0,0,,you've done that in just again maybe minutes or hours you can start calling the model and
Dialogue: 1,0:18:11.48,0:18:17.28,Secondary,,0,0,0,,start making inferences and so there are applications that used to take me maybe six months or a
Dialogue: 1,0:18:17.28,0:18:22.84,Secondary,,0,0,0,,year to build that you can now build in minutes or hours maybe very small numbers of days
Dialogue: 1,0:18:22.84,0:18:29.12,Secondary,,0,0,0,,using prompting and this is revolutionizing what AI applications can be built quickly.
Dialogue: 1,0:18:29.12,0:18:34.68,Secondary,,0,0,0,,One important caveat this applies to many unstructured data applications including specifically
Dialogue: 1,0:18:34.68,0:18:40.60,Secondary,,0,0,0,,text applications and maybe increasingly vision applications although the vision technology
Dialogue: 1,0:18:40.60,0:18:44.52,Secondary,,0,0,0,,is much less mature right now but it's kind of getting there.
Dialogue: 1,0:18:44.52,0:18:49.52,Secondary,,0,0,0,,This recipe doesn't really work for structured data applications meaning machine learning
Dialogue: 1,0:18:49.52,0:18:54.84,Secondary,,0,0,0,,applications on tabular data with lots of numerical values in the Excel spreadsheet
Dialogue: 1,0:18:54.84,0:19:01.12,Secondary,,0,0,0,,say but for applications to which this does apply the fact that AI components can be built
Dialogue: 1,0:19:01.12,0:19:07.48,Secondary,,0,0,0,,so quickly is changing the workflow of how the entire system might be built.
Dialogue: 1,0:19:07.48,0:19:11.8,Secondary,,0,0,0,,Building entire system might still take days or weeks or something but at least this piece
Dialogue: 1,0:19:11.8,0:19:16.88,Secondary,,0,0,0,,of it can be done much faster and so with that let's go on to the next video where Isa
Dialogue: 1,0:19:16.88,0:19:24.40,Secondary,,0,0,0,,will show how to use these components to evaluate the input to a customer service assistant
Dialogue: 1,0:19:24.40,0:19:29.4,Secondary,,0,0,0,,and this will be part of a bigger example that you see developed through this course
Dialogue: 1,0:19:29.4,0:19:33.76,Secondary,,0,0,0,,for building a customer service assistant for an online retailer.
Dialogue: 1,0:05:00.0,0:00:11.8,Default,,0,0,0,,在这第一个视频中，我想和大家\N分享一下大型语言模型（LLM）的\N概述，它们是如何工作的。
Dialogue: 1,0:00:11.8,0:00:16.48,Default,,0,0,0,,我们将深入了解它们的训练过\N程，以及诸如分词器之类的细节\N，以及这些细节如何影响
Dialogue: 1,0:00:16.48,0:00:20.8,Default,,0,0,0,,在提示LLM时的输出结果。
Dialogue: 1,0:00:20.8,0:00:25.80,Default,,0,0,0,,我们还将了解LLM的聊天格式，这\N是一种既可以指定系统消息，
Dialogue: 1,0:00:25.80,0:00:31.36,Default,,0,0,0,,也可以指定用户消息的方法，了解\N您可以利用这种功能做什么。
Dialogue: 1,0:00:31.36,0:00:32.68,Default,,0,0,0,,让我们来看看。
Dialogue: 1,0:00:32.68,0:00:36.28,Default,,0,0,0,,首先，大型语言模型是如何工作的？
Dialogue: 1,0:00:36.28,0:00:41.8,Default,,0,0,0,,您可能已经熟悉了文本生成过程，您可\N以给出一个提示，比如“我喜欢吃”，
Dialogue: 1,0:00:41.8,0:00:47.36,Default,,0,0,0,,然后让LLM填充在这个提\N示下可能的补全内容。
Dialogue: 1,0:00:47.36,0:00:52.56,Default,,0,0,0,,它可能会说“百吉饼加奶油奶酪”或“我妈\N妈做的肉饼”或“和朋友一起在外面（吃）”。
Dialogue: 1,0:00:52.56,0:00:55.32,Default,,0,0,0,,但是模型是如何学会这个的呢？
Dialogue: 1,0:00:55.32,0:01:00.80,Default,,0,0,0,,实际上，训练LLM的主要工具是监督学习。
Dialogue: 1,0:01:00.80,0:01:07.32,Default,,0,0,0,,在监督学习中，计算机使用带标签的训\N练数据学习输入/输出或X/Y映射。
Dialogue: 1,0:01:07.32,0:01:12.48,Default,,0,0,0,,所以，例如，如果你使用监督\N学习来学习对餐厅评价
Dialogue: 1,0:01:12.48,0:01:17.0,Default,,0,0,0,,进行好评/差评分类，你可能会\N收集这样的训练集，其中像
Dialogue: 1,0:01:17.0,0:01:23.32,Default,,0,0,0,,“熏牛肉三明治很棒”被标\N记为好评，依此类推。
Dialogue: 1,0:01:23.32,0:01:28.32,Default,,0,0,0,,而“服务慢，食物一般”是差\N评，“伯爵灰茶非常棒”
Dialogue: 1,0:01:28.32,0:01:30.44,Default,,0,0,0,,带有正面标签。
Dialogue: 1,0:01:30.44,0:01:35.96,Default,,0,0,0,,顺便说一下，Isa和我都出生在英国，\N所以我们都喜欢我们的伯爵茶。
Dialogue: 1,0:01:35.96,0:01:41.74,Default,,0,0,0,,因此，监督学习的过程通常是\N获得带标签的数据，然后
Dialogue: 1,0:01:41.74,0:01:47.24,Default,,0,0,0,,在数据上训练一个模型，训练完成\N后，您可以部署并调用该模型，
Dialogue: 1,0:01:47.24,0:01:50.92,Default,,0,0,0,,给它一个新的餐厅评论，比如\N"我吃过的最好的披萨"。
Dialogue: 1,0:01:50.92,0:01:54.32,Default,,0,0,0,,希望您能输出这是一个积极的情感。
Dialogue: 1,0:01:54.32,0:02:00.28,Default,,0,0,0,,事实证明，监督学习是训练大型\N语言的核心构建模块模型。
Dialogue: 1,0:02:00.28,0:02:05.84,Default,,0,0,0,,具体来说，可以通过使用监督学习来反复
Dialogue: 1,0:02:05.84,0:02:08.16,Default,,0,0,0,,预测下一个单词来构建大型语言模型。
Dialogue: 1,0:02:08.16,0:02:14.74,Default,,0,0,0,,假设在您的训练集中有很多文本数\N据，您必须将句子"我最喜欢的
Dialogue: 1,0:02:14.74,0:02:17.48,Default,,0,0,0,,食物是百吉饼配上奶油奶酪和熏鲑鱼"。
Dialogue: 1,0:02:17.48,0:02:23.48,Default,,0,0,0,,那么这个句子就会变成一个训\N练示例序列，给定一个句子
Dialogue: 1,0:02:23.48,0:02:28.80,Default,,0,0,0,,片段"我最喜欢的食物是"，如果你想\N预测下一个单词，这种情况下是
Dialogue: 1,0:02:28.80,0:02:36.52,Default,,0,0,0,,"百吉饼"，或者给定句子片段或句子\N前缀"我最喜欢的食物是百吉饼"，
Dialogue: 1,0:02:36.52,0:02:41.12,Default,,0,0,0,,接下来的单词就是"配上"，依此类推。
Dialogue: 1,0:02:41.12,0:02:45.60,Default,,0,0,0,,而且，给定一个包含数千亿甚\N至更多单词的大型训练集，
Dialogue: 1,0:02:45.60,0:02:51.52,Default,,0,0,0,,你就可以创建一个庞大的训练\N集，从一句话的一部分开始
Dialogue: 1,0:02:51.52,0:02:56.60,Default,,0,0,0,,或者一段文字的一部分，反\N复让语言模型学会预测
Dialogue: 1,0:02:56.60,0:02:58.44,Default,,0,0,0,,下一个单词是什么。
Dialogue: 1,0:02:58.44,0:03:04.82,Default,,0,0,0,,所以今天有两种主要类\N型的大型语言模型。
Dialogue: 1,0:03:04.82,0:03:12.52,Default,,0,0,0,,第一种是基础LLM，第二种是越来\N越多地被使用的指令调优\NLLM（Instruction Tuned LLM）。
Dialogue: 1,0:03:12.52,0:03:18.20,Default,,0,0,0,,所以基础LLM会根据文本训练\N数据反复预测下一个单词。
Dialogue: 1,0:03:18.20,0:03:22.72,Default,,0,0,0,,因此，如果我给它一个提示“从前有一个\N独角兽”，那么它可能会通过反复地
Dialogue: 1,0:03:22.72,0:03:27.20,Default,,0,0,0,,一次预测一个单词，编写一\N个关于独角兽的故事，
Dialogue: 1,0:03:27.20,0:03:30.48,Default,,0,0,0,,讲述独角兽与所有独角兽朋友一\N起生活在一个神奇的森林里。
Dialogue: 1,0:03:30.48,0:03:36.60,Default,,0,0,0,,现在的一个缺点是，如果你给它一\N个提示“法国的首都是什么？”
Dialogue: 1,0:03:36.60,0:03:40.60,Default,,0,0,0,,在互联网上很有可能存在关于\N法国的一系列测验问题。
Dialogue: 1,0:03:40.60,0:03:42.92,Default,,0,0,0,,所以它可能会用“法国最大的城\N市是什么？”来完成这个问题，
Dialogue: 1,0:03:42.92,0:03:46.40,Default,,0,0,0,,“法国的人口是多少？”等等。
Dialogue: 1,0:03:46.40,0:03:50.88,Default,,0,0,0,,但是你真正想要的是让它告\N诉你法国的首都是什么，
Dialogue: 1,0:03:50.88,0:03:54.36,Default,,0,0,0,,而不是列出所有这些问题。
Dialogue: 1,0:03:54.36,0:03:58.80,Default,,0,0,0,,所以，指令调优LLM尝试遵循指\N令，并希望能正确回答出，
Dialogue: 1,0:03:58.80,0:04:02.16,Default,,0,0,0,,法国的首都是巴黎。
Dialogue: 1,0:04:02.16,0:04:06.12,Default,,0,0,0,,如何从基础LLM转变为指令调优LLM？
Dialogue: 1,0:04:06.12,0:04:11.76,Default,,0,0,0,,这就是训练一个类似ChatGPT\N的指令调优LLM的过程。
Dialogue: 1,0:04:11.76,0:04:16.8,Default,,0,0,0,,首先，你需要在大量数据上训练一\N个基础LLM，可能是数千亿甚至
Dialogue: 1,0:04:16.8,0:04:22.8,Default,,0,0,0,,更多的词汇，这个过程可能需要在\N大型超级计算系统上进行数月。
Dialogue: 1,0:04:22.8,0:04:28.48,Default,,0,0,0,,在训练了基础LLM之后，你可以通过\N在一小部分的例子上微调模型
Dialogue: 1,0:04:28.48,0:04:35.82,Default,,0,0,0,,来进一步训练它，这些例子\N的输出遵循输入的指令。
Dialogue: 1,0:04:35.82,0:04:42.60,Default,,0,0,0,,所以，例如，你可以请负责数据标注的承\N包商帮助你编写很多指令的示例，
Dialogue: 1,0:04:42.60,0:04:48.4,Default,,0,0,0,,以及如何对这些指令高质量的回应，这\N样就形成了一套训练集，可以进行
Dialogue: 1,0:04:48.4,0:04:55.8,Default,,0,0,0,,额外的微调，使其在尝试遵循指令的情\N况下，学会预测下一个词是什么。
Dialogue: 1,0:04:55.8,0:05:01.52,Default,,0,0,0,,在此之后，为了提高LLM输出质\N量的常见方法是获得人类
Dialogue: 1,0:05:01.52,0:05:07.52,Default,,0,0,0,,对许多不同LLM输出质量\N的评分，例如输出是否
Dialogue: 1,0:05:07.52,0:05:14.12,Default,,0,0,0,,有帮助、诚实和无害，然后您可\N以进一步调整LLM以提高
Dialogue: 1,0:05:14.12,0:05:18.0,Default,,0,0,0,,生成更高评分输出的概率。
Dialogue: 1,0:05:18.0,0:05:24.60,Default,,0,0,0,,最常用的技术是RLHF，即来\N自人类反馈的强化学习。
Dialogue: 1,0:05:24.60,0:05:29.80,Default,,0,0,0,,而训练基础LLM可能需要几\N个月的时间，从基础LLM
Dialogue: 1,0:05:29.80,0:05:36.68,Default,,0,0,0,,到指令调优LLM的过程可能只\N需要几天时间，数据集规模
Dialogue: 1,0:05:36.68,0:05:40.36,Default,,0,0,0,,和计算资源都要小得多。
Dialogue: 1,0:05:40.36,0:05:42.56,Default,,0,0,0,,所以这就是你如何使用LLM的方法。
Dialogue: 1,0:05:42.56,0:05:45.56,Default,,0,0,0,,我将导入一些库。
Dialogue: 1,0:05:45.56,0:05:48.84,Default,,0,0,0,,我将在这里加载我的OpenAI密钥。
Dialogue: 1,0:05:48.84,0:05:52.62,Default,,0,0,0,,稍后在这个视频中，我会\N更详细地介绍这个。
Dialogue: 1,0:05:52.62,0:05:57.58,Default,,0,0,0,,这里有一个辅助函数，用于根据\N提示获取补全（completion）。
Dialogue: 1,0:05:57.58,0:06:04.64,Default,,0,0,0,,如果您还没有在计算机上安装\NOpenAI软件包，您可能需要运行pip
Dialogue: 1,0:06:04.64,0:06:09.96,Default,,0,0,0,,安装OpenAI，但我已经在这里安\N装了，所以我不会运行那个。
Dialogue: 1,0:06:09.96,0:06:20.28,Default,,0,0,0,,让我按shift enter运行这些，现在我可\N以设置response=get_completion
Dialogue: 1,0:06:20.28,0:06:34.82,Default,,0,0,0,,“法国的首都是什么？”，希望\N它能给我一个好结果。
Dialogue: 1,0:06:34.82,0:06:42.62,Default,,0,0,0,,现在关于大型语言模型的描述\N，到目前为止，我谈论的是
Dialogue: 1,0:06:42.62,0:06:47.94,Default,,0,0,0,,一次预测一个单词，但实际上还\N有一个更重要的技术细节。
Dialogue: 1,0:06:47.94,0:06:54.98,Default,,0,0,0,,如果你让它“把单词\Nlollipop中的字母倒过来”。
Dialogue: 1,0:06:54.98,0:07:00.38,Default,,0,0,0,,这看起来像是一个简单的任务，也许一个\N四岁的孩子就能完成这个任务，但如果你
Dialogue: 1,0:07:00.38,0:07:09.12,Default,,0,0,0,,让ChatGPT去做这个，它实际上输出\N的是一种有点乱七八糟的东西。
Dialogue: 1,0:07:09.12,0:07:14.10,Default,,0,0,0,,这不是L-O-L-L-I-P-O-P，这不\N是lollipop的字母倒过来。
Dialogue: 1,0:07:14.10,0:07:19.62,Default,,0,0,0,,那么为什么ChatGPT无法完成\N看似相对简单的任务呢。
Dialogue: 1,0:07:19.62,0:07:24.30,Default,,0,0,0,,事实证明，大型语言模型的工作\N方式还有一个更重要的细节，
Dialogue: 1,0:07:24.30,0:07:30.22,Default,,0,0,0,,那就是它实际上不是反复预测\N下一个单词，而是反复预测
Dialogue: 1,0:07:30.22,0:07:36.72,Default,,0,0,0,,下一个标记（Token），而一个LLM实际上会\N做的是，它会接收一系列字符，比如
Dialogue: 1,0:07:36.72,0:07:43.26,Default,,0,0,0,,“learning new things is fun”，并\N将字符组合在一起形成代表常见
Dialogue: 1,0:07:43.26,0:07:46.58,Default,,0,0,0,,字符序列的标记。
Dialogue: 1,0:07:46.58,0:07:52.66,Default,,0,0,0,,所以在这里“learning new things is \Nfun”，每个都是相当常见的词，所以每个标记
Dialogue: 1,0:07:52.66,0:07:57.98,Default,,0,0,0,,对应一个词或一个词的空格或感叹号。
Dialogue: 1,0:07:57.98,0:08:03.54,Default,,0,0,0,,但是，如果你给它一些不太常用的\N词作为输入，比如“Prompting
Dialogue: 1,0:08:03.54,0:08:10.34,Default,,0,0,0,,is a powerful developer tool.”\N对于开发者来说，Prompting这个词\N在英语中还不是那么常见
Dialogue: 1,0:08:10.34,0:08:14.90,Default,,0,0,0,,但肯定越来越受欢迎，所以\N“Prompting”实际上被分解成三个
Dialogue: 1,0:08:14.90,0:08:21.34,Default,,0,0,0,,标记，分别是"prompt"、"pt"和"ing"\N，因为这三个是常见的字母序列
Dialogue: 1,0:08:21.34,0:08:27.22,Default,,0,0,0,,如果你给它一个“棒棒糖\N（lollipop）”这个词，分词器实际上\N把这个
Dialogue: 1,0:08:27.22,0:08:34.30,Default,,0,0,0,,分成三个标记"l"、"o"和"ipop"，\N因为ChatGPT没有看到单独的
Dialogue: 1,0:08:34.30,0:08:40.20,Default,,0,0,0,,字母，而是看到了这三个标记，所\N以要正确地按相反的顺序打印
Dialogue: 1,0:08:40.20,0:08:43.42,Default,,0,0,0,,出这些字母就更困难了。
Dialogue: 1,0:08:43.42,0:08:52.54,Default,,0,0,0,,所以这里有一个技巧，如果我在\N这些字母之间加上破折号，
Dialogue: 1,0:08:52.54,0:08:56.78,Default,,0,0,0,,空格也可以，或者其他东西也可以\N，然后把字母和棒棒糖拿过来
Dialogue: 1,0:08:56.78,0:09:03.10,Default,,0,0,0,,反过来，它实际上做得更好，\N这个LOLLIPOP，原因是
Dialogue: 1,0:09:03.10,0:09:08.74,Default,,0,0,0,,如果你用破折号把lollipop的\N字母隔开，它会把每一个
Dialogue: 1,0:09:08.74,0:09:14.14,Default,,0,0,0,,这些字符分成一个个的标记，\N让它更容易看到单独的
Dialogue: 1,0:09:14.14,0:09:17.14,Default,,0,0,0,,字母，然后按相反的顺序打印出来。
Dialogue: 1,0:09:17.14,0:09:23.18,Default,,0,0,0,,所以如果你想用ChatGPT玩像\Nwordle或scrap这样的单词游戏，
Dialogue: 1,0:09:23.18,0:09:29.46,Default,,0,0,0,,这个巧妙的技巧有助于更好\N地看到单词的各个字母。
Dialogue: 1,0:09:29.46,0:09:34.70,Default,,0,0,0,,对于英语，一个标记大致\N平均对应四个字符，
Dialogue: 1,0:09:34.70,0:09:40.98,Default,,0,0,0,,或者大约三分之二个单词，所以不同\N的大型语言模型通常会有不同的
Dialogue: 1,0:09:40.98,0:09:45.78,Default,,0,0,0,,关于它可以接受的输入加\N输出标记的数量限制。
Dialogue: 1,0:09:45.78,0:09:50.70,Default,,0,0,0,,输入通常被称为上下文（context），输\N出通常被称为补全（completion）
Dialogue: 1,0:09:50.70,0:09:56.70,Default,,0,0,0,,例如，最常用的ChatGPT模型GPT-3.5\N Turbo在输入和输出中的限制
Dialogue: 1,0:09:56.70,0:10:01.70,Default,,0,0,0,,大约是4000个标记。
Dialogue: 1,0:10:01.70,0:10:05.66,Default,,0,0,0,,所以，如果你尝试输入一个超过这个\N长度的上下文，它实际上会抛出
Dialogue: 1,0:10:05.66,0:10:08.54,Default,,0,0,0,,一个异常或产生一个错误。
Dialogue: 1,0:10:08.54,0:10:17.46,Default,,0,0,0,,接下来，我想和你分享另一种使用\NLLM API的强大方法，它涉及指定
Dialogue: 1,0:10:17.46,0:10:22.58,Default,,0,0,0,,独立的系统（system）、用户\N（user）和助手（assistant）消息。
Dialogue: 1,0:10:22.58,0:10:29.90,Default,,0,0,0,,让我给你展示一个例子，然后我们可以\N更详细地解释它实际上在做什么。
Dialogue: 1,0:10:29.90,0:10:35.66,Default,,0,0,0,,这里有一个新的辅助函数，叫做从消息\N中获取补全，当我们提示这个LLM时
Dialogue: 1,0:10:35.66,0:10:39.46,Default,,0,0,0,,我们将给它提供多条消息。
Dialogue: 1,0:10:39.46,0:10:42.50,Default,,0,0,0,,这是一个你可以做的例子。
Dialogue: 1,0:10:42.50,0:10:49.86,Default,,0,0,0,,首先，我将以系统的角色指定一\N条信息，以便这个助手信息
Dialogue: 1,0:10:49.86,0:10:55.50,Default,,0,0,0,,系统信息的内容是，你是一个以\N苏斯博士的风格回应的助手。
Dialogue: 1,0:10:56.50,0:11:00.70,Default,,0,0,0,,然后我将指定一个用户信息，所\N以第二条信息的角色是用户，
Dialogue: 1,0:11:00.70,0:11:07.18,Default,,0,0,0,,而这个内容是“给我写一首关于\N快乐胡萝卜的非常短的诗。”
Dialogue: 1,0:11:07.18,0:11:12.34,Default,,0,0,0,,所以让我们运行这个，温度\N（Temperature）参数设置为1，我\N实际上永远不知道会出现什么
Dialogue: 1,0:11:12.34,0:11:14.74,Default,,0,0,0,,但好吧，这是一首很酷的诗。
Dialogue: 1,0:11:14.74,0:11:19.58,Default,,0,0,0,,“哦，我看到的这根胡萝卜是多么快乐\N”，而且它实际上运行得相当好。
Dialogue: 1,0:11:19.58,0:11:22.42,Default,,0,0,0,,好吧，干得好ChatGPT。
Dialogue: 1,0:11:22.42,0:11:30.38,Default,,0,0,0,,所以在这个例子中，系统\N消息指定了你想要的
Dialogue: 1,0:11:30.38,0:11:35.50,Default,,0,0,0,,大语言模型的整体语言风格，而\N用户消息是一个具体的指令，
Dialogue: 1,0:11:35.50,0:11:41.90,Default,,0,0,0,,你希望在系统中执行这个\N更高级别的行为消息。
Dialogue: 1,0:11:41.90,0:11:46.12,Default,,0,0,0,,这里有一个说明它是如何工作的例子。
Dialogue: 1,0:11:46.12,0:11:51.70,Default,,0,0,0,,所以这就是聊天格式的工作原理。
Dialogue: 1,0:11:51.70,0:11:57.10,Default,,0,0,0,,系统消息设定了大型语言模型或
Dialogue: 1,0:11:57.10,0:12:03.14,Default,,0,0,0,,助手的整体行为风格，然后当你\N给出用户消息，比如说“给我
Dialogue: 1,0:12:03.14,0:12:10.86,Default,,0,0,0,,讲个笑话或者写首诗”，它会根据\N你在用户消息中要求的内容，
Dialogue: 1,0:12:10.86,0:12:19.22,Default,,0,0,0,,以及在系统消息中设定的整体行\N为风格，输出一个合适的回应。
Dialogue: 1,0:12:19.22,0:12:24.50,Default,,0,0,0,,顺便说一下，虽然我没有在这里展\N示，但如果你想在多轮对话中
Dialogue: 1,0:12:24.50,0:12:33.14,Default,,0,0,0,,使用这个功能，你也可以用这种消息\N格式输入助手消息，让ChatGPT
Dialogue: 1,0:12:33.14,0:12:41.82,Default,,0,0,0,,了解它之前说过什么，如果你想根\N据之前的对话继续对话的话。
Dialogue: 1,0:12:41.82,0:12:45.18,Default,,0,0,0,,但是这里有更多的例子。
Dialogue: 1,0:12:45.18,0:12:52.34,Default,,0,0,0,,如果你想设置语言风格，告\N诉它输出一句话，那么在
Dialogue: 1,0:12:52.34,0:12:58.46,Default,,0,0,0,,系统消息中，我可以说你的所有回\N答都必须是一句话长，当我执行
Dialogue: 1,0:12:58.46,0:13:03.54,Default,,0,0,0,,这个操作时，它输出的一句话不再是诗\N歌，也不是苏斯博士的风格，但这
Dialogue: 1,0:13:03.54,0:13:10.6,Default,,0,0,0,,是一句关于快乐胡萝卜的故事。
Dialogue: 1,0:13:10.6,0:13:17.62,Default,,0,0,0,,如果我们想同时指定风格和长度\N，那么我可以使用系统消息，
Dialogue: 1,0:13:17.62,0:13:22.2,Default,,0,0,0,,在系统回应中说，以苏斯博士的风格，\N你的所有句子都必须是一句话长，
Dialogue: 1,0:13:22.2,0:13:32.86,Default,,0,0,0,,现在这生成了一个很好的一句话\N诗歌，总是微笑，从不可怕。
Dialogue: 1,0:13:32.86,0:13:35.74,Default,,0,0,0,,我喜欢这是一首非常快乐的诗。
Dialogue: 1,0:13:35.74,0:13:42.78,Default,,0,0,0,,然后最后，只是为了好玩，如果你在\N使用LLM，想知道有多少个tokens
Dialogue: 1,0:13:42.78,0:13:49.2,Default,,0,0,0,,你在这里使用了一个稍微\N复杂一点的辅助函数。
Dialogue: 1,0:13:49.2,0:13:57.50,Default,,0,0,0,,它从OpenAI API获取响应，然\N后使用响应中的其他值
Dialogue: 1,0:13:57.50,0:14:05.6,Default,,0,0,0,,告诉你在你的API调用中使用了多\N少“prompt tokens”、“completion \Ntokens”和“total tokens”。
Dialogue: 1,0:14:05.6,0:14:19.26,Default,,0,0,0,,让我定义一下，如果我现在运行这个，这是\N响应，这是一个关于我们使用了多少个
Dialogue: 1,0:14:19.26,0:14:20.62,Default,,0,0,0,,tokens的计数。
Dialogue: 1,0:14:20.62,0:14:27.98,Default,,0,0,0,,所以这个输出有55个tokens，而提\N示输入有37个tokens，所以这个
Dialogue: 1,0:14:27.98,0:14:31.74,Default,,0,0,0,,一共使用了92个tokens。
Dialogue: 1,0:14:31.74,0:14:38.74,Default,,0,0,0,,当我在实践中使用LLM模型时，我实际上\N并不太担心我使用的tokens数量。
Dialogue: 1,0:14:38.74,0:14:42.42,Default,,0,0,0,,也许在一个情况下，检查tokens数\N量是值得的，那就是如果你担心
Dialogue: 1,0:14:42.42,0:14:48.10,Default,,0,0,0,,用户可能给你太长的输入，超\N过了4000个左右的tokens
Dialogue: 1,0:14:48.10,0:14:52.94,Default,,0,0,0,,ChatGPT的限制，在这种情况下，你可以\N仔细检查一下tokens数量并截断
Dialogue: 1,0:14:52.94,0:14:58.38,Default,,0,0,0,,它以确保你在大型语言模型的\N输入tokens限制范围内。
Dialogue: 1,0:14:58.38,0:15:04.78,Default,,0,0,0,,现在我想和你分享如何使用大\N型语言模型的另一个技巧。
Dialogue: 1,0:15:04.78,0:15:10.90,Default,,0,0,0,,调用OpenAI API需要使用与免费\N或付费账户绑定的API密钥，
Dialogue: 1,0:15:10.90,0:15:17.94,Default,,0,0,0,,因此许多开发者会像这样将API密\N钥以纯文本形式写入他们的
Dialogue: 1,0:15:17.94,0:15:26.46,Default,,0,0,0,,Jupyter Notebook，这是一种使用API\N密钥的不太安全的方式，我不建议
Dialogue: 1,0:15:26.46,0:15:32.58,Default,,0,0,0,,你使用，因为这样太容易与他人共\N享这个Notebook或将其检查到
Dialogue: 1,0:15:32.58,0:15:38.70,Default,,0,0,0,,GitHub或其他地方，从而泄露\N你的API密钥给其他人。
Dialogue: 1,0:15:38.70,0:15:46.14,Default,,0,0,0,,相比之下，你在 Jupyter Notebook中\N看到我做的是这段代码，我使用
Dialogue: 1,0:15:46.14,0:15:52.90,Default,,0,0,0,,一个库 dotenv，然后运行这个命令 load \Ndotenv find dotenv 来读取一个本地
Dialogue: 1,0:15:52.90,0:15:58.66,Default,,0,0,0,,文件，叫做 .env，里面包含了我的密钥。
Dialogue: 1,0:15:58.66,0:16:04.42,Default,,0,0,0,,所以通过这段代码片段，我在本地存储\N了一个名为 .env 的文件，其中包含
Dialogue: 1,0:16:04.42,0:16:13.14,Default,,0,0,0,,我的 API 密钥，然后将其加载到操作系\N统的环境变量中，然后 os.getenv，
Dialogue: 1,0:16:13.14,0:16:21.18,Default,,0,0,0,,OPENAI_API_KEY将其存储到这个变\N量中，在整个过程中，我不需要将
Dialogue: 1,0:16:21.18,0:16:27.2,Default,,0,0,0,,API 密钥以明文形式输入到我\N的 Jupyter Notebook中。
Dialogue: 1,0:16:27.2,0:16:33.38,Default,,0,0,0,,所以这是一个相对更安全、更好的\N访问 API 密钥的方法，实际上
Dialogue: 1,0:16:33.38,0:16:39.42,Default,,0,0,0,,这是一种用于存储来自许多不同在线\N服务的不同 API 密钥的通用方法
Dialogue: 1,0:16:39.42,0:16:44.18,Default,,0,0,0,,你可能想要使用并从你的 \NJupyter Notebook中调用。
Dialogue: 1,0:16:44.18,0:16:54.22,Default,,0,0,0,,我认为，提示（prompting）正在革新AI\N应用开发的程度，仍然被低估了。
Dialogue: 1,0:16:54.22,0:16:58.90,Default,,0,0,0,,在传统的监督式机器学习工作流中，就像\N我刚才提到的餐厅评论好评/差评分类
Dialogue: 1,0:16:58.90,0:17:03.34,Default,,0,0,0,,的例子，如果你想建立一个分类器来
Dialogue: 1,0:17:03.34,0:17:07.90,Default,,0,0,0,,分类餐厅评论的正面或负面情感，\N你首先需要获得一堆标签数据，
Dialogue: 1,0:17:07.90,0:17:13.20,Default,,0,0,0,,也许是几百个例子，这可能需\N要几周，也许一个月。然后，
Dialogue: 1,0:17:13.20,0:17:18.62,Default,,0,0,0,,你会在数据上训练一个模型，\N获取一个合适的开源模型，
Dialogue: 1,0:17:18.62,0:17:25.54,Default,,0,0,0,,对模型进行调优，评估模型，这可能需要几\N天、几周甚至几个月的时间。接下来，
Dialogue: 1,0:17:25.54,0:17:31.14,Default,,0,0,0,,您可能需要找到一个云服务来部\N署它，然后将模型上传到云端，
Dialogue: 1,0:17:31.14,0:17:34.94,Default,,0,0,0,,然后运行模型，最后才能调用你的\N模型。再说，这种情况并不少见，
Dialogue: 1,0:17:34.94,0:17:39.82,Default,,0,0,0,,这可能需要一个团队几个\N月的时间才能完成。
Dialogue: 1,0:17:39.82,0:17:46.90,Default,,0,0,0,,与基于提示的机器学习相比，当你有\N一个文本应用程序时，你可以指定
Dialogue: 1,0:17:46.90,0:17:51.78,Default,,0,0,0,,一个提示，这可能需要几分钟甚至几个\N小时，如果你需要迭代几次才能得到
Dialogue: 1,0:17:51.78,0:17:59.88,Default,,0,0,0,,一个有效的提示，然后在几个小时甚至最\N多几天内，但坦率地说，更多的是几个小时
Dialogue: 1,0:17:59.88,0:18:05.88,Default,,0,0,0,,你可以使用API调用来运行这个\N程序，并开始调用模型，一旦
Dialogue: 1,0:18:05.88,0:18:11.48,Default,,0,0,0,,你完成了这个过程，可能只需要几分钟或\N几个小时，你就可以开始调用模型并
Dialogue: 1,0:18:11.48,0:18:17.28,Default,,0,0,0,,开始进行推断，所以有些应用程序过\N去可能需要我花费六个月甚至一
Dialogue: 1,0:18:17.28,0:18:22.84,Default,,0,0,0,,年的时间来构建，现在你可以在几分\N钟或几小时内，甚至很少的几天内
Dialogue: 1,0:18:22.84,0:18:29.12,Default,,0,0,0,,使用提示来构建，这正在彻底改变\N可以快速构建的AI应用程序。
Dialogue: 1,0:18:29.12,0:18:34.68,Default,,0,0,0,,一个重要的注意事项是，这适用于许\N多非结构化数据应用，包括特别是
Dialogue: 1,0:18:34.68,0:18:40.60,Default,,0,0,0,,文本应用程序，也许越来越多的\N视觉应用程序，尽管视觉技术
Dialogue: 1,0:18:40.60,0:18:44.52,Default,,0,0,0,,目前还不够成熟，但它正\N在朝这个方向发展。
Dialogue: 1,0:18:44.52,0:18:49.52,Default,,0,0,0,,这个方法对于结构化数据应用（\N即机器学习）并不十分适用，
Dialogue: 1,0:18:49.52,0:18:54.84,Default,,0,0,0,,在Excel表格中有大量数\N值的表格数据应用上，
Dialogue: 1,0:18:54.84,0:19:01.12,Default,,0,0,0,,但对于适用的应用来说，AI组\N件可以快速构建的事实，
Dialogue: 1,0:19:01.12,0:19:07.48,Default,,0,0,0,,正在改变整个系统可能构建的工作流程。
Dialogue: 1,0:19:07.48,0:19:11.8,Default,,0,0,0,,构建整个系统可能仍然需要几天\N或几周的时间，但至少这部分
Dialogue: 1,0:19:11.8,0:19:16.88,Default,,0,0,0,,可以更快地完成，所以让我们继\N续下一个视频，Isa将展示
Dialogue: 1,0:19:16.88,0:19:24.40,Default,,0,0,0,,如何使用这些组件来评估\N客户服务助手的输入，
Dialogue: 1,0:19:24.40,0:19:29.4,Default,,0,0,0,,这将是一个更大示例的一部\N分，您将在本课程中看到
Dialogue: 1,0:19:29.4,0:19:33.76,Default,,0,0,0,,如何为在线零售商构建\N一个客户服务助手。