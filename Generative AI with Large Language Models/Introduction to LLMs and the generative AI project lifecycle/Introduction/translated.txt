欢迎回来。
本周有很多令人激动的内容要学习讨论，
Mike将在稍后与你分享的第一个主题是：深入了解Transformer网络的工作原理。

是的，这是一个复杂的话题，对吗？
2017年发表的论文《Attension is all You Need》详细阐述了在Transformer架构中会发生的一系列相当复杂的数据处理过程。


我们会从高层次的角度进行讨论，但也会深入探究一些内容。
我们会讨论自注意力和多头自注意力机制等内容。

这样我们就能理解这些模型为什么有效，它们是如何理解语言的。

Transformer已经存在了很长时间，令人惊奇的是，它仍然是许多模型的最先进技术。

我记得当我第一次看到Transformer论文时，我觉得，是的，我懂这个等式。

我承认这是一个数学等式。
但它实际上是做什么的？
这一直似乎有点神奇。
我花了很长时间去研究它，最后终于明白，这就是它有效的原因。
所以我认为在第一周，你会学习到一些你可能听过的专业名词背后的知识，比如多头注意力（Multi-headed Attention）。

那是什么，它的意义是什么？
为什么Transformer架构真正流行起来？
我认为注意力机制（Attention）已经存在了很长时间，但我认为真正让它流行起来的是，它允许注意力以大规模并行的方式工作。


所以它使得现代GPU上的工作成为可能，并且可以扩大其规模。
我认为很多人并不了解关于Transformer的这些细微之处，所以我期待你们能深入研究这个问题。

绝对的，我想说的是，规模是一部分，它能够处理所有的数据。

我只是想说，我们不会在这个问题上深入到让人头疼的程度。

如果他们想这样做，他们可以去阅读那篇论文。
我们要做的是，我们要看看那个Transformer架构的真正重要的部分，
让你得到你需要的知识，这样你就可以实际地利用这些模型。

一件让我感到惊讶和高兴的事情是，虽然这个课程主要关注文本，

但看到基础的Transformer架构如何为视觉Transformer创建基础实在是非常有趣。

所以，虽然在这门课程中你主要学习的是大语言模型、关于文本的模型，
我认为理解Transformer也有助于人们理解这种非常激动人心的视觉Transformer以及其他形式。

这将是很多机器学习的重要基础。

绝对的。
然后，除了Transformer，还有第二个主要的话题，
期待在这第一周覆盖，那就是生成式AI项目生命周期。
我知道很多人在想，哇，所有这些LLM的项目，我该怎么去构建呢？
生成式AI项目生命周期，我们稍后会谈到，它可以帮助你规划如何构建你自己的生成式AI项目。

没错，生成式AI项目生命周期会引导你了解在开发生成式AI应用时需要做出的各个阶段和决定。


所以你首先需要决定的是，是选择一个现成的基础模型，还是实际上要预训练你自己的模型，

然后作为后续，你是否希望微调和定制该模型以适应你的特定数据。

是的，事实上，现在有很多大语言模型可供选择，
有些是开源的，有些不是，我看到很多开发人员在纠结，我应该选择哪些模型？

因此，有一个评估并选择正确模型大小的方式是非常重要的。
我知道在你的其他工作中，你谈到过何时需要一个巨大的模型，比如1000亿甚至更大的模型，
相对的，何时一个1到30亿参数的模型或者小于10亿参数的模型对于特定的应用来说就足够好？

确实，可能有一些使用场景，你真的需要模型非常全面，
能够适应很多不同的任务。
也可能有一些使用场景，你只是为了单一的使用场景进行优化，对吧？

你可能只需要使用一个小一些的模型就能获得相似甚至非常好的结果。

是的，我认为这可能是一些人会感到非常惊讶的事情，
那就是你实际上可以使用相当小的模型，仍然能够从中获取很多的能力。

是的，我认为当你希望你的大语言模型具有很多关于世界的通用知识，
当你希望它知道关于历史和哲学的东西，知道大小，知道如何编写Python代码等等时。

拥有一个拥有上千亿参数的巨大模型是有帮助的。
但是对于像对话总结或者作为一家公司的客户服务代理这样的单一任务，对于这样的应用，

有时候你可以使用上百亿参数的模型。
但这并不总是必要的。
所以本周有很多真正令人兴奋的材料要深入研究。
那么，让我们进入下一个视频，Mike将开始深入探讨大语言模型的许多不同使用案例。
