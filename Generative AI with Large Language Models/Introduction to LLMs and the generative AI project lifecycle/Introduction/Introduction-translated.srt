1
00:00:00,330 --> 00:00:00,996
欢迎回来。

2
00:00:00,996 --> 00:00:04,907
本周有很多令人激动的内容要学习讨论，

3
00:00:04,907 --> 00:00:13,252
Mike将在稍后与你分享的第一个主题是：深入了解Transformer网络的工作原理。

5
00:00:13,252 --> 00:00:16,532
是的，这是一个复杂的话题，对吗？

6
00:00:16,532 --> 00:00:26,594
2017年发表的论文《Attension is all You Need》详细阐述了在Transformer架构中会发生的一系列相当复杂的数据处理过程。

9
00:00:26,594 --> 00:00:31,298
我们会从高层次的角度进行讨论，但也会深入探究一些内容。

10
00:00:31,298 --> 00:00:36,354
我们会讨论自注意力和多头自注意力机制等内容。

12
00:00:36,354 --> 00:00:45,126
这样我们就能理解这些模型为什么有效，它们是如何理解语言的。

14
00:00:45,126 --> 00:00:51,494
Transformer已经存在了很长时间，令人惊奇的是，它仍然是许多模型的最先进技术。

16
00:00:51,494 --> 00:00:58,058
我记得当我第一次看到Transformer论文时，我觉得，是的，我懂这个等式。

18
00:00:58,058 --> 00:01:00,538
我承认这是一个数学等式。

19
00:01:00,538 --> 00:01:02,500
但它实际上是做什么的？

20
00:01:02,500 --> 00:01:04,154
这一直似乎有点神奇。

21
00:01:04,154 --> 00:01:08,056
我花了很长时间去研究它，最后终于明白，这就是它有效的原因。

22
00:01:08,056 --> 00:01:17,102
所以我认为在第一周，你会学习到一些你可能听过的专业名词背后的知识，比如多头注意力（Multi-headed Attention）。

24
00:01:17,102 --> 00:01:19,116
那是什么，它的意义是什么？

25
00:01:19,116 --> 00:01:23,276
为什么Transformer架构真正流行起来？

26
00:01:23,276 --> 00:01:32,092
我认为注意力机制（Attention）已经存在了很长时间，但我认为真正让它流行起来的是，它允许注意力以大规模并行的方式工作。

29
00:01:32,092 --> 00:01:35,628
所以它使得现代GPU上的工作成为可能，并且可以扩大其规模。

30
00:01:35,628 --> 00:01:43,188
我认为很多人并不了解关于Transformer的这些细微之处，所以我期待你们能深入研究这个问题。

32
00:01:43,188 --> 00:01:47,220
绝对的，我想说的是，规模是一部分，它能够处理所有的数据。

34
00:01:47,220 --> 00:01:53,370
我只是想说，我们不会在这个问题上深入到让人头疼的程度。

36
00:01:53,370 --> 00:01:56,644
如果他们想这样做，他们可以去阅读那篇论文。

37
00:01:56,644 --> 00:02:00,302
我们要做的是，我们要看看那个Transformer架构的真正重要的部分，

38
00:02:00,302 --> 00:02:08,490
让你得到你需要的知识，这样你就可以实际地利用这些模型。

40
00:02:08,490 --> 00:02:17,566
一件让我感到惊讶和高兴的事情是，虽然这个课程主要关注文本，

42
00:02:17,566 --> 00:02:23,792
但看到基础的Transformer架构如何为视觉Transformer创建基础实在是非常有趣。

44
00:02:23,792 --> 00:02:27,986
所以，虽然在这门课程中你主要学习的是大语言模型、关于文本的模型，

45
00:02:27,986 --> 00:02:37,092
我认为理解Transformer也有助于人们理解这种非常激动人心的视觉Transformer以及其他形式。

47
00:02:37,092 --> 00:02:40,660
这将是很多机器学习的重要基础。

49
00:02:40,660 --> 00:02:41,396
绝对的。

50
00:02:41,396 --> 00:02:46,688
然后，除了Transformer，还有第二个主要的话题，

51
00:02:46,688 --> 00:02:51,934
期待在这第一周覆盖，那就是生成式AI项目生命周期。

52
00:02:51,934 --> 00:02:56,216
我知道很多人在想，哇，所有这些LLM的项目，我该怎么去构建呢？

53
00:02:56,216 --> 00:03:06,810
生成式AI项目生命周期，我们稍后会谈到，它可以帮助你规划如何构建你自己的生成式AI项目。

55
00:03:06,810 --> 00:03:18,140
没错，生成式AI项目生命周期会引导你了解在开发生成式AI应用时需要做出的各个阶段和决定。

58
00:03:18,140 --> 00:03:26,869
所以你首先需要决定的是，是选择一个现成的基础模型，还是实际上要预训练你自己的模型，

60
00:03:26,869 --> 00:03:32,852
然后作为后续，你是否希望微调和定制该模型以适应你的特定数据。

62
00:03:32,852 --> 00:03:37,045
是的，事实上，现在有很多大语言模型可供选择，

63
00:03:37,045 --> 00:03:43,752
有些是开源的，有些不是，我看到很多开发人员在纠结，我应该选择哪些模型？

65
00:03:43,752 --> 00:03:48,295
因此，有一个评估并选择正确模型大小的方式是非常重要的。

66
00:03:48,295 --> 00:03:53,313
我知道在你的其他工作中，你谈到过何时需要一个巨大的模型，比如1000亿甚至更大的模型，

67
00:03:53,313 --> 00:04:03,152
相对的，何时一个1到30亿参数的模型或者小于10亿参数的模型对于特定的应用来说就足够好？

69
00:04:03,152 --> 00:04:07,750
确实，可能有一些使用场景，你真的需要模型非常全面，

70
00:04:07,750 --> 00:04:11,862
能够适应很多不同的任务。

71
00:04:11,862 --> 00:04:16,417
也可能有一些使用场景，你只是为了单一的使用场景进行优化，对吧？

73
00:04:16,417 --> 00:04:21,466
你可能只需要使用一个小一些的模型就能获得相似甚至非常好的结果。

75
00:04:21,466 --> 00:04:24,683
是的，我认为这可能是一些人会感到非常惊讶的事情，

76
00:04:24,683 --> 00:04:30,488
那就是你实际上可以使用相当小的模型，仍然能够从中获取很多的能力。

78
00:04:30,488 --> 00:04:34,420
是的，我认为当你希望你的大语言模型具有很多关于世界的通用知识，

79
00:04:34,420 --> 00:04:42,060
当你希望它知道关于历史和哲学的东西，知道大小，知道如何编写Python代码等等时。

81
00:04:42,060 --> 00:04:46,543
拥有一个拥有上千亿参数的巨大模型是有帮助的。

82
00:04:46,543 --> 00:04:54,833
但是对于像对话总结或者作为一家公司的客户服务代理这样的单一任务，对于这样的应用，

84
00:04:54,833 --> 00:04:59,027
有时候你可以使用上百亿参数的模型。

85
00:04:59,027 --> 00:05:01,114
但这并不总是必要的。

86
00:05:01,114 --> 00:05:05,508
所以本周有很多真正令人兴奋的材料要深入研究。

87
00:05:05,508 --> 00:05:14,730
那么，让我们进入下一个视频，Mike将开始深入探讨大语言模型的许多不同使用案例。
