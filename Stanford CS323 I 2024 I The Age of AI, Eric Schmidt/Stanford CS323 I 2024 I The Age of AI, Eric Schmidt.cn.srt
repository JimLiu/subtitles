1
00:00:05,159 --> 00:00:06,920
今天的嘉宾其实无需过多介绍。

2
00:00:07,240 --> 00:00:10,100
我记得大约 25 年前第一次见到 Eric，

3
00:00:10,220 --> 00:00:13,580
那时他作为 Novell 的首席执行官来访斯坦福商学院。

4
00:00:14,060 --> 00:00:15,800
从那时起，他做了很多事情，

5
00:00:15,940 --> 00:00:18,580
他在 Google（大概是从 2001 年开始）

6
00:00:18,580 --> 00:00:22,860
和 Schmidt Futures（从 2017 年开始）

7
00:00:22,860 --> 00:00:24,760
做了很多事情，

8
00:00:24,820 --> 00:00:26,340
还有很多其他的事情你们可以查询了解。

9
00:00:26,720 --> 00:00:29,580
但他只能待到下午 5 点 15 分，所以我想

10
00:00:29,400 --> 00:00:31,320
我们直接进入问题环节。

11
00:00:31,900 --> 00:00:33,680
我知道你们也有一些问题。

12
00:00:34,180 --> 00:00:34,580
我这里有一些

13
00:00:34,580 --> 00:00:34,980
我写下的问题，

14
00:00:35,060 --> 00:00:37,840
但我们在楼上刚刚谈论的内容更有趣。

15
00:00:38,000 --> 00:00:40,460
所以我想从那开始，Eric，如果你不介意的话。

16
00:00:40,940 --> 00:00:42,620
那就是，你预见

17
00:00:42,660 --> 00:00:46,620
AI 在短期内，我认为你定义的是未来一两年，会有怎样的发展？

18
00:00:48,960 --> 00:00:49,900
事情变化得如此之快，

19
00:00:50,000 --> 00:00:53,020
我感觉我每六个月都需要做一次新的演讲，

20
00:00:53,080 --> 00:00:53,800
讲述即将发生的事情。

21
00:00:54,880 --> 00:00:58,660
在座的有没有人，一群计算机科学家在这里，有没有人

22
00:00:58,520 --> 00:01:02,520
可以解释一下什么是百万  token 的上下文窗口，

23
00:01:02,640 --> 00:01:03,380
为其他同学解释一下？

24
00:01:04,739 --> 00:01:05,140
在这里。

25
00:01:05,600 --> 00:01:06,420
说出你的名字。

26
00:01:06,700 --> 00:01:07,480
告诉我们它的作用是什么。

27
00:01:07,800 --> 00:01:08,020
好的。

28
00:01:09,800 --> 00:01:16,540
基本上，它允许你用百万个 token 或者百万个词进行提示。

29
00:01:17,520 --> 00:01:19,640
所以你可以提出一个上百万词的问题。

30
00:01:20,280 --> 00:01:23,920
我了解到，这是当前通识教育关注的一个重要方向。

31
00:01:24,800 --> 00:01:25,720
不，他们的目标是 10。

32
00:01:26,060 --> 00:01:27,380
对，一千万？没错。

33
00:01:27,560 --> 00:01:30,260
接着，Anthropic 现在是 20 万，他们的目标是100 万，

34
00:01:30,520 --> 00:01:31,180
以此类推。

35
00:01:31,580 --> 00:01:33,800
你可以设想 OpenAI 也有类似的目标。

36
00:01:34,760 --> 00:01:38,900
谁能给出 AI 智能体的技术定义？

37
00:01:40,780 --> 00:01:41,540
我知道你们都是科学家。

38
00:01:43,800 --> 00:01:44,180
是的，先生。

39
00:01:44,700 --> 00:01:45,140
- 我叫 Jared。

40
00:01:46,400 --> 00:01:50,720
AI 智能体基本上是执行某种活动的实体。

41
00:01:50,860 --> 00:01:54,580
这可能涉及在网上、

42
00:01:55,200 --> 00:01:58,340
代表你处理一些事情，可能是许多不同的事项，

43
00:01:58,440 --> 00:01:59,380
类似这些。

44
00:01:59,920 --> 00:02:01,980
可能有各种各样的事情。

45
00:02:03,060 --> 00:02:06,680
所以，一个智能体就是执行某种任务的实体。

46
00:02:07,580 --> 00:02:11,600
另一个定义是，它是一个大语言模型，具有状态和记忆功能。

47
00:02:12,920 --> 00:02:17,480
再来一次，计算机科学家，你们中有谁能解释什么是"将文本转换为行动指令"？

48
00:02:24,200 --> 00:02:26,740
就是把文本转换为行动指令。

49
00:02:27,000 --> 00:02:27,400
在这里。

50
00:02:27,720 --> 00:02:27,960
你来说。

51
00:02:28,660 --> 00:02:28,840
对的。

52
00:02:29,300 --> 00:02:32,980
不是将文本转化成更多的文本。

53
00:02:33,100 --> 00:02:33,580
更多的文本。

54
00:02:33,960 --> 00:02:37,480
而是让文本触发 AI 的行动。

55
00:02:38,080 --> 00:02:42,800
另一个定义是，将语言转化为 Python 代码。

56
00:02:44,360 --> 00:02:47,660
这是我一直不想看到的编程语言。

57
00:02:48,280 --> 00:02:50,360
然而，目前所有的 AI 工作都是在使用 Python 进行的。

58
00:02:50,520 --> 00:02:53,220
有一种新的语言叫 Mojo，刚刚出现，

59
00:02:53,760 --> 00:02:56,020
看起来他们终于解决了 AI 编程的问题。

60
00:02:56,280 --> 00:02:58,760
但我们还要看，这是否能在 Python 的主导地位下生存下来。

61
00:03:01,399 --> 00:03:02,760
再来一个技术问题。

62
00:03:03,420 --> 00:03:08,800
为什么 NVIDIA 的价值和地位如此之高，而其他公司却在挣扎呢？

63
00:03:12,460 --> 00:03:13,180
给你一个技术答案。

64
00:03:13,520 --> 00:03:15,100
我认为，这主要是因为，

65
00:03:15,960 --> 00:03:18,860
大量的代码需要在 CUDA 优化下运行，

66
00:03:18,860 --> 00:03:21,120
而这是只有 NVIDIA 的 GPU 才支持的，

67
00:03:21,520 --> 00:03:23,860
所以，其他公司可以制造他们想要的任何东西，

68
00:03:23,960 --> 00:03:27,080
但是如果他们没有 10 年的软件开发经验，

69
00:03:27,440 --> 00:03:29,680
就不可能有机器学习优化。

70
00:03:29,820 --> 00:03:34,940
我个人喜欢把 CUDA 想象成 GPU 的 C 语言，

71
00:03:35,600 --> 00:03:35,600
对吗？

72
00:03:35,740 --> 00:03:36,820
这就是我喜欢的看法。

73
00:03:36,820 --> 00:03:38,080
它成立于 2008 年。

74
00:03:38,520 --> 00:03:39,800
我一直觉得它是一种糟糕的编程语言，

75
00:03:40,580 --> 00:03:42,020
然而，但它却成为了市场主导。

76
00:03:42,600 --> 00:03:43,500
还有一点值得注意。

77
00:03:43,680 --> 00:03:45,880
有一套开源库，它们

78
00:03:45,920 --> 00:03:48,420
针对 CUDA 进行了高度优化，而对其他平台的优化却很少。

79
00:03:48,920 --> 00:03:51,160
每个构建所有这些堆栈的人——

80
00:03:51,260 --> 00:03:53,760
这在任何讨论中都被完全忽视了。

81
00:03:56,080 --> 00:03:59,400
这在技术上被称为 VLLM 以及其他一大堆类似的库。

82
00:03:59,720 --> 00:04:03,520
它们都是专门为 CUDA 而优化的，对于竞争对手来说，很难复制这个。

83
00:04:04,500 --> 00:04:06,040
那么，这些观点对我们来说有何影响或意义呢？

84
00:04:07,160 --> 00:04:10,279
在接下来的一年里，我们将看到非常大的上下文窗口、

85
00:04:10,280 --> 00:04:17,899
智能体和"文本转换为行动指令"等新技术的兴起，当它们能够大规模应用时

86
00:04:18,339 --> 00:04:19,260
将对世界产生的影响

87
00:04:19,339 --> 00:04:22,360
将超出我们目前的理解范围。

88
00:04:22,920 --> 00:04:27,560
这种影响将远超过社交媒体所带来的影响，我个人是这样认为的。

89
00:04:28,400 --> 00:04:29,060
以下是我的原因。

90
00:04:29,800 --> 00:04:31,800
在一个上下文窗口中，你基本上可以

91
00:04:32,220 --> 00:04:33,300
将其作为短期记忆。

92
00:04:34,420 --> 00:04:38,060
我对上下文窗口能达到如此之长感到惊讶。

93
00:04:38,240 --> 00:04:42,020
这主要由于它的计算和处理难度很高。

94
00:04:42,500 --> 00:04:45,360
短期记忆的有趣之处在于， 当你输入信息，

95
00:04:46,440 --> 00:04:49,900
比如你问一个问题，"读了 20 本书，

96
00:04:50,160 --> 00:04:53,400
你输入这些书的文本作为查询，然后你说，

97
00:04:53,520 --> 00:04:55,080
'告诉我它们说了什么。'"它会忘记

98
00:04:55,180 --> 00:04:58,140
中间的部分，这与人类大脑的工作方式相似。

99
00:04:58,780 --> 00:04:58,880
对吗？

100
00:04:58,980 --> 00:04:59,780
这就是我们现在的状况。

101
00:05:00,540 --> 00:05:01,580
关于智能体，

102
00:05:02,660 --> 00:05:07,160
现在有人正在开发基于大语言模型的智能体，

103
00:05:07,000 --> 00:05:09,560
他们的做法是阅读一些像化学一样的学科，

104
00:05:09,960 --> 00:05:13,320
发现其内在原理，然后进行测试。

105
00:05:14,020 --> 00:05:17,560
然后他们将这些知识融入到他们的理解中。

106
00:05:18,000 --> 00:05:19,620
这是非常强大的。

107
00:05:20,180 --> 00:05:22,900
我提到的第三个要点是"文本转换为行动指令"。

108
00:05:23,560 --> 00:05:24,820
那么，我来举个例子，

109
00:05:25,300 --> 00:05:28,440
政府正在尝试禁止 TikTok，

110
00:05:28,560 --> 00:05:29,620
我们拭目以待看结果如何。

111
00:05:30,480 --> 00:05:34,080
如果 TikTok 被禁，我建议你们每个人都这样做，

112
00:05:34,600 --> 00:05:36,620
告诉你的大语言模型，接下去的操作。

113
00:05:38,220 --> 00:05:39,700
复制一份 TikTok。

114
00:05:40,940 --> 00:05:41,980
获取所有用户信息。

115
00:05:42,460 --> 00:05:43,300
获取所有音乐资源。

116
00:05:44,160 --> 00:05:45,500
加入我的个性化设置。

117
00:05:46,400 --> 00:05:49,500
在接下来的 30 秒内编制这个程序。

118
00:05:50,120 --> 00:05:50,740
然后发布出去。

119
00:05:51,400 --> 00:05:53,060
如果一小时内它没有迅速传播开来，

120
00:05:53,500 --> 00:05:55,200
那就沿着同样的思路尝试另一种方式。

121
00:05:55,520 --> 00:05:56,220
这就是命令。

122
00:05:57,380 --> 00:05:59,120
一步接一步，就这样。

123
00:05:59,660 --> 00:05:59,820
明白了吗？

124
00:06:01,120 --> 00:06:03,300
你知道这有多强大吗。

125
00:06:03,300 --> 00:06:07,900
如果你能从任意自然语言转换为任意数字命令，

126
00:06:08,100 --> 00:06:10,460
这在这个情况下就相当于 Python，

127
00:06:10,960 --> 00:06:13,400
试想一下，如果地球上的每个人都

128
00:06:13,400 --> 00:06:15,840
有属于自己的程序员，

129
00:06:15,840 --> 00:06:17,580
他们会真正按照你的要求去做事，

130
00:06:17,840 --> 00:06:20,720
而不是像我手下的那些程序员那样并不总是按照我说的去做。

131
00:06:20,960 --> 00:06:21,480
明白了吗？

132
00:06:22,640 --> 00:06:24,600
在场的程序员都明白我在说什么。

133
00:06:24,600 --> 00:06:29,180
所以，想象一下，有一位既不自大，又会真正按照你的要求去做事的程序员，

134
00:06:29,500 --> 00:06:31,280
你甚至不需要付他一大笔工资。

135
00:06:31,600 --> 00:06:33,640
而且这样的程序无穷无尽。

136
00:06:33,720 --> 00:06:35,260
这一切都将在未来一两年内实现？

137
00:06:35,320 --> 00:06:35,960
马上就要到来。

138
00:06:37,240 --> 00:06:38,640
这三件事，

139
00:06:39,060 --> 00:06:40,760
我深信

140
00:06:40,760 --> 00:06:42,660
只有结合这三件事，

141
00:06:42,660 --> 00:06:45,360
下一波浪潮才会到来。

142
00:06:46,620 --> 00:06:48,960
那么，你问的是接下来会发生什么。

143
00:06:50,900 --> 00:06:54,960
我的观点每六个月会有所改变，这就像一个周期性的摆动。

144
00:06:55,640 --> 00:06:59,740
比如说，现在，那些前沿模型

145
00:06:59,880 --> 00:07:02,280
（只有三个，我待会会详细介绍）

146
00:07:02,680 --> 00:07:06,880
与其他所有人之间的差距，我感觉正在变大。

147
00:07:07,860 --> 00:07:10,680
六个月前，我坚信这个差距正在缩小。

148
00:07:11,280 --> 00:07:13,580
于是我在一些小公司投入了大量的资金。

149
00:07:14,120 --> 00:07:15,100
但现在，我对此已不再那么确定了。

150
00:07:16,320 --> 00:07:17,760
我现在正在和大公司们交谈，

151
00:07:18,040 --> 00:07:19,800
他们告诉我他们需要

152
00:07:19,860 --> 00:07:25,520
投入 100 亿、200 亿、500 亿甚至 1000 亿。

153
00:07:26,700 --> 00:07:28,580
比如说，Stargate 的投入就达到了 1000 亿，对吧？

154
00:07:28,700 --> 00:07:30,420
这确实非常困难。

155
00:07:31,040 --> 00:07:32,220
Sam Altman 是我的密友。

156
00:07:33,160 --> 00:07:37,240
他认为这可能需要投入高达 3000 亿，甚至更多。

157
00:07:38,060 --> 00:07:39,020
我向他指出，

158
00:07:39,020 --> 00:07:40,020
我已经计算出了

159
00:07:40,020 --> 00:07:41,760
这需要的能源量。

160
00:07:42,600 --> 00:07:46,020
然后，在完全公开的精神下，

161
00:07:46,420 --> 00:07:48,180
我上周五去了白宫，告诉他们

162
00:07:48,280 --> 00:07:50,960
我们需要与加拿大建立最紧密的关系。

163
00:07:51,760 --> 00:07:55,920
因为加拿大有非常好的人，参与了人工智能的发明，

164
00:07:56,500 --> 00:07:57,680
还有大量的水力发电资源。

165
00:07:58,320 --> 00:08:01,700
因为我们国家没有足够的能源来完成这件事。

166
00:08:02,240 --> 00:08:04,680
另一个选择就是让阿拉伯人来资助。

167
00:08:05,180 --> 00:08:06,340
我个人非常喜欢阿拉伯人。

168
00:08:07,140 --> 00:08:08,920
我在那里待过很久，对吧？

169
00:08:09,460 --> 00:08:12,000
但他们不会遵守我们的国家安全规则，

170
00:08:12,400 --> 00:08:13,760
而加拿大和美国

171
00:08:13,880 --> 00:08:16,060
是共同遵守安全规则的三方联盟（或三国集团）的一部分。

172
00:08:16,140 --> 00:08:18,560
因此，对于这些价值 3000 亿美元的数据中心来说，

173
00:08:18,860 --> 00:08:21,140
电力开始变得稀缺。

174
00:08:22,720 --> 00:08:24,780
顺便说一下，如果你沿着这个逻辑走下去，

175
00:08:25,220 --> 00:08:27,040
我为什么要讨论 CUDA 和 NVIDIA 呢？

176
00:08:27,700 --> 00:08:30,700
如果有 3000 亿美元都要流向 NVIDIA，

177
00:08:31,440 --> 00:08:32,799
你应该知道在股市里应该怎么做。

178
00:08:34,700 --> 00:08:36,299
这不是股票推荐，

179
00:08:36,539 --> 00:08:37,240
我并不是许可证发放者。

180
00:08:37,240 --> 00:08:38,220
（观众笑）

181
00:08:39,159 --> 00:08:41,400
部分原因是，我们需要更多的芯片，

182
00:08:41,679 --> 00:08:43,419
但英特尔正在从美国政府和 AMD 那里得到大笔资金，

183
00:08:43,580 --> 00:08:49,780
他们正准备在韩国建造半导体工厂。

184
00:08:50,060 --> 00:08:54,980
- 有谁的计算设备里有英特尔的电脑或者芯片呢，请举手。

185
00:08:57,100 --> 00:08:59,280
看来，垄断不再是什么大问题了。

186
00:09:01,140 --> 00:09:02,520
- 这正是我想说的。

187
00:09:02,620 --> 00:09:03,680
他们曾经垄断过。

188
00:09:03,720 --> 00:09:04,080
- 没错。

189
00:09:04,880 --> 00:09:06,280
- 而现在 Nvidia 有垄断。

190
00:09:06,520 --> 00:09:07,840
那些对进入的障碍呢？

191
00:09:07,940 --> 00:09:14,080
例如 CUDA，还有其他的，就像我前几天和 Percy Lanny 聊天时提到的。

192
00:09:14,400 --> 00:09:19,820
他根据训练模型所能获得的设备，会在 TPUs 和 NVIDIA 芯片之间做选择。

193
00:09:19,840 --> 00:09:20,960
- 那是因为他别无选择。

194
00:09:21,160 --> 00:09:23,060
如果他有无限的资金，我会

195
00:09:23,560 --> 00:09:27,080
今天我会选择 NVIDIA 的 B200 架构，

196
00:09:27,160 --> 00:09:27,940
因为它运行更快。

197
00:09:28,600 --> 00:09:29,880
我并不在这里提倡什么，我只是想说

198
00:09:29,940 --> 00:09:30,920
有竞争是好事。

199
00:09:31,100 --> 00:09:33,640
我和 AMD 的 Lisa Su 有过长时间的交谈。

200
00:09:34,200 --> 00:09:37,160
他们正在开发一种能将

201
00:09:37,400 --> 00:09:40,640
你描述的这种 CUDA 架构

202
00:09:41,240 --> 00:09:42,580
转换为他们自己的架构，即 ROCm。

203
00:09:42,680 --> 00:09:43,840
目前它还不能完全运行，

204
00:09:44,840 --> 00:09:45,440
他们正在努力改进。

205
00:09:47,660 --> 00:09:49,220
- 你在谷歌工作了很长时间，

206
00:09:49,420 --> 00:09:52,320
他们是 Transformer 架构的发明者。

207
00:09:53,180 --> 00:09:55,080
- 是彼得，都是彼得的错。

208
00:09:55,120 --> 00:09:59,200
- 那里有像彼得和杰夫·迪恩这样的出色人才。

209
00:10:00,800 --> 00:10:02,260
但现在，他们似乎已经失去了

210
00:10:03,840 --> 00:10:05,680
对 OpenAI 的主动权。

211
00:10:05,840 --> 00:10:09,240
我看到的最新排行榜上，Anthropic's Claude 是榜首。

212
00:10:10,840 --> 00:10:12,280
我问过 Sundar 这方面的问题，

213
00:10:12,340 --> 00:10:14,060
他并没有给我一个明确的答案。

214
00:10:14,240 --> 00:10:19,540
或许你能给出一个更明确或更客观的解释。 

215
00:10:19,920 --> 00:10:21,440
我现在已经不再是谷歌的员工了。

216
00:10:21,720 --> 00:10:21,980
确实如此。

217
00:10:22,800 --> 00:10:23,780
我要坦白的说，

218
00:10:25,080 --> 00:10:28,860
谷歌认为工作与生活的平衡，早点下班、

219
00:10:29,140 --> 00:10:32,180
以及在家工作比赢得比赛更重要。

220
00:10:32,280 --> 00:10:33,640
（笑）

221
00:10:35,640 --> 00:10:37,880
创业公司之所以能够成功，

222
00:10:38,200 --> 00:10:39,860
是因为员工拼命工作。

223
00:10:40,260 --> 00:10:43,860
很抱歉如此直言不讳，但事实是，

224
00:10:43,940 --> 00:10:47,820
如果你们离开大学去创办公司，

225
00:10:48,420 --> 00:10:50,600
你不会允许员工在家办公，

226
00:10:50,740 --> 00:10:55,080
而且每周只来公司一天，如果想要与其他创业公司竞争的话。

227
00:10:56,420 --> 00:10:57,640
- Google 创业初期，

228
00:10:58,260 --> 00:10:59,160
Microsoft 就是这样。

229
00:10:59,220 --> 00:10:59,620
- 对的。

230
00:11:00,240 --> 00:11:02,260
- 但现在似乎——

231
00:11:02,260 --> 00:11:05,440
在我们这个行业里有很多公司，

232
00:11:06,000 --> 00:11:09,200
以真正创造性的方式赢得市场

233
00:11:09,380 --> 00:11:14,320
并在某一领域取得主导地位，但却未能完成下一次转型。

234
00:11:14,660 --> 00:11:15,900
这种现象很常见，并且有很多文献记录。

235
00:11:16,940 --> 00:11:21,740
我认为，创始人是特殊的，他们需要掌控一切，

236
00:11:21,800 --> 00:11:24,600
与他们共事可能会很艰难，

237
00:11:24,760 --> 00:11:26,000
他们会给员工施加很大的压力。

238
00:11:27,200 --> 00:11:31,500
我们可能并不喜欢马斯克的个人行为， 但你看看他是如何推动员工的。

239
00:11:31,960 --> 00:11:35,600
我曾和他共进晚餐，当时他在蒙大拿州，

240
00:11:36,060 --> 00:11:40,100
而那天晚上 10 点他要飞往另一个地方，凌晨 12 点与 X.AI 开会。

241
00:11:41,180 --> 00:11:41,340
对吧？

242
00:11:41,880 --> 00:11:42,440
你想想看吧。

243
00:11:42,820 --> 00:11:45,360
我曾去过台湾，

244
00:11:45,560 --> 00:11:49,280
有着完全不同的文化，他们（台积电）

245
00:11:49,520 --> 00:11:50,300
让我印象深刻的一点是，

246
00:11:50,300 --> 00:11:55,620
他们有一条规定：这些刚入职的优秀物理学博士

247
00:11:56,180 --> 00:12:01,120
需要在地下一层的工厂工作。

248
00:12:01,700 --> 00:12:05,840
你能想象让美国的物理博士去做那样的事吗？

249
00:12:05,920 --> 00:12:06,520
几乎不可能。

250
00:12:06,940 --> 00:12:07,640
他们的工作态度和我们有所不同。

251
00:12:08,580 --> 00:12:09,440
而问题在于，

252
00:12:09,560 --> 00:12:12,960
我之所以对工作要求这么严格，是因为

253
00:12:12,960 --> 00:12:14,440
这些系统

254
00:12:14,440 --> 00:12:15,560
具有网络效应，

255
00:12:16,160 --> 00:12:17,760
时间是非常关键的。

256
00:12:18,300 --> 00:12:21,380
在大部分业务中，时间其实不那么重要。

257
00:12:22,240 --> 00:12:23,280
你有充足的时间。

258
00:12:23,500 --> 00:12:25,420
可口可乐和百事可乐会一直存在，

259
00:12:25,660 --> 00:12:27,400
他们之间的竞争

260
00:12:27,560 --> 00:12:29,800
也会持续，这一切都在慢慢发展。

261
00:12:30,880 --> 00:12:33,940
我和电信公司打交道时，一般的电信交易

262
00:12:34,100 --> 00:12:36,200
要花费 18 个月才能完成。

263
00:12:37,940 --> 00:12:40,260
实际上，没有任何事情需要花费 18 个月去完成。

264
00:12:40,380 --> 00:12:40,920
要迅速行动。

265
00:12:42,260 --> 00:12:45,580
我们现在处于最大的发展期，最大的进步期。

266
00:12:47,260 --> 00:12:49,780
这也需要一些疯狂的想法。

267
00:12:49,860 --> 00:12:51,920
比如当微软与 OpenAI 达成交易时，

268
00:12:52,080 --> 00:12:54,760
我认为那是我所听过的最愚蠢的想法。

269
00:12:55,900 --> 00:12:58,360
将 AI 的主导地位让渡给 OpenAI，

270
00:12:58,520 --> 00:13:01,000
包括 Sam 和他的团队，这简直太疯狂了。

271
00:13:01,100 --> 00:13:03,400
在微软或其他任何地方，都没有人会这么做。

272
00:13:03,980 --> 00:13:05,920
然而现在，他们正在朝着

273
00:13:05,940 --> 00:13:07,240
成为最有价值的公司的目标前进。

274
00:13:07,380 --> 00:13:09,380
他们和苹果公司的竞争激烈。

275
00:13:09,460 --> 00:13:11,100
苹果公司并没有一个好的 AI 解决方案，

276
00:13:11,800 --> 00:13:12,960
而微软看起来已经成功了。

277
00:13:15,100 --> 00:13:15,320
是的，先生。

278
00:13:16,080 --> 00:13:18,780
在国家安全或地缘政治利益方面，

279
00:13:18,880 --> 00:13:22,020
你认为 AI 将如何

280
00:13:22,080 --> 00:13:22,780
在与中国的竞争中发挥作用？

281
00:13:22,960 --> 00:13:26,240
我曾经是一个 AI 委员会的主席，我们

282
00:13:26,280 --> 00:13:27,060
对此进行了非常详细的研究。

283
00:13:28,060 --> 00:13:29,780
你可以去看看。

284
00:13:29,900 --> 00:13:31,560
它有大约 752 页。

285
00:13:32,080 --> 00:13:34,440
我只是总结一下，我们现在处于领先位置。

286
00:13:34,700 --> 00:13:35,500
我们需要保持这种领先地位，

287
00:13:35,920 --> 00:13:37,280
并且需要大量的资金来做到这一点。

288
00:13:37,780 --> 00:13:39,580
我们的主要对象是参议院和众议院。

289
00:13:41,480 --> 00:13:45,520
由此促成了《芯片法案》以及其他相关立法。

290
00:13:46,840 --> 00:13:52,780
如果你假设前沿模型不断发展，

291
00:13:52,820 --> 00:13:54,380
少数开源模型也在进步，

292
00:13:55,100 --> 00:13:57,640
很可能只有少数国家能够

293
00:13:57,700 --> 00:13:58,460
参与这场竞争。

294
00:13:59,040 --> 00:13:59,880
我是指国家，而不是公司。

295
00:14:00,620 --> 00:14:02,380
那么这些国家是谁呢？

296
00:14:02,740 --> 00:14:08,260
有大量资金、丰富人才、强大教育系统以及获胜意愿的国家。 

297
00:14:08,520 --> 00:14:09,380
美国就是其中的一个。

298
00:14:10,180 --> 00:14:11,020
中国也是。

299
00:14:11,560 --> 00:14:13,540
还有其他国家吗？

300
00:14:15,540 --> 00:14:16,560
我不知道，也许有。

301
00:14:17,340 --> 00:14:19,640
在你们这一代人的有生之年，

302
00:14:19,780 --> 00:14:25,280
围绕知识霸权的美中对抗将会是主要的斗争。

303
00:14:26,820 --> 00:14:30,300
因此，美国政府基本上禁止了 NVIDIA 芯片出口到中国，

304
00:14:30,520 --> 00:14:32,420
尽管他们并不愿明说这是他们的初衷，

305
00:14:32,500 --> 00:14:34,520
但实际上确实如此。

306
00:14:35,860 --> 00:14:37,940
我们在芯片制造技术上大约领先 10 年。

307
00:14:38,080 --> 00:14:43,560
在次紫外光刻（sub-DUV），即小于 5 纳米的芯片方面，我们大约领先 10 年。

308
00:14:43,560 --> 00:14:43,960
10 年，这么久？

309
00:14:43,980 --> 00:14:44,660
大概 10 年。

310
00:14:44,920 --> 00:14:45,020
哦。

311
00:14:46,020 --> 00:14:51,780
所以，以现在的情况为例，我们比中国领先了几年。

312
00:14:52,080 --> 00:14:54,120
我猜我们可能还会领先中国几年。

313
00:14:54,340 --> 00:14:56,820
中国对此非常不满。

314
00:14:56,940 --> 00:14:58,480
他们对此感到非常沮丧。

315
00:14:59,380 --> 00:15:00,260
这是个大问题。

316
00:15:00,360 --> 00:15:02,240
这是特朗普政府的决定，

317
00:15:02,240 --> 00:15:03,980
并且拜登政府也同样执行了这个决定。

318
00:15:05,040 --> 00:15:07,300
你觉得现在的政府

319
00:15:07,300 --> 00:15:09,600
和国会听取你的建议了吗？

320
00:15:09,980 --> 00:15:13,100
你认为他们会进行如此大规模的投资吗？

321
00:15:13,240 --> 00:15:18,700
显然，《芯片法案》已经出台，但除此之外，是否还会建立一个庞大的 AI 系统？

322
00:15:18,960 --> 00:15:25,480
你知道，我领导的是一个非正式、特设的、不受法律约束的小组。

323
00:15:26,280 --> 00:15:27,900
这和违法是不同的。

324
00:15:27,920 --> 00:15:29,060
确切地说，只是为了明确。

325
00:15:30,040 --> 00:15:34,120
这包括所有的同行。

326
00:15:34,480 --> 00:15:36,980
在过去的一年里，这些同行

327
00:15:36,980 --> 00:15:39,400
提出了一些理论基础，

328
00:15:39,400 --> 00:15:46,460
最终成为了拜登政府《AI 法案》的核心内容，

329
00:15:47,160 --> 00:15:49,800
这是历史上最长的总统行政命令。

330
00:15:50,080 --> 00:15:52,180
你是在谈论特殊竞争研究项目吗？

331
00:15:52,180 --> 00:15:57,220
不，这是来自行政办公室的实际法案。

332
00:15:58,120 --> 00:15:59,780
他们现在正忙于实施细节。

333
00:16:00,100 --> 00:16:01,620
到目前为止，他们做得很好。

334
00:16:02,580 --> 00:16:03,360
例如，

335
00:16:03,460 --> 00:16:08,320
过去一年中我们讨论的一个问题是，

336
00:16:08,340 --> 00:16:12,500
如何在一个已经学习到危险内容的系统中检测这些危险，

337
00:16:12,600 --> 00:16:14,260
但是你不知道该问它什么？

338
00:16:15,340 --> 00:16:18,280
换句话说，这是一个核心难题。

339
00:16:18,940 --> 00:16:22,780
系统可能学到了一些有害的东西，但它无法告诉你学到了什么，

340
00:16:23,040 --> 00:16:24,120
而你也不知道该如何询问它。

341
00:16:24,440 --> 00:16:26,260
而且威胁种类繁多，

342
00:16:26,360 --> 00:16:28,220
比如，它学会了一种

343
00:16:28,320 --> 00:16:30,080
你不知道如何询问的新的化学混合方式。

344
00:16:31,240 --> 00:16:32,800
因此，人们正在努力解决这个问题。

345
00:16:32,880 --> 00:16:32,960
但

346
00:16:33,020 --> 00:16:36,920
最终我们在给他们的备忘录中写道，有一个阈值，

347
00:16:37,260 --> 00:16:38,560
我们任意设定为

348
00:16:38,820 --> 00:16:43,600
10 的 26 次方的浮点运算，这在技术上是一个计算量的衡量标准。

349
00:16:44,400 --> 00:16:45,060
超过这个阈值，

350
00:16:45,060 --> 00:16:48,660
你必须向政府报告你正在进行这种操作。

351
00:16:48,960 --> 00:16:50,000
这就是规则的一部分。

352
00:16:50,860 --> 00:16:53,360
欧盟为了有所区别，

353
00:16:53,380 --> 00:16:55,220
把阈值定为 10 的 25 次方。

354
00:16:55,640 --> 00:16:55,740
是的。

355
00:16:56,600 --> 00:16:57,860
但这差距其实很小。

356
00:16:58,260 --> 00:16:59,960
我认为所有这些区别都会消失，

357
00:17:00,100 --> 00:17:02,660
因为现在的技术 - 专业术语是

358
00:17:02,720 --> 00:17:08,180
联邦学习技术，基本上你可以将不同部分联合起来进行训练。

359
00:17:08,900 --> 00:17:12,740
所以我们可能无法让人们完全免受这些新威胁的影响。

360
00:17:12,740 --> 00:17:18,300
据传言，这也是 OpenAI 必须这样训练的部分原因，因为电力消耗太大

361
00:17:18,700 --> 00:17:20,400
无法集中在一个地方进行训练。

362
00:17:20,660 --> 00:17:23,160
好了，让我们谈谈正在进行的真正战争。

363
00:17:23,240 --> 00:17:28,260
我知道你非常关注乌克兰战争，

364
00:17:28,520 --> 00:17:29,060
尤其是，

365
00:17:30,340 --> 00:17:31,820
关于“白鹳”项目，我不确定你能谈多少，

366
00:17:31,820 --> 00:17:39,380
关于用 500 美元的无人机摧毁 500 万美元的坦克。

367
00:17:40,540 --> 00:17:41,480
这个改变了战争方式吗？

368
00:17:41,740 --> 00:17:44,580
我曾在国防部工作了七年，

369
00:17:45,520 --> 00:17:49,680
试图改变我们管理军队的方式。

370
00:17:50,020 --> 00:17:51,860
虽然我并不特别喜欢军队，

371
00:17:52,000 --> 00:17:52,840
但是军队的运行开支非常大，

372
00:17:53,040 --> 00:17:54,440
我想看看我能否对此提供一些帮助。

373
00:17:55,160 --> 00:17:56,860
而现在看来，我觉得我基本上失败了。

374
00:17:57,140 --> 00:17:58,160
他们给了我一枚勋章，

375
00:17:59,000 --> 00:18:03,020
所以可能失败者也能得到勋章吧，或者随便怎么说。

376
00:18:03,680 --> 00:18:06,580
但我对自己的批评是，什么都没有真正改变，

377
00:18:07,140 --> 00:18:11,460
美国的体系不会带来真正的创新。

378
00:18:12,540 --> 00:18:19,060
看着俄罗斯人用坦克摧毁有老人和孩子的公寓楼，

379
00:18:19,080 --> 00:18:19,980
我感到非常愤怒。

380
00:18:20,940 --> 00:18:23,200
所以我决定和你的朋友、

381
00:18:23,380 --> 00:18:26,380
斯坦福大学的前任教授塞巴斯蒂安·特鲁恩，

382
00:18:26,840 --> 00:18:28,080
以及一批斯坦福人一起创办一家公司。

383
00:18:29,080 --> 00:18:32,760
其实，我们的目标主要有两个。

384
00:18:33,120 --> 00:18:37,160
首先是用复杂而强大的方式将 AI 应用于这些机器人战争中，

385
00:18:37,900 --> 00:18:40,260
其次是降低机器人的成本。

386
00:18:40,880 --> 00:18:43,500
你可能会好奇，一个像我这样的自由派

387
00:18:43,700 --> 00:18:44,500
为何会有这样的想法？

388
00:18:45,000 --> 00:18:49,340
答案是，现有的军队理论

389
00:18:49,660 --> 00:18:51,260
以坦克、炮兵和迫击炮为主，

390
00:18:52,020 --> 00:18:53,580
而我们可以消除它们。

391
00:18:54,320 --> 00:18:56,980
我们可以让入侵一个国家的代价，

392
00:18:57,220 --> 00:19:00,200
至少在陆地上，几乎是不可能的。

393
00:19:00,400 --> 00:19:02,940
这应该可以避免大规模的陆地战争。

394
00:19:03,220 --> 00:19:04,860
- 这确实是一个很有趣的问题，

395
00:19:05,100 --> 00:19:07,880
这种方式是否能让防守方获得更多优势？

396
00:19:07,880 --> 00:19:10,240
我们能否做出这样的区分呢？

397
00:19:10,400 --> 00:19:11,800
- 在过去的一年里，我一直在做这个，

398
00:19:11,840 --> 00:19:14,720
我学到了很多关于战争的知识，而这些知识我原本不想知道。

399
00:19:15,460 --> 00:19:20,240
其中一个关键点是，进攻方总是占据优势，

400
00:19:20,500 --> 00:19:23,600
因为他们总能压倒防御系统。

401
00:19:24,580 --> 00:19:27,900
所以，作为国家防御策略，

402
00:19:28,000 --> 00:19:29,680
拥有一套强大的进攻机制是很有必要的，

403
00:19:30,000 --> 00:19:31,440
以备不时之需。

404
00:19:32,040 --> 00:19:34,920
而我和其他人正在构建的系统将能够实现这一点。

405
00:19:35,920 --> 00:19:37,900
由于系统的运作方式，

406
00:19:38,080 --> 00:19:40,180
我现在是一名持证军火商。

407
00:19:41,580 --> 00:19:44,440
所以我现在既是计算机科学家，商人，也是军火商。

408
00:19:45,240 --> 00:19:45,980
(笑)

409
00:19:46,960 --> 00:19:48,140
我很抱歉地说——这算是一种职业进步吗？

410
00:19:49,460 --> 00:19:51,960
- 我不太确定，但我并不建议你把这作为你的职业发展路径。

411
00:19:52,040 --> 00:19:52,760
我建议你还是继续做 AI。

412
00:19:54,120 --> 00:19:55,740
由于法律的规定，

413
00:19:56,600 --> 00:19:59,520
我们是以私人方式进行这些工作，

414
00:19:59,640 --> 00:20:00,640
并且政府对此予以支持，

415
00:20:00,800 --> 00:20:02,100
因此我们直接进入乌克兰，

416
00:20:02,140 --> 00:20:02,980
随后战争开始了。

417
00:20:04,120 --> 00:20:06,140
不详细展开，

418
00:20:06,680 --> 00:20:07,480
但局势非常严峻。

419
00:20:07,540 --> 00:20:13,100
我认为，如果在五月或六月，俄罗斯

420
00:20:13,220 --> 00:20:17,900
如预期那样进行军事集结，乌克兰将会失去大片的领土， 

421
00:20:17,980 --> 00:20:18,940
并开始逐渐失去整个国家。

422
00:20:19,680 --> 00:20:21,200
所以情况非常严重。

423
00:20:21,720 --> 00:20:24,220
如果有人认识 Marjorie Taylor Greene，

424
00:20:24,800 --> 00:20:28,300
我建议你从通讯录中删除她。

425
00:20:28,940 --> 00:20:32,439
因为她就是那个，一个人阻止了

426
00:20:32,440 --> 00:20:35,200
数十亿美元援助

427
00:20:35,360 --> 00:20:36,920
这些援助本可以拯救一个重要的民主国家。

428
00:20:38,080 --> 00:20:40,640
- 我想谈谈一个稍微带有哲学性质的问题。

429
00:20:40,800 --> 00:20:43,300
去年你和亨利·基辛格

430
00:20:43,600 --> 00:20:46,020
以及丹·赫特洛克写了一篇

431
00:20:46,620 --> 00:20:48,940
关于知识本质及其演变的文章。

432
00:20:49,040 --> 00:20:50,920
前几天我也和别人讨论了这个话题。

433
00:20:51,360 --> 00:20:53,760
对于历史上的大部分时间，

434
00:20:54,000 --> 00:20:56,720
人类对宇宙的理解更多是神秘的，

435
00:20:57,060 --> 00:20:59,900
然后出现了科学革命和启蒙运动。

436
00:21:01,120 --> 00:21:03,740
而在你的文章中，你们提出了一个观点，现在的模型

437
00:21:03,900 --> 00:21:10,540
变得如此复杂和难以理解，以至于我们

438
00:21:10,600 --> 00:21:12,120
不再真正知道其中发生了什么。

439
00:21:12,600 --> 00:21:14,340
我要引用理查德·费曼的一句话。

440
00:21:14,460 --> 00:21:17,040
他说：“我不能理解我无法创造的东西”

441
00:21:17,040 --> 00:21:18,040
我最近看到这句话。

442
00:21:18,600 --> 00:21:21,280
但现在人们能够创造出一些东西，

443
00:21:21,340 --> 00:21:22,940
却并不真正理解其中的原理。

444
00:21:23,460 --> 00:21:25,840
知识的本质是否正在发生变化？

445
00:21:25,900 --> 00:21:27,620
我们是否要开始接受

446
00:21:27,720 --> 00:21:32,020
这些模型的结果，而不再需要它们解释给我们听？

447
00:21:32,340 --> 00:21:34,160
- 我想，可以将其比作青少年。

448
00:21:34,680 --> 00:21:37,280
如果你有个十来岁的孩子，你知道他们是人类，

449
00:21:37,440 --> 00:21:39,020
但你却无法完全理解他们的想法。

450
00:21:39,340 --> 00:21:40,140
（笑）

451
00:21:40,800 --> 00:21:42,540
然而，我们的社会已经适应了

452
00:21:42,700 --> 00:21:44,780
青少年的存在，对吧？

453
00:21:44,820 --> 00:21:46,020
他们总会长大成人。

454
00:21:46,860 --> 00:21:47,980
我是认真的。

455
00:21:48,820 --> 00:21:51,960
因此，我们可能会有

456
00:21:52,300 --> 00:21:55,260
无法完全理解的知识系统，

457
00:21:56,440 --> 00:21:58,540
但我们了解它们的边界，对吗？

458
00:21:58,840 --> 00:22:00,700
我们理解它们的能力范围。

459
00:22:01,220 --> 00:22:03,540
这可能是我们能够获得的最好结果。

460
00:22:03,580 --> 00:22:04,740
- 你认为我们能理解这些边界吗？

461
00:22:05,960 --> 00:22:07,360
- 我们会变得越来越好。

462
00:22:07,640 --> 00:22:11,440
我每周都会和我的团队会面，

463
00:22:12,120 --> 00:22:14,340
我们的共识是，最终，你会使用

464
00:22:15,100 --> 00:22:21,880
所谓的对抗性 AI，实际上会有一些公司，你可以雇用他们，付钱让他们

465
00:22:22,380 --> 00:22:23,940
去破坏你的 AI 系统。

466
00:22:23,960 --> 00:22:24,560
- 就像网络安全中的红队一样。

467
00:22:24,720 --> 00:22:26,640
- 那么，将会是 AI 红队，而不是现在的人类红队，

468
00:22:26,800 --> 00:22:30,000
你将会看到整个公司

469
00:22:30,080 --> 00:22:33,980
和行业的 AI 系统，它们的任务是挖掘

470
00:22:34,100 --> 00:22:36,360
现有 AI 系统的漏洞，

471
00:22:36,980 --> 00:22:39,580
特别是那些我们无法理解的知识点。

472
00:22:40,260 --> 00:22:41,100
我认为这个观点是有道理的。

473
00:22:41,200 --> 00:22:43,760
对于斯坦福来说，这也是一个很好的项目，

474
00:22:44,440 --> 00:22:46,920
因为如果有一个研究生能够弄清楚

475
00:22:46,920 --> 00:22:49,220
如何攻击这些大型模型

476
00:22:49,340 --> 00:22:53,260
并理解它们的运作，那将为下一代技术积累的宝贵经验。

477
00:22:53,800 --> 00:22:56,680
我觉得这两者会齐头并进。

478
00:22:57,480 --> 00:22:58,920
- 好吧，让我们听听学生的问题。

479
00:22:59,040 --> 00:22:59,900
后面有一个同学，

480
00:22:59,980 --> 00:23:00,400
先说下你的名字。

481
00:23:01,700 --> 00:23:04,240
- 之前你提到过，这点和你刚才的评论有关，

482
00:23:04,320 --> 00:23:07,300
需要让 AI 真正按照我们的意愿行事。

483
00:23:07,580 --> 00:23:11,600
你刚刚提到了对抗性 AI，我想如果你能再详细说明一下就更好了。

484
00:23:11,780 --> 00:23:15,140
所以，除了计算能力肯定会增加，

485
00:23:15,200 --> 00:23:18,420
我们可以有更强大的模型，但是如何让它们

486
00:23:18,520 --> 00:23:20,120
真正按照我们的意愿行动，这个问题你觉得解决了吗？

487
00:23:20,500 --> 00:23:22,360
在我看来，这个问题似乎还没有完全解答。

488
00:23:23,180 --> 00:23:29,600
- 好吧，你必须认为，当前的问题会随着技术的进步而减少，对吗？

489
00:23:29,980 --> 00:23:33,320
我并不是说这些问题会完全消失。

490
00:23:34,280 --> 00:23:36,820
然后你还需要假设，我们会有

491
00:23:36,960 --> 00:23:37,640
对 AI 有效性的测试方法。

492
00:23:38,160 --> 00:23:40,740
因此，我们必须有一种方式，去确认事情是否成功。

493
00:23:41,620 --> 00:23:43,920
举刚才我提到的 TikTok 竞争对手的例子，

494
00:23:44,080 --> 00:23:47,700
我并不是建议你非法盗用其他人的音乐。

495
00:23:48,320 --> 00:23:50,220
如果你是硅谷的企业家，希望你们都能成为的话，

496
00:23:50,520 --> 00:23:52,900
如果这个产品火了，

497
00:23:52,960 --> 00:23:55,620
那么你就需要雇一大堆律师来收拾烂摊子，

498
00:23:56,280 --> 00:23:56,420
是吧？

499
00:23:56,960 --> 00:23:58,680
但如果没有人使用你的产品，

500
00:23:58,860 --> 00:24:00,980
那你盗用了所有内容也无关紧要。

501
00:24:01,240 --> 00:24:02,340
当然，不要引用我的话。

502
00:24:02,900 --> 00:24:03,020
是吧？

503
00:24:03,580 --> 00:24:03,720
（观众笑了）

504
00:24:04,300 --> 00:24:05,300
- 对，你正在被摄影机记录。

505
00:24:05,420 --> 00:24:06,000
- 是的，没错。

506
00:24:06,480 --> 00:24:06,520
（观众笑了）

507
00:24:07,200 --> 00:24:08,180
但你明白我的意思。

508
00:24:08,400 --> 00:24:10,780
换句话说，硅谷会进行这样的尝试，

509
00:24:10,820 --> 00:24:11,860
并收拾残局，

510
00:24:12,360 --> 00:24:13,920
这通常就是事情的运作方式。

511
00:24:14,540 --> 00:24:21,439
所以在我看来，你会看到越来越多的性能系统，甚至有更好的测试，

512
00:24:21,440 --> 00:24:22,860
最终会有对抗性测试，

513
00:24:23,240 --> 00:24:24,680
这将保证它们的行为在一个可控的范围内。

514
00:24:25,580 --> 00:24:27,600
我们通常把这称为链式思维推理。

515
00:24:28,400 --> 00:24:31,120
人们相信，在未来几年里，

516
00:24:31,300 --> 00:24:33,520
你将能够实现长达 1000 步的

517
00:24:33,580 --> 00:24:34,540
思维链条推理。

518
00:24:35,580 --> 00:24:36,400
做这个，再做那个。

519
00:24:36,440 --> 00:24:37,660
就好比是在制作菜谱。

520
00:24:38,480 --> 00:24:40,680
你可以照着菜谱做，

521
00:24:40,820 --> 00:24:43,560
并测试它是否产生了正确的结果。

522
00:24:44,100 --> 00:24:45,400
这就是系统运作的方式。

523
00:24:45,460 --> 00:24:45,720
有问题吗，先生？

524
00:24:50,200 --> 00:24:53,800
- 总的来说，你对 AI 进步的潜力似乎持有非常积极的态度。

525
00:24:54,240 --> 00:24:56,280
我很好奇，你认为什么会推动这个进步呢？

526
00:24:56,640 --> 00:24:57,680
是更强大的计算力量？

527
00:24:57,840 --> 00:24:58,540
还是更多的数据？

528
00:24:58,880 --> 00:25:01,100
或是基础性或实质性的变化？

529
00:25:02,080 --> 00:25:04,820
是的。

530
00:25:04,820 --> 00:25:09,160
现在投入的资金数额简直令人难以置信。

531
00:25:10,380 --> 00:25:13,540
我选择投资于几乎所有领域， 

532
00:25:13,540 --> 00:25:15,380
因为我无法确定谁会是最后的赢家。

533
00:25:16,420 --> 00:25:20,180
跟随我投资的资金规模非常庞大，

534
00:25:20,180 --> 00:25:23,900
我觉得这部分原因是早期投资的回报已经出现，

535
00:25:23,900 --> 00:25:25,080
那些大资金的投资者，

536
00:25:25,080 --> 00:25:26,260
他们对他们自己在做什么并不清楚，

537
00:25:26,260 --> 00:25:28,020
他们只知道他们的投资中必须包含 AI 成分。

538
00:25:28,960 --> 00:25:31,820
现在所有的投资看似都与 AI 有关，所以他们无法区分哪些是真正的 AI 投资。

539
00:25:32,020 --> 00:25:35,480
我把 AI 定义为能够学习的系统，真正能够自我学习的系统。

540
00:25:35,700 --> 00:25:36,480
所以我认为这是其中的一个驱动力。

541
00:25:37,160 --> 00:25:40,500
另一个驱动力是，有许多非常复杂的新算法，

542
00:25:40,500 --> 00:25:42,440
这些算法可以说是超越了 Transformer。

543
00:25:42,920 --> 00:25:45,120
我的朋友，也是长期的合作伙伴，

544
00:25:45,260 --> 00:25:47,520
创造了一种新的非 Transformer 架构。

545
00:25:48,080 --> 00:25:49,560
我在巴黎资助的一个团队

546
00:25:49,560 --> 00:25:51,320
声称他们也做出了相同的成果。

547
00:25:51,700 --> 00:25:55,580
这里有巨大的发明潜力，斯坦福大学也有很多创新。

548
00:25:56,500 --> 00:25:57,420
最后一点是，

549
00:25:57,420 --> 00:25:59,860
市场普遍认为

550
00:25:59,860 --> 00:26:03,540
智能的发明将带来无尽的回报。

551
00:26:04,440 --> 00:26:08,900
比如说，你投资了一家公司 500 亿美元，

552
00:26:09,740 --> 00:26:12,740
你必须从智能中获得大量的收益才能回本。

553
00:26:13,320 --> 00:26:17,320
因此，我们可能会经历一场大的投资泡沫，

554
00:26:17,320 --> 00:26:19,180
最终市场会自行调整。

555
00:26:19,280 --> 00:26:23,160
这在过去一直如此，而这一次也极有可能是如此。

556
00:26:23,760 --> 00:26:25,120
你之前提到，

557
00:26:25,120 --> 00:26:27,820
你认为领先者正在逐渐拉开与其他人的差距，

558
00:26:27,820 --> 00:26:28,560
现在。

559
00:26:28,780 --> 00:26:35,180
这个问题可以这样理解，

560
00:26:35,380 --> 00:26:37,200
有一家叫做 Mistral 的法国公司，

561
00:26:37,340 --> 00:26:38,360
他们的表现非常出色，

562
00:26:39,840 --> 00:26:40,980
我也是他们的投资者。

563
00:26:42,160 --> 00:26:44,000
他们已经推出了第二个版本，

564
00:26:44,340 --> 00:26:47,540
但由于成本过高，第三个版本可能无法推出。

565
00:26:48,180 --> 00:26:48,920
他们需要收入，

566
00:26:49,760 --> 00:26:51,300
不能免费提供模型。

567
00:26:51,860 --> 00:26:56,280
所以，我们行业中关于开源与闭源的争论非常激烈。

568
00:26:57,120 --> 00:26:57,440
而

569
00:26:57,820 --> 00:27:00,700
我的整个职业生涯都是基于人们

570
00:27:00,700 --> 00:27:03,840
愿意共享开源的软件。

571
00:27:04,100 --> 00:27:05,920
我的一切都与开源有关。

572
00:27:06,440 --> 00:27:09,140
谷歌的许多基础设施都是开源的。

573
00:27:09,260 --> 00:27:10,340
我在技术上所做的一切也是如此。

574
00:27:11,040 --> 00:27:14,860
然而，可能是因为资本成本如此巨大，

575
00:27:15,760 --> 00:27:17,480
这从根本上改变了软件开发的方式。

576
00:27:17,900 --> 00:27:18,560
你和我之前谈到的，

577
00:27:20,260 --> 00:27:23,820
我认为软件程序员的生产率

578
00:27:23,820 --> 00:27:24,480
至少会提高一倍。

579
00:27:24,820 --> 00:27:28,660
有三四家软件公司正在努力实现这一目标。

580
00:27:28,740 --> 00:27:29,800
我投资了所有这些公司。

581
00:27:30,900 --> 00:27:31,280
本着这种精神。

582
00:27:31,960 --> 00:27:34,840
他们都在努力提高软件程序员的效率。

583
00:27:35,000 --> 00:27:37,340
我最近接触的一个最有趣的公司叫 Augment。

584
00:27:38,220 --> 00:27:40,140
我总是把它想象成单个程序员，

585
00:27:40,240 --> 00:27:41,320
但他们告诉我，“我们的目标并不是他。

586
00:27:41,420 --> 00:27:44,380
我们的目标是那些上 100 人的开发团队，

587
00:27:44,380 --> 00:27:46,120
代码行数达到数百万行，

588
00:27:46,120 --> 00:27:47,600
谁也不知道里面发生了什么。”

589
00:27:48,000 --> 00:27:50,180
这确实是一个非常适合 AI 解决的问题。

590
00:27:50,240 --> 00:27:50,960
他们能赚钱吗？

591
00:27:51,420 --> 00:27:51,920
我希望他们能。

592
00:27:53,240 --> 00:27:54,440
有很多问题需要解答。

593
00:27:54,560 --> 00:27:54,860
是的，女士。

594
00:27:55,260 --> 00:27:55,400
完美。

595
00:27:55,600 --> 00:27:55,800
你好。

596
00:27:56,000 --> 00:27:57,080
你一开始提到，

597
00:27:57,200 --> 00:28:03,800
上下文窗口的扩展、

598
00:28:04,480 --> 00:28:08,120
“智能体”和“将文本转换为行动指令”的组合将产生无法想象的影响。

599
00:28:09,180 --> 00:28:11,440
首先，为什么这种组合如此重要呢？

600
00:28:11,800 --> 00:28:16,920
其次，我知道你没有水晶球，不能预测未来。

601
00:28:17,000 --> 00:28:19,780
但为什么你认为这些影响将超出我们的想象？

602
00:28:22,820 --> 00:28:25,260
这有助于解决新“时效性”问题。

603
00:28:26,000 --> 00:28:31,020
目前的模型需要训练大约一年半的时间。

604
00:28:31,200 --> 00:28:33,280
包括六个月的准备，六个月的训练，

605
00:28:33,760 --> 00:28:34,640
六个月的微调。

606
00:28:35,060 --> 00:28:35,960
所以它们总是过时的。

607
00:28:37,060 --> 00:28:39,880
通过扩展上下文窗口，你可以提供最新的信息。

608
00:28:40,180 --> 00:28:45,700
例如，你可以在上下文中询问关于哈马斯与以色列战争的问题。

609
00:28:45,780 --> 00:28:46,720
这是非常强大的。

610
00:28:47,100 --> 00:28:48,540
它让模型能够像谷歌一样保持时效性。

611
00:28:49,520 --> 00:28:51,220
关于智能体，我给你一个例子。

612
00:28:51,940 --> 00:28:54,860
我成立了一个基金会，该基金会资助了一个非营利组织。

613
00:28:55,120 --> 00:28:59,760
首先，如果在场有化学家，我并不真正了解化学。

614
00:29:00,660 --> 00:29:03,520
有一个叫做 ChemCrow 的工具，

615
00:29:04,300 --> 00:29:07,140
它是一个基于大语言模型的系统，学习了化学知识。

616
00:29:07,840 --> 00:29:10,220
他们使用这个系统生成关于蛋白质的化学假设，

617
00:29:10,460 --> 00:29:13,080
然后在实验室测试一晚上，

618
00:29:13,660 --> 00:29:16,980
之后它会学习。

619
00:29:17,520 --> 00:29:22,080
这大大加速了化学、材料科学等领域的研究进程。

620
00:29:22,500 --> 00:29:24,440
这就是智能体模型的一个例子。

621
00:29:25,240 --> 00:29:30,120
然后对于“文本转换为行动指令”，可以理解为你拥有大量廉价程序员，对吧？

622
00:29:31,400 --> 00:29:33,640
我们可能还不清楚会发生什么——

623
00:29:33,640 --> 00:29:35,940
这是你的专长领域——

624
00:29:35,940 --> 00:29:38,100
当每个人都有自己的程序员时会发生什么。

625
00:29:38,540 --> 00:29:40,560
我并不是在谈论开灯和关灯的事情。

626
00:29:41,560 --> 00:29:44,220
想象一下——再举一个例子——

627
00:29:44,220 --> 00:29:45,580
比如，你不喜欢 Google。

628
00:29:46,120 --> 00:29:48,440
于是你说，给我打造一个可以与 Google 竞争的搜索引擎。

629
00:29:48,700 --> 00:29:50,560
没错，你一个人就能打造一个能与 Google 抗衡的搜索引擎，

630
00:29:52,080 --> 00:29:55,840
搜索网页，有一个用户界面，照原样山寨一份，

631
00:29:57,220 --> 00:29:59,240
以一种有趣的方式在其中融入生成式 AI，

632
00:29:59,680 --> 00:30:02,880
用 30 秒钟完成，看看效果如何。

633
00:30:05,900 --> 00:30:09,100
很多人因此相信，包括

634
00:30:09,360 --> 00:30:12,940
Google 在内的许多大公司，可能会受到这种方式的威胁。

635
00:30:13,040 --> 00:30:13,460
我们拭目以待。

636
00:30:14,360 --> 00:30:16,260
有很多通过 Slido 提交的问题，

637
00:30:16,460 --> 00:30:17,520
有些得到了投票。

638
00:30:18,020 --> 00:30:20,640
这里有一个，我们去年讨论过这个问题。

639
00:30:21,420 --> 00:30:23,740
我们如何阻止 AI 影响公众舆论，

640
00:30:24,080 --> 00:30:26,060
制造假消息，特别是在即将到来的选举期间？

641
00:30:26,660 --> 00:30:28,360
有什么短期和长期的解决方案呢？

642
00:30:30,220 --> 00:30:33,140
- 在这次即将到来的选举中，

643
00:30:33,320 --> 00:30:34,880
以及在全球范围内，大部分的假消息都会出现在社交媒体上。

644
00:30:35,620 --> 00:30:39,420
而社交媒体公司没有足够的组织能力来管理这些问题。 

645
00:30:40,240 --> 00:30:41,600
以 TikTok 为例，

646
00:30:42,120 --> 00:30:47,200
有很多指控称 TikTok 偏袒某些类型的误导信息，而忽略其他类型的误导信息。

647
00:30:47,660 --> 00:30:53,460
许多人声称中国强迫他们这么做，但我没有看到任何证据。

648
00:30:54,140 --> 00:30:55,660
我觉得这是个混乱的局面。

649
00:30:56,380 --> 00:31:00,800
我们这个国家必须学会批判性思考。

650
00:31:02,340 --> 00:31:04,620
这对美国来说可能是个不可能的挑战。

651
00:31:05,240 --> 00:31:07,340
但是，别人告诉你的，

652
00:31:07,520 --> 00:31:08,560
并不一定就是真的。

653
00:31:09,120 --> 00:31:11,760
事情可能会走向另一个极端，导致

654
00:31:11,780 --> 00:31:13,760
真实的信息也不再被人们相信吗？

655
00:31:13,860 --> 00:31:16,400
有些人称这为认识论危机，

656
00:31:16,620 --> 00:31:21,480
你们都知道，比如马斯克就说过，“我从未做过那件事。

657
00:31:21,880 --> 00:31:22,320
证明它。”

658
00:31:23,220 --> 00:31:24,220
- 以唐纳德·特朗普为例。

659
00:31:25,200 --> 00:31:25,640
- 好的。

660
00:31:25,840 --> 00:31:28,860
- 我认为我们社会存在信任问题。

661
00:31:28,980 --> 00:31:30,360
民主制度有时也会失败。

662
00:31:31,120 --> 00:31:33,720
我认为对民主制度最大的威胁

663
00:31:33,900 --> 00:31:37,200
是虚假的信息，因为我们会变得非常擅长制造这些虚假信息。

664
00:31:38,280 --> 00:31:41,720
当我管理 YouTube 的时候，我们在 YouTube 上面临的最大问题是

665
00:31:41,760 --> 00:31:45,260
有人上传虚假的视频，

666
00:31:45,260 --> 00:31:46,780
然后有人因此受害甚至丧生。

667
00:31:46,980 --> 00:31:48,820
我们有一个禁止涉及死亡信息的政策，令人震惊。

668
00:31:49,780 --> 00:31:53,740
我们致力于解决这个问题，但这是一项艰巨的任务。

669
00:31:53,880 --> 00:31:55,360
而且，这还是在生成式 AI 出现之前的事情。

670
00:31:56,840 --> 00:31:59,080
- 嗯，所以-- - 我没有好的答案。

671
00:31:59,940 --> 00:32:01,080
- 有一个技术解决方案，这并不是答案，

672
00:32:01,160 --> 00:32:02,360
但可能有助于缓解这个问题，

673
00:32:02,520 --> 00:32:03,860
我不明白为什么这种方法没有被更广泛使用，

674
00:32:04,000 --> 00:32:05,640
那就是公钥认证。

675
00:32:05,800 --> 00:32:07,360
当乔·拜登发表演讲时，

676
00:32:07,580 --> 00:32:09,740
为什么不像 SSL 那样进行数字签名呢？

677
00:32:09,900 --> 00:32:14,280
或者，比如那些名人或公众人物，

678
00:32:14,280 --> 00:32:16,820
他们不能有一个公钥吗？

679
00:32:17,220 --> 00:32:19,060
- 对，这就是公钥的一种形式，

680
00:32:19,360 --> 00:32:22,000
然后有一种确定的方式了解系统如何--

681
00:32:22,140 --> 00:32:23,380
- 嘿，当我向亚马逊发送我的

682
00:32:23,580 --> 00:32:25,380
信用卡信息时，我知道这是亚马逊。

683
00:32:25,560 --> 00:32:29,520
- 我和乔纳森·海德一起撰写并发表了一篇论文，

684
00:32:29,580 --> 00:32:32,180
他一直在研究生成焦虑问题。

685
00:32:33,180 --> 00:32:35,120
这篇论文并未产生任何影响。

686
00:32:36,700 --> 00:32:39,460
他是个非常出色的传播者，我可能不如他。

687
00:32:40,080 --> 00:32:44,920
所以我得出的结论是，系统没有按照你所说的那样组织起来。

688
00:32:45,040 --> 00:32:47,060
- 你有一篇论文赞同我们的做法。

689
00:32:47,500 --> 00:32:48,300
- 也即是对你的建议的赞同。

690
00:32:48,540 --> 00:32:48,580
- 好的。

691
00:32:48,840 --> 00:32:49,220
我的话筒。

692
00:32:49,480 --> 00:32:50,460
- 不，是你刚才说的那个问题。

693
00:32:50,560 --> 00:32:50,880
- 是的，没错。

694
00:32:51,140 --> 00:32:56,100
- 我的结论是，总的来说，CEO 们都在努力最大化收入。

695
00:32:56,580 --> 00:32:58,760
为了最大化收入，他们最大化用户参与度。

696
00:32:59,160 --> 00:33:01,320
为了最大化用户参与度，他们最大化煽动性内容。

697
00:33:01,960 --> 00:33:05,920
算法选择煽动性内容，因为这会带来更多的收入，对吧？

698
00:33:06,640 --> 00:33:07,140
由此，

699
00:33:07,360 --> 00:33:09,480
算法更倾向于推荐些极端的内容。

700
00:33:10,080 --> 00:33:10,980
这并不是单方面的问题。

701
00:33:11,100 --> 00:33:12,500
我并不是在这里做出任何偏袒的声明。

702
00:33:13,460 --> 00:33:14,260
这是一个问题。

703
00:33:14,440 --> 00:33:15,380
这个问题必须得到解决。

704
00:33:15,560 --> 00:33:18,940
在一个民主制度中，我对 TikTok 的解决方案是，

705
00:33:19,160 --> 00:33:23,120
我们之前私下讨论过，在我年轻时，

706
00:33:23,280 --> 00:33:24,660
有一个叫做 "平等播放时间"的规则。

707
00:33:25,500 --> 00:33:28,560
因为 TikTok 实际上并不仅仅是社交媒体，它更像是电视，对吧？

708
00:33:29,020 --> 00:33:33,760
有程序在幕后控制你。根据统计，TikTok 在美国的用户每天观看 90 分钟，

709
00:33:33,960 --> 00:33:38,220
每个用户平均观看 200 个 TikTok 视频。 

710
00:33:38,540 --> 00:33:39,700
这是很大的数量，对吧？

711
00:33:40,460 --> 00:33:45,900
因此，政府必须采取某种形式的平衡措施。

712
00:33:46,140 --> 00:33:47,200
好的，我们继续回答几个问题。

713
00:33:49,800 --> 00:33:50,560
两个简短的问题。

714
00:33:51,240 --> 00:33:59,040
 一，大语言模型的经济影响，比如劳动力市场的影响，比你最初预期的要慢，

715
00:33:59,440 --> 00:34:01,860
Chegg （一家在线教育技术公司）和其中的一些服务人员也受到了影响。

716
00:34:02,760 --> 00:34:07,940
你认为学术界应该获得 AI 方面的补贴，

717
00:34:07,940 --> 00:34:09,460
还是你认为他们应该

718
00:34:09,280 --> 00:34:10,900
选择与业界的大公司合作？

719
00:34:11,480 --> 00:34:15,800
- 我非常努力地推动为大学建立数据中心。

720
00:34:16,060 --> 00:34:18,400
如果我在这里的计算机科学系任教，

721
00:34:18,520 --> 00:34:22,480
我会非常失望，因为我无法

722
00:34:22,639 --> 00:34:26,460
与我的研究生一起构建能够进行博士研究的算法。

723
00:34:26,920 --> 00:34:28,840
我被迫和这些公司合作。

724
00:34:29,340 --> 00:34:30,860
我认为，这些公司在这方面

725
00:34:30,960 --> 00:34:33,100
并没有表现出足够的慷慨。

726
00:34:33,600 --> 00:34:35,480
我和许多你们认识的教师交谈，

727
00:34:35,600 --> 00:34:38,120
他们花费大量时间等待

728
00:34:38,120 --> 00:34:39,960
Google Cloud 的配额。

729
00:34:40,840 --> 00:34:41,760
这是非常糟糕的情况。

730
00:34:42,219 --> 00:34:43,239
这个领域正在快速发展。

731
00:34:43,420 --> 00:34:44,540
我们希望美国能胜出。

732
00:34:45,000 --> 00:34:48,199
我们希望美国的大学——出于多种原因，

733
00:34:48,280 --> 00:34:49,800
我认为正确的做法是把资源提供给他们。

734
00:34:50,179 --> 00:34:51,800
我在这方面做了很多努力。

735
00:34:52,260 --> 00:34:54,100
你的第一个问题是关于劳动力市场的影响？

736
00:34:55,380 --> 00:34:57,380
我将这个问题交给这里的真正专家。

737
00:34:58,100 --> 00:35:00,680
作为 Eric 教导的业余经济学家，

738
00:35:02,040 --> 00:35:07,040
我坚信那些接受过大学教育、

739
00:35:07,200 --> 00:35:08,720
从事高技能工作的人将会适应，

740
00:35:09,200 --> 00:35:10,740
因为人们将会与这些系统一起工作。

741
00:35:11,440 --> 00:35:14,560
我认为这些系统与以往的技术浪潮没有区别。

742
00:35:15,040 --> 00:35:20,400
那些危险的工作和不需要太多人为判断的工作将被取代。

743
00:35:21,000 --> 00:35:22,360
- 我们还剩下大约五分钟，

744
00:35:22,420 --> 00:35:23,860
所以让我们快速回答一些问题。

745
00:35:23,920 --> 00:35:25,120
我会让 Eric 来挑选问题。

746
00:35:25,320 --> 00:35:25,680
- 是的，女士。

747
00:35:28,040 --> 00:35:35,560
- 我对“文本转换为行动指令”及其对计算机科学教育的影响非常感兴趣。

748
00:35:36,160 --> 00:35:43,800
我想知道你对计算机科学教育应该如何变革以适应新时代有什么看法。

749
00:35:44,160 --> 00:35:44,380
好的，

750
00:35:44,500 --> 00:35:51,600
嗯，我认为计算机科学专业的本科生群体将始终有一位编程搭档。

751
00:35:51,920 --> 00:35:55,720
因此，当你学习你的第一个 for 循环这些的时候，

752
00:35:56,280 --> 00:35:59,000
你会有一个工具作为你的伙伴。

753
00:35:59,160 --> 00:36:02,680
这就是未来的教学方式，教授

754
00:36:03,320 --> 00:36:06,440
会讲解概念，但你会通过这种方式参与其中。

755
00:36:06,500 --> 00:36:07,140
这是我的猜测。

756
00:36:08,360 --> 00:36:09,040
是的，女士，你在后面。

757
00:36:10,260 --> 00:36:13,617
- 你谈到一些让你兴奋的非 Transformer 架构。 

758
00:36:13,617 --> 00:36:18,864
我想其中一个被提到的是状态模型，但现在还有一种是更长上下文的模型。

759
00:36:18,864 --> 00:36:23,160
我更好奇的是你在这方面看到的情况。

760
00:36:23,160 --> 00:36:26,000
- 我对数学的理解还不够深入。

761
00:36:26,220 --> 00:36:30,540
我非常高兴我们为数学家们提供了工作机会。

762
00:36:31,660 --> 00:36:33,560
因为这里的数学太复杂了。

763
00:36:34,060 --> 00:36:37,400
但基本上，他们是在采用不同的方法来进行梯度下降

764
00:36:37,840 --> 00:36:40,280
和矩阵乘法，使其更快更好。

765
00:36:41,360 --> 00:36:42,400
而 Transformer，你知道，

766
00:36:42,400 --> 00:36:45,500
是一种能够同时进行乘法运算的系统化方式，

767
00:36:45,560 --> 00:36:46,780
这是我理解的方式。

768
00:36:47,240 --> 00:36:48,180
它和其他的很像。

769
00:36:48,280 --> 00:36:48,980
但数学部分不同。

770
00:36:49,760 --> 00:36:51,020
让我们看看，这边，是的，先生。

771
00:36:51,200 --> 00:36:51,380
请说。

772
00:36:52,360 --> 00:36:53,200
- 是的，你。

773
00:36:53,420 --> 00:36:53,500
- 是的。

774
00:36:53,900 --> 00:36:56,560
你在关于国家安全的论文中提到，

775
00:36:56,980 --> 00:37:00,966
中美两国在现代架构的帮助下处于关键地位。

776
00:37:00,966 --> 00:37:03,000
接下来的10个国家，以及稍后的一组国家，

777
00:37:03,120 --> 00:37:08,300
都是美国的盟友，或者与美国盟友关系密切。

778
00:37:08,840 --> 00:37:11,760
我很好奇你对这10个国家，

779
00:37:12,040 --> 00:37:14,160
尤其是那些中间地带但并非正式盟友的国家有什么看法。

780
00:37:15,940 --> 00:37:23,780
它们有多大可能愿意加入我们的安全阵营？

781
00:37:23,980 --> 00:37:27,320
又是什么因素可能会阻碍它们加入？

782
00:37:27,680 --> 00:37:29,520
- 最值得关注的国家是印度，

783
00:37:30,320 --> 00:37:33,420
因为顶尖的 AI 人才从印度来到美国，

784
00:37:34,240 --> 00:37:36,700
我们应该让印度保留一部分顶尖人才。

785
00:37:36,860 --> 00:37:38,200
并不是所有，但有一些。

786
00:37:39,340 --> 00:37:43,280
而且，他们没有我们这里如此丰富的培训设施和项目。

787
00:37:43,700 --> 00:37:46,200
在我看来，印度是这一领域的重要摇摆国家。

788
00:37:46,440 --> 00:37:48,700
中国已经没有机会了，不会再回来。

789
00:37:49,320 --> 00:37:52,300
无论人们多么希望他们改变政权，这都不太可能发生。

790
00:37:53,000 --> 00:37:55,380
日本和韩国显然站在我们这一边。

791
00:37:56,180 --> 00:37:59,580
台湾很棒，但他们的软件开发很差，

792
00:38:00,140 --> 00:38:01,620
所以这方面不可行。

793
00:38:02,740 --> 00:38:03,440
不过他们的硬件很出色。

794
00:38:04,160 --> 00:38:05,780
在全球范围内，

795
00:38:05,840 --> 00:38:07,900
其他大国的选择并不多。

796
00:38:09,300 --> 00:38:11,300
欧洲因为欧盟的政策和管理机制已经陷入困境。

797
00:38:11,740 --> 00:38:12,640
这并不是什么新鲜事。

798
00:38:12,760 --> 00:38:14,120
我曾与他们斗争了 10 年。

799
00:38:14,840 --> 00:38:19,340
我付出了巨大的努力，希望他们能修正欧盟法案。

800
00:38:19,540 --> 00:38:21,120
但他们仍然有许多限制，

801
00:38:21,280 --> 00:38:23,940
导致我们在欧洲开展研究非常困难。

802
00:38:24,700 --> 00:38:27,480
我的法国朋友们把所有时间都花在与欧盟的斗争上，

803
00:38:27,820 --> 00:38:29,540
马克龙，作为我的好朋友，

804
00:38:30,480 --> 00:38:31,820
正在全力以赴地推进这个事情。

805
00:38:31,880 --> 00:38:33,260
所以，我认为法国还有机会。

806
00:38:33,800 --> 00:38:35,000
我并不看好德国的前景。

807
00:38:35,140 --> 00:38:36,280
其余的国家都不够有影响力。

808
00:38:37,960 --> 00:38:38,240
请问下一位。

809
00:38:38,420 --> 00:38:38,700
请讲，女士。

810
00:38:40,100 --> 00:38:42,440
- 我是一名编译器工程师。

811
00:38:44,120 --> 00:38:48,260
考虑到你设想这些模型将具备的能力，

812
00:38:48,440 --> 00:38:50,220
我们还需要花时间学习编程吗？

813
00:38:50,480 --> 00:38:53,260
- 是的，这其实是一个老生常谈的问题，

814
00:38:53,360 --> 00:38:55,380
为什么你已经会说英语还要学习英语呢？

815
00:38:56,180 --> 00:38:56,940
是为了让你的英语更好。

816
00:38:57,680 --> 00:39:00,040
你确实需要理解这些系统的工作原理，

817
00:39:00,100 --> 00:39:01,340
我对此非常坚定——先生，你有什么问题吗？

818
00:39:01,760 --> 00:39:05,740
- 我很好奇你是否探索过分布式环境，

819
00:39:05,880 --> 00:39:09,040
我之所以问这个问题是因为，虽然构建大型集群很困难，

820
00:39:09,100 --> 00:39:10,720
但是 MacBooks 的性能很强大。

821
00:39:10,860 --> 00:39:12,820
世界各地有很多小型机器。

822
00:39:13,360 --> 00:39:17,580
所以你认为类似“Folding@home”的想法适合训练吗？

823
00:39:18,040 --> 00:39:18,040
- 是的。

824
00:39:18,820 --> 00:39:19,980
是的，我们对此进行了深入研究。

825
00:39:20,680 --> 00:39:22,620
算法的工作方式是你有

826
00:39:22,680 --> 00:39:26,380
一个非常大的矩阵，基本上是一个乘法函数。

827
00:39:27,020 --> 00:39:30,560
可以想象它在不断地来回运算。

828
00:39:31,140 --> 00:39:33,180
这些系统的速度完全受限于

829
00:39:33,340 --> 00:39:36,280
内存到 CPU 或 GPU 的传输速度。

830
00:39:37,260 --> 00:39:43,200
事实上，Nvidia 的下一代芯片已经将所有这些功能整合到了一块芯片中。

831
00:39:43,840 --> 00:39:46,220
如今的芯片已经足够大到需要将它们都粘合在一起。

832
00:39:47,040 --> 00:39:49,460
事实上，芯片的封装非常敏感，

833
00:39:49,680 --> 00:39:52,780
芯片的封装和芯片本身都需要在无尘室中进行。

834
00:39:53,640 --> 00:39:57,620
所以，看起来超级计算和光速计算，

835
00:39:57,960 --> 00:40:00,340
特别是内存互连，占据了主导地位。

836
00:40:00,520 --> 00:40:02,040
因此，我认为短时间内不太可能发生什么变化。

837
00:40:02,180 --> 00:40:04,040
- 那么有没有办法将大语言模型进行分段处理？

838
00:40:04,180 --> 00:40:06,320
所以，杰夫·迪恩在去年的演讲中

839
00:40:06,640 --> 00:40:11,940
谈到过一种将不同部分分别训练然后再进行联邦式训练的方法。

840
00:40:12,640 --> 00:40:14,860
- 为了实现这一点，

841
00:40:14,920 --> 00:40:16,620
你需要拥有一千万个这样的部分，

842
00:40:16,860 --> 00:40:19,660
但这样一来，提问的过程会变得非常缓慢。

843
00:40:20,340 --> 00:40:22,460
他谈到的是使用八台、十台或十二台超级计算机。

844
00:40:22,520 --> 00:40:22,960
- 是的，是的，没错，没错。

845
00:40:22,960 --> 00:40:23,960
- 是的，不是在 MacBooks 这种水平上。

846
00:40:23,980 --> 00:40:24,580
- 没有达到他的水平。

847
00:40:24,620 --> 00:40:25,220
- 是的，明白了。

848
00:40:25,240 --> 00:40:25,820
- 他在后面吗？

849
00:40:25,860 --> 00:40:26,440
是的，就在后方。

850
00:40:27,200 --> 00:40:29,800
- 我知道在 ChatGPT 发布后，

851
00:40:29,840 --> 00:40:32,900
OpenAI 因为使用他们的作品进行训练而被《纽约时报》起诉。

852
00:40:33,080 --> 00:40:34,220
你觉得这种情况会如何发展，

853
00:40:34,240 --> 00:40:35,400
这对数据隐私有何影响？

854
00:40:36,040 --> 00:40:38,340
- 我曾经从事过大量的音乐版权工作，

855
00:40:38,560 --> 00:40:41,200
我了解到在 60 年代，

856
00:40:41,500 --> 00:40:50,220
有一系列诉讼最终达成了一项协议：每次播放你的歌曲，你都能获得一定的版权费。

857
00:40:50,980 --> 00:40:52,640
他们甚至不需要知道你是谁。

858
00:40:52,660 --> 00:40:53,720
这笔钱就直接打入了一个银行账户。

859
00:40:54,220 --> 00:40:55,600
我猜可能会有类似的情况。

860
00:40:55,660 --> 00:40:58,560
可能会有很多诉讼，最后可能会达成一种协议，

861
00:40:59,140 --> 00:41:03,600
要求你必须支付你的一部分收入，

862
00:41:03,600 --> 00:41:05,560
以便使用 ASCAP EMI。

863
00:41:06,320 --> 00:41:08,120
关于 ASCAP BMI，你可以查一下，这个话题可能有些久远，

864
00:41:08,460 --> 00:41:09,540
对你来说可能会感觉很陈旧，

865
00:41:09,620 --> 00:41:11,020
但我认为这就是最终的结果。

866
00:41:11,520 --> 00:41:11,720
- 是的，先生。

867
00:41:13,080 --> 00:41:16,460
- 现在看起来几个主要的玩家在 AI 领域占据主导地位，

868
00:41:16,540 --> 00:41:17,860
他们会继续维持这个地位，

869
00:41:18,340 --> 00:41:24,460
这些公司似乎与那些受到反垄断法规关注的大公司有所交叉。

870
00:41:24,880 --> 00:41:27,980
你对这两种趋势有何看法，

871
00:41:28,120 --> 00:41:30,280
像是监管部门会否对这些公司进行分拆，

872
00:41:30,320 --> 00:41:33,240
这将会对现状产生怎样的影响。

873
00:41:33,580 --> 00:41:37,680
在我的职业生涯中，我曾帮助微软避免被拆分，

874
00:41:37,780 --> 00:41:38,720
结果它确实没有被拆分。

875
00:41:39,100 --> 00:41:42,120
我也曾努力阻止谷歌被分拆，

876
00:41:42,160 --> 00:41:43,240
目前来看，谷歌还未被分拆。

877
00:41:43,680 --> 00:41:46,420
因此，从我的观察来看，这些公司被分拆的趋势并不明显。

878
00:41:47,580 --> 00:41:50,240
只要这些公司避免成为像约翰·D·洛克菲勒那样的企业巨头。

879
00:41:50,360 --> 00:41:52,760
我研究过这个。你可以查一下，

880
00:41:53,460 --> 00:41:55,100
这就是反垄断法的起源。

881
00:41:55,500 --> 00:41:56,880
我认为政府不会采取行动。

882
00:41:57,900 --> 00:42:00,500
你看到这些大公司占据主导地位的原因是，

883
00:42:00,500 --> 00:42:03,880
他们有足够的资本来建设这些数据中心，

884
00:42:05,540 --> 00:42:08,060
以我的朋友 Reed 和 Mustafa 为例——

885
00:42:08,060 --> 00:42:10,200
- 他将在两周后到来。

886
00:42:10,500 --> 00:42:16,640
- 让 Reed 和你谈谈他们为什么决定将 Inflection 部分业务交给微软。

887
00:42:17,740 --> 00:42:19,980
他们主要是因为无法筹集到足够的资金，

888
00:42:20,060 --> 00:42:21,160
数额高达数百亿美元。

889
00:42:21,320 --> 00:42:22,800
- 你之前提到的那个数字是公开的吗？

890
00:42:22,980 --> 00:42:23,060
- 不是。

891
00:42:23,420 --> 00:42:23,480
- 好的。

892
00:42:24,020 --> 00:42:24,840
- 让 Reed 给你那个数字。

893
00:42:24,840 --> 00:42:25,380
- 好的，也许 Reed 可以透露给你。

894
00:42:25,580 --> 00:42:27,500
我知道你要走，我不想耽搁你。

895
00:42:27,600 --> 00:42:29,300
我想让你带着-- - 等等，等等，等等。

896
00:42:29,300 --> 00:42:30,460
我们让这位先生-- 再问一个。

897
00:42:30,460 --> 00:42:31,120
我有个问题要问你。

898
00:42:31,240 --> 00:42:31,300
再问一个。

899
00:42:31,400 --> 00:42:31,660
是的，你说吧。

900
00:42:31,740 --> 00:42:32,380
非常感谢。

901
00:42:33,040 --> 00:42:33,440
[笑声]

902
00:42:33,440 --> 00:42:34,140
- 非常感谢。

903
00:42:34,160 --> 00:42:34,600
我会尽快问完的。

904
00:42:35,620 --> 00:42:36,920
我想知道，这一切

905
00:42:36,940 --> 00:42:43,100
将会把那些不参与前沿模型开发和无法获取计算资源的国家带向何方？

906
00:42:43,900 --> 00:42:47,300
富国越来越富，穷国尽力而为。

907
00:42:48,220 --> 00:42:50,260
他们将不得不-- 事实上，

908
00:42:50,460 --> 00:42:52,860
这就是富国的游戏，对吧？

909
00:42:53,320 --> 00:42:56,200
巨大的资本，众多技术过硬的人才，

910
00:42:56,500 --> 00:42:58,520
强有力的政府支持，对吧？

911
00:42:58,840 --> 00:42:59,980
这有两个例子。

912
00:43:00,500 --> 00:43:02,640
还有许多其他国家面临各种问题，

913
00:43:02,760 --> 00:43:03,680
他们没有这些资源。

914
00:43:03,900 --> 00:43:05,300
他们必须找到一个合作伙伴，

915
00:43:05,800 --> 00:43:07,260
或者与其他国家联合，

916
00:43:07,440 --> 00:43:07,940
这类情况。

917
00:43:08,520 --> 00:43:10,160
- 我想在这里结束……我记得，我们上次见面时，

918
00:43:10,260 --> 00:43:12,780
你正在 AGI House 参加黑客马拉松。

919
00:43:12,880 --> 00:43:15,700
我知道你花了很多时间帮助年轻人，

920
00:43:16,060 --> 00:43:17,620
他们正在创造大量财富，

921
00:43:17,660 --> 00:43:20,800
你也非常热情地谈论了这件事。

922
00:43:21,060 --> 00:43:22,740
你对在座的各位有什么建议吗？

923
00:43:23,200 --> 00:43:25,940
他们正在为这门课写商业计划书，

924
00:43:25,960 --> 00:43:29,080
或撰写政策提案、研究提案，

925
00:43:29,480 --> 00:43:32,260
未来职业发展方面你有什么建议？

926
00:43:32,800 --> 00:43:35,700
我在商学院教过一门关于这个主题的课程，

927
00:43:36,020 --> 00:43:37,160
你应该来听我的课。

928
00:43:39,781 --> 00:43:46,920
我对你们能以如此快的速度展示新想法感到震惊。

929
00:43:48,040 --> 00:43:53,240
比如，在我参加的一个黑客马拉松中，获胜的团队接受了一个任务：

930
00:43:53,440 --> 00:43:55,712
“让无人机在两座塔之间飞行”，

931
00:43:55,712 --> 00:43:57,400
并且是在一个虚拟的无人机空间中。

932
00:43:58,100 --> 00:43:59,740
他们弄清楚了如何驾驶无人机，

933
00:44:00,020 --> 00:44:03,520
理解了“between”这个词的含义，用 Python 生成了代码，

934
00:44:03,720 --> 00:44:05,660
并使无人机在模拟器中穿过了塔楼。

935
00:44:06,120 --> 00:44:11,340
这样的任务，如果交给专业的程序员，可能需要一周或两周的时间。

936
00:44:12,960 --> 00:44:16,180
我想说的是，快速制作原型的能力真的非常重要，

937
00:44:17,340 --> 00:44:20,580
成为创业者的一个难点在于一切都发生得非常快。

938
00:44:21,460 --> 00:44:29,140
现在，如果你不能在一天之内利用这些工具构建你的原型， 那你就需要考虑一下了。

939
00:44:30,280 --> 00:44:32,080
因为你的竞争对手正在这么做。

940
00:44:32,200 --> 00:44:35,640
所以我想给出的最大建议是，当你开始考虑创办公司时，

941
00:44:35,700 --> 00:44:36,900
写一份商业计划是完全没问题的。

942
00:44:37,860 --> 00:44:40,840
实际上，你应该让电脑帮你写商业计划书，

943
00:44:41,900 --> 00:44:42,560
只要这是合法的。

944
00:44:43,240 --> 00:44:45,320
实际上，我应该在你们离开后再谈谈这个问题。

945
00:44:47,900 --> 00:44:54,060
但我认为，尽快使用这些工具把你的想法制作成原型非常重要，

946
00:44:54,700 --> 00:44:58,760
因为可以肯定的是，在其他公司、

947
00:44:58,760 --> 00:44:59,980
其他大学，

948
00:45:00,300 --> 00:45:01,240
或者你从未去过的地方，

949
00:45:01,980 --> 00:45:03,060
一定有人正在做和你一样的事情。

950
00:45:03,860 --> 00:45:04,000
好的。

951
00:45:04,380 --> 00:45:05,040
好，感谢大家。

952
00:45:05,180 --> 00:45:05,760
我要赶紧离开了。

953
00:45:07,100 --> 00:45:07,660
谢谢。

954
00:45:09,840 --> 00:45:10,460
谢谢。

955
00:45:12,620 --> 00:45:14,760
实际上，让我再强调一下最后一点，

956
00:45:14,900 --> 00:45:18,280
因为我觉得在第一节课中我没有谈到关于使用大语言模型的事情，

957
00:45:18,460 --> 00:45:23,300
这在这门课的作业中是可以接受的，

958
00:45:23,600 --> 00:45:25,900
但必须充分披露。

959
00:45:26,540 --> 00:45:27,420
所以，当你使用它们，

960
00:45:27,860 --> 00:45:31,640
如果你在完成每周作业或期末项目时使用了它们，

961
00:45:31,860 --> 00:45:38,560
就像你向你友好的叔叔或同学寻求建议一样，

962
00:45:38,660 --> 00:45:39,200
你也应该这样做，

963
00:45:39,280 --> 00:45:42,100
或者如果你有笔记需要包含在内。

964
00:45:42,800 --> 00:45:50,220
所以我想谈一谈关于 AI，特别是 GPT，

965
00:45:50,220 --> 00:45:53,460
这对商业及其影响意味着什么。

966
00:45:53,820 --> 00:45:54,780
但在我们讨论这个之前，

967
00:45:54,800 --> 00:45:57,160
我想看看你们是否有任何问题想要跟进，

968
00:45:57,260 --> 00:45:59,200
特别是关于 Eric 提到的内容，

969
00:45:59,200 --> 00:46:02,460
我会尽量表达他的想法，

970
00:46:02,460 --> 00:46:05,380
我们可以讨论一下这些话题，然后再继续。

971
00:46:05,460 --> 00:46:05,980
好的，你请说。

972
00:46:06,680 --> 00:46:09,440
- 我想问的一个问题是关于监管的。

973
00:46:09,920 --> 00:46:11,160
如果目标是保持领先地位，

974
00:46:11,380 --> 00:46:14,600
如何创造正确的激励机制，使每个人，

975
00:46:14,880 --> 00:46:16,080
无论是盟友还是非盟友，

976
00:46:16,760 --> 00:46:17,680
都有动力去遵守？

977
00:46:17,900 --> 00:46:20,600
- 你是指在竞争中的公司之间吗？

978
00:46:20,640 --> 00:46:21,300
是公司还是国家？

979
00:46:21,520 --> 00:46:21,840
- 是国家，比如美国和欧盟。

980
00:46:22,660 --> 00:46:29,100
这是否不会成为那些选择遵循法规的国家或公司发展的阻碍？

981
00:46:29,100 --> 00:46:29,860
这是非常棘手的问题。

982
00:46:30,080 --> 00:46:34,280
Barry J. Nalebuff 写了一本叫做《合作竞争》的书， 其中讨论了这些问题。

983
00:46:34,280 --> 00:46:43,040
确实有些情况下，监管可以帮助公司和整个行业生存下来。

984
00:46:43,100 --> 00:46:45,180
所以监管并不一定会减缓进展。

985
00:46:45,240 --> 00:46:47,080
标准就是一个很好的例子，

986
00:46:48,859 --> 00:46:53,600
明确标准反而能帮助竞争。

987
00:46:53,700 --> 00:46:56,300
我和许多公司的高级管理人员进行过交谈，

988
00:46:56,460 --> 00:46:59,180
他们确实希望在某些领域有一些共同的标准，

989
00:46:59,700 --> 00:47:03,420
有时候在一些危险领域还存在“竞相降低标准”的现象。

990
00:47:03,600 --> 00:47:08,040
谷歌的团队曾表示，他们没有更快推进的另一个原因是，

991
00:47:08,080 --> 00:47:13,020
他们担心大语言模型可能会被滥用或存在危险，

992
00:47:13,980 --> 00:47:15,120
但他们在某种程度上感到被迫加快步伐。

993
00:47:15,620 --> 00:47:19,280
我还与另一家大公司的员工谈过，

994
00:47:19,860 --> 00:47:22,320
他们说，“我们本来并不打算发布这个功能，

995
00:47:22,460 --> 00:47:25,740
但现在竞争对手都在做，

996
00:47:25,840 --> 00:47:27,600
所以我们不得不也发布。”

997
00:47:27,600 --> 00:47:29,140
这也是为什么在某些情况下，

998
00:47:29,360 --> 00:47:32,660
可能会有协调监管的兴趣，

999
00:47:33,260 --> 00:47:35,080
但显然，

1000
00:47:35,180 --> 00:47:39,740
更常见的情况是，监管被用来阻碍竞争。

1001
00:47:40,280 --> 00:47:41,380
例如，很多人认为，

1002
00:47:41,480 --> 00:47:47,720
一些大公司反对开源和推动更广泛的开源的部分原因， 

1003
00:47:47,820 --> 00:47:49,660
就是他们想要放慢竞争对手的步伐。

1004
00:47:50,000 --> 00:47:51,460
所以，这两种情况都存在。

1005
00:47:52,100 --> 00:47:53,200
是的，那边有个问题。

1006
00:47:53,340 --> 00:47:53,400
- 好的。

1007
00:47:53,840 --> 00:47:58,800
我想跟进一下刚才的讨论：我们还应该学编程吗？

1008
00:47:59,060 --> 00:48:00,420
我们还需要学习英语吗？

1009
00:48:00,680 --> 00:48:01,320
这些技能还会有用吗？

1010
00:48:01,460 --> 00:48:07,840
Eric 的回答是肯定的，比如受过大学教育的高技能工作或任务仍然是安全的，

1011
00:48:08,060 --> 00:48:10,360
但其他的，比如停车管理等，可能就不能保证了。

1012
00:48:10,760 --> 00:48:11,840
这种情况可能有一些主观因素影响。

1013
00:48:11,840 --> 00:48:15,320
这个话题很有趣，或许我们可以在接下来的几分钟内再讨论更多。

1014
00:48:15,440 --> 00:48:17,980
但确实值得思考的是，

1015
00:48:17,980 --> 00:48:21,980
AI 系统到底是在替代人类的工作，

1016
00:48:21,980 --> 00:48:23,700
还是在补充人类的工作。

1017
00:48:24,260 --> 00:48:26,860
在编程领域，目前看起来，

1018
00:48:27,440 --> 00:48:31,680
AI 似乎对最顶尖的程序员并没有太大的帮助，

1019
00:48:31,900 --> 00:48:35,100
反而对中等水平的程序员非常有帮助。

1020
00:48:35,580 --> 00:48:38,900
但如果你对编程一无所知，它同样也帮不上什么忙。

1021
00:48:39,120 --> 00:48:40,720
所以这有点像一个倒 U 型。

1022
00:48:41,319 --> 00:48:43,100
可以理解这种情况，

1023
00:48:43,260 --> 00:48:48,560
因为如果你连AI生成的代码都无法理解，

1024
00:48:48,560 --> 00:48:49,420
那这些代码往往会有问题，

1025
00:48:49,420 --> 00:48:50,480
或者并不完全正确。

1026
00:48:50,800 --> 00:48:53,420
如果你不能理解代码的运行逻辑，

1027
00:48:53,540 --> 00:48:55,680
那么你也无法有效地使用它。

1028
00:48:56,420 --> 00:48:59,120
另一方面，对于顶尖的程序员来说，

1029
00:48:59,300 --> 00:49:02,780
生成的代码仍不够优秀，

1030
00:49:02,860 --> 00:49:03,980
所以才会出现这种倒 U 型曲线。

1031
00:49:04,260 --> 00:49:06,640
但这也就意味着，如果你完全不懂代码，

1032
00:49:06,660 --> 00:49:08,520
你仍然需要一些基础知识，才能让AI的帮助变得有用。

1033
00:49:08,840 --> 00:49:12,860
我认为这也适用于当前的很多应用，

1034
00:49:12,980 --> 00:49:16,820
你需要有一些基本的理解，才能最大程度地利用它。

1035
00:49:17,380 --> 00:49:19,300
我觉得这也是一个值得探讨的问题：

1036
00:49:19,740 --> 00:49:21,520
这种情况是否会一直存在。

1037
00:49:22,640 --> 00:49:25,120
在上一节课中，我简要介绍了

1038
00:49:25,340 --> 00:49:31,380
从 0 级到 5 级的自动驾驶汽车。

1039
00:49:32,500 --> 00:49:34,740
这其实也是我们可以讨论的一个话题，

1040
00:49:34,920 --> 00:49:36,540
我在试图梳理，

1041
00:49:37,280 --> 00:49:41,800
如果把这种模式应用到经济中的所有任务上，

1042
00:49:42,400 --> 00:49:43,980
那它们会经历多少级别的变化？

1043
00:49:44,100 --> 00:49:49,720
就自动驾驶汽车而言，我们目前还没有真正达到第5级，

1044
00:49:49,820 --> 00:49:52,580
虽然我不知道你们当中有多少人坐过 Waymo 的自动驾驶汽车。

1045
00:49:52,740 --> 00:49:58,380
这款车的表现相当好，但我和 Sebastian Thrun 一起乘坐时，他说

1046
00:49:58,500 --> 00:49:59,880
目前的运营成本非常高。

1047
00:50:00,500 --> 00:50:02,020
可能每辆车的运营成本会亏损 50 到 100 美元。

1048
00:50:02,400 --> 00:50:02,780
他并不确定具体的数额。

1049
00:50:02,780 --> 00:50:03,140
因为他已经不在那儿了，

1050
00:50:03,140 --> 00:50:05,420
虽然他是项目的创始人，但现在不再参与。

1051
00:50:05,840 --> 00:50:09,480
运营成本太高，使得这款车并不实用。

1052
00:50:09,960 --> 00:50:13,880
或许随着时间的推移，激光雷达的成本会降低，使得运营成本会降低。

1053
00:50:15,060 --> 00:50:23,440
我们有很多 2 级、3 级甚至 4 级的自动驾驶汽车，这些车辆还需要人类的参与。

1054
00:50:24,020 --> 00:50:25,520
其他很多任务也是如此，比如编程，

1055
00:50:25,560 --> 00:50:26,360
我刚才已经提到过了。

1056
00:50:26,820 --> 00:50:30,060
另一方面，关于国际象棋，在前一张幻灯片中

1057
00:50:30,640 --> 00:50:34,940
我提到了一种有时被称为“高级国际象棋”或“自由式国际象棋”的概念。

1058
00:50:35,400 --> 00:50:41,500
加里·卡斯帕罗夫在 1997 年输给深蓝之后，

1059
00:50:43,220 --> 00:50:49,000
他发起了一系列比赛，让人类和机器可以合作。

1060
00:50:49,880 --> 00:50:57,200
在相当长的一段时间里，比如在我 2012 年或 2013 年做TED演讲时，

1061
00:50:57,200 --> 00:51:03,820
当时的情况是，人类与机器合作可以击败深蓝或任何国际象棋计算机。

1062
00:51:04,420 --> 00:51:09,700
因此，当时最强的国际象棋选手是这些人机组合。

1063
00:51:11,240 --> 00:51:12,040
但现在情况已经不同了。

1064
00:51:13,080 --> 00:51:18,140
像 AlphaZero 这样的程序，

1065
00:51:18,140 --> 00:51:19,820
即使有人的参与，对他们来说毫无益处。

1066
00:51:19,880 --> 00:51:22,880
反而可能对棋局造成干扰。

1067
00:51:23,120 --> 00:51:27,900
所以，它经历了从机器什么都做不了，

1068
00:51:27,980 --> 00:51:30,300
到人机合作，

1069
00:51:30,900 --> 00:51:33,740
再到完全自主的阶段，

1070
00:51:33,800 --> 00:51:36,500
我不太确定，大概用了 20 年的时间吧。

1071
00:51:37,600 --> 00:51:39,700
如果有人想做研究项目，

1072
00:51:39,740 --> 00:51:41,620
或者你们现在有想法的话，

1073
00:51:41,680 --> 00:51:48,940
可以探讨一下，哪些经济任务会处于那个中间地带。

1074
00:51:49,280 --> 00:51:51,940
因为那个中间地带对我们人类来说是一个不错的区域，

1075
00:51:52,400 --> 00:51:54,440
机器可以帮助我们，

1076
00:51:54,920 --> 00:51:58,680
但人类在创造价值方面依然不可或缺。

1077
00:51:59,200 --> 00:52:03,400
在这个区域里，可以提高生产力、创造更多财富

1078
00:52:03,480 --> 00:52:06,420
和提高绩效，同时也更有可能实现共享繁荣。

1079
00:52:06,520 --> 00:52:10,680
因为劳动力的分布性使得其更广泛，

1080
00:52:11,520 --> 00:52:14,480
而技术和资本正如 Eric 刚才提到的，

1081
00:52:14,640 --> 00:52:15,880
可能会高度集中。

1082
00:52:17,020 --> 00:52:17,820
你对此有什么看法吗？

1083
00:52:18,000 --> 00:52:20,260
- 我想问一个相关的问题。

1084
00:52:20,360 --> 00:52:24,620
Eric 还提到我们在芯片制造方面有一个10年的计划。

1085
00:52:24,800 --> 00:52:26,120
- 是的，我对此感到惊讶。

1086
00:52:26,600 --> 00:52:29,720
- 是的，我觉得有趣的是，

1087
00:52:29,800 --> 00:52:34,660
作为一名劳动经济学家，文献和新闻中对此有一个积极的信号，

1088
00:52:35,200 --> 00:52:37,860
如果我们正在将所有的芯片制造都转移到美国国内，

1089
00:52:38,060 --> 00:52:41,440
那么这可能会带来蓝领工作的复兴吗？

1090
00:52:41,800 --> 00:52:43,140
我想知道你对

1091
00:52:43,760 --> 00:52:46,680
智能机器人模型或人类劳力有什么看法。

1092
00:52:46,700 --> 00:52:48,560
- 嗯，我认为这不会有太大的影响。

1093
00:52:49,080 --> 00:52:51,360
你们当中有多少人参观过芯片工厂？

1094
00:52:51,880 --> 00:52:52,820
有人去过吗？有几个人去过。

1095
00:52:53,180 --> 00:52:54,940
工厂里有多少工人在工作？

1096
00:52:56,260 --> 00:52:59,460
- 是台积电吗？是纽约大学的人去的，所以我不确定。

1097
00:52:59,720 --> 00:53:03,360
- 好吧，答案是零。

1098
00:53:04,420 --> 00:53:07,300
他们不让人们进去的原因是我们人类太笨拙、

1099
00:53:07,560 --> 00:53:10,740
太不卫生，

1100
00:53:10,960 --> 00:53:13,580
所以全部都是机器人操作。

1101
00:53:14,040 --> 00:53:21,220
所有设备都在密封环境中运行。因此，确实有人需要运送物资等工作。

1102
00:53:21,720 --> 00:53:24,140
如果机器人倒了或者出现了问题，

1103
00:53:24,420 --> 00:53:27,400
他们得穿上像太空服一样的防护服进去调整， 

1104
00:53:27,960 --> 00:53:33,520
然后再出来，希望没有弄坏什么。

1105
00:53:36,000 --> 00:53:36,400
所以，

1106
00:53:36,780 --> 00:53:38,840
基本上是全自动化作业。

1107
00:53:38,960 --> 00:53:48,180
对，我认为这需要一些更复杂的劳动力， 但我不认为这会有太大的蓝领工人复兴。

1108
00:53:48,260 --> 00:53:48,480
实际上，

1109
00:53:48,540 --> 00:53:54,220
苹果之所以将 MacBook 的生产线转移到德克萨斯，

1110
00:53:54,220 --> 00:53:57,060
不是因为德克萨斯的劳动力便宜， 

1111
00:53:57,620 --> 00:54:01,900
而是因为他们实际上不再需要太多劳动力。

1112
00:54:02,160 --> 00:54:03,360
所以这个过程很像做体力劳动。

1113
00:54:03,720 --> 00:54:10,920
因此，美国制造业虽然在产出方面增长明显，但在就业方面却并未有多大增长。

1114
00:54:12,480 --> 00:54:13,340
我们来到这里看看。

1115
00:54:13,340 --> 00:54:13,540
是的。

1116
00:54:14,220 --> 00:54:20,160
- 你认为 AI 智能体或“文本转换为行动指令”模型在明年会出现转折点吗？

1117
00:54:20,220 --> 00:54:20,600
哦，当然。

1118
00:54:20,760 --> 00:54:20,960
不，不是。

1119
00:54:21,060 --> 00:54:23,480
Eric说的这些，我也听说过类似的东西。

1120
00:54:23,500 --> 00:54:25,360
他很好地总结了这三大趋势。

1121
00:54:25,640 --> 00:54:26,680
我之前已经分别听说过这些趋势，

1122
00:54:26,800 --> 00:54:28,800
但他能够将它们整合起来确实不错。

1123
00:54:29,640 --> 00:54:30,960
今天早些时候，我和吴恩达谈过，

1124
00:54:31,060 --> 00:54:33,500
他一直在强调， 

1125
00:54:33,500 --> 00:54:39,740
特别是预计在 2024 年将会迎来智能体的大潮，

1126
00:54:40,620 --> 00:54:43,320
吴恩达老师描述得很好，

1127
00:54:43,320 --> 00:54:47,540
就像你们都知道的，如果你让一个大语言模型写一篇文章，

1128
00:54:48,040 --> 00:54:49,180
它会一个字一个字地生成，

1129
00:54:49,180 --> 00:54:52,340
从头到尾一次性完成，

1130
00:54:53,359 --> 00:54:54,340
效果还不错。

1131
00:54:55,060 --> 00:55:01,120
但想象一下，如果你写文章时不能使用退格键，

1132
00:55:01,260 --> 00:55:02,640
也不能先做大纲，

1133
00:55:02,800 --> 00:55:03,500
只能直接写完。

1134
00:55:04,300 --> 00:55:06,480
现在的 AI 智能体会说，好的，

1135
00:55:07,079 --> 00:55:08,160
首先我们需要制定一个大纲。

1136
00:55:08,660 --> 00:55:10,260
这是写文章的第一步。

1137
00:55:10,260 --> 00:55:11,880
接着，填充每一段内容，

1138
00:55:12,040 --> 00:55:13,940
再回头检查文章的流畅性。

1139
00:55:14,360 --> 00:55:16,500
然后，再看看语气是否合适，

1140
00:55:16,620 --> 00:55:19,500
这是否符合目标读者的水平。

1141
00:55:20,040 --> 00:55:22,580
通过这样反复迭代，

1142
00:55:22,680 --> 00:55:25,520
你可以写出更好的文章或完成其他任务。

1143
00:55:26,040 --> 00:55:27,080
这是一场真正的革命，

1144
00:55:27,260 --> 00:55:30,380
通过这种方式，你可以把很多事情做得更好。

1145
00:55:30,700 --> 00:55:31,820
还有关于上下文窗口的事情，

1146
00:55:31,940 --> 00:55:32,740
也非常重要。

1147
00:55:33,560 --> 00:55:35,700
所以我引用一些我认识的聪明人的话，

1148
00:55:35,940 --> 00:55:40,460
Eric Horvitz，我上周在 GSB 的一个讨论小组上与他同台，有些人可能在场。

1149
00:55:41,960 --> 00:55:44,140
他提出了一个很好的分类法。

1150
00:55:44,580 --> 00:55:45,920
人们在问他关于微调的问题，

1151
00:55:46,220 --> 00:55:47,820
我记得是 Susan 问的。

1152
00:55:48,420 --> 00:55:53,840
他说，其实有三种方法可以让模型更加个性化。

1153
00:55:54,500 --> 00:55:55,580
第一种是微调，

1154
00:55:55,680 --> 00:55:57,140
也就是进一步训练模型。

1155
00:55:57,640 --> 00:55:59,900
第二种是使用更大、更强的上下文窗口。

1156
00:56:00,440 --> 00:56:05,620
第三种是使用 RAG（检索增强生成） 或类似的技术， 

1157
00:56:05,720 --> 00:56:08,980
它能够访问外部数据。

1158
00:56:09,360 --> 00:56:14,420
但现在这些上下文窗口似乎效果非常好。

1159
00:56:14,560 --> 00:56:17,040
我想，正如 Eric 所说，我们本以为这很难。

1160
00:56:17,200 --> 00:56:18,080
也许 Peter 可以解释一下。

1161
00:56:18,500 --> 00:56:20,740
但不管怎样，现在我们能构建更大的上下文窗口了，

1162
00:56:20,780 --> 00:56:23,960
现在，你可以输入一整本书或一整套书籍。

1163
00:56:24,080 --> 00:56:25,980
你可以输入各种各样的信息。

1164
00:56:26,720 --> 00:56:29,960
这能给你提供所有的上下文环境。

1165
00:56:30,040 --> 00:56:32,060
所以这是相当革命性的。

1166
00:56:32,180 --> 00:56:37,540
这为我们带来了许多之前没有的能力，

1167
00:56:37,800 --> 00:56:40,900
包括让信息实时性更强，就像 Eric 提到的那样。

1168
00:56:41,480 --> 00:56:42,160
你想再深入讨论这个话题吗？

1169
00:56:43,400 --> 00:56:43,720
- [听不清]

1170
00:56:50,200 --> 00:56:51,200
- 这是一个很好的问题。

1171
00:56:51,520 --> 00:56:53,340
我的意思是，虽然确实有更多的资本投入，

1172
00:56:53,400 --> 00:56:54,680
但这也引发了一些问题和评论。

1173
00:56:54,760 --> 00:56:57,020
为什么所有这些资本都投向 AI，而不是其他地方呢？

1174
00:56:57,800 --> 00:57:00,660
我认为，你知道，如果你观察历史的发展轨迹，

1175
00:57:00,860 --> 00:57:03,120
有时看起来很平滑，但如果仔细观察，

1176
00:57:03,300 --> 00:57:06,260
就会发现很多跳跃式的发展。

1177
00:57:06,400 --> 00:57:09,220
有一些重大的发明和小的创新。

1178
00:57:09,840 --> 00:57:13,200
Andrew Karpathy 曾表示，他曾研究物理学，

1179
00:57:13,300 --> 00:57:17,980
要在物理学上取得重大进展，成为顶尖的物理学家，

1180
00:57:18,120 --> 00:57:20,160
你必须非常聪明，学习很多东西。

1181
00:57:20,220 --> 00:57:21,620
也许如果足够幸运，

1182
00:57:21,680 --> 00:57:24,500
你可以做出一些微小的贡献，

1183
00:57:24,680 --> 00:57:25,340
有些人确实做到了。

1184
00:57:26,480 --> 00:57:29,060
但他表示，现在在人工智能和机器学习领域，

1185
00:57:29,480 --> 00:57:34,320
我们似乎正处于一个有大量“低垂果实”的时代， 已经有了一些重大的突破。

1186
00:57:34,520 --> 00:57:40,320
相比于像摘树上所有果实那样耗尽资源，

1187
00:57:41,000 --> 00:57:42,279
 现状更像是组合数学。

1188
00:57:43,520 --> 00:57:45,440
在讨论第二次机器时代时，人们提到了"积木"这个概念。

1189
00:57:45,860 --> 00:57:48,540
当你把两块积木或两块乐高积木组合在一起时，

1190
00:57:48,660 --> 00:57:49,660
你可以创造出越来越多的新东西。

1191
00:57:50,020 --> 00:57:54,440
现在我们似乎正处于这样一个充满机会的时代，

1192
00:57:54,500 --> 00:57:55,420
人们也开始意识到这一点。

1193
00:57:56,060 --> 00:58:00,060
一个发现往往会带来另一个发现，进而产生新的机会。

1194
00:58:00,660 --> 00:58:08,060
由于这个原因，越来越多的投资和人力被吸引进来。

1195
00:58:08,880 --> 00:58:14,360
在经济学中，有时候更多的资源投入会导致边际收益递减，

1196
00:58:14,560 --> 00:58:16,760
比如在农业或采矿业中。

1197
00:58:17,360 --> 00:58:20,920
然而在别的地方，资源的投入则可能带来增长的回报。

1198
00:58:21,580 --> 00:58:25,980
更多工程师来到硅谷，会让现有的工程师变得更加有价值，

1199
00:58:26,120 --> 00:58:26,860
而不是更不值钱。

1200
00:58:27,880 --> 00:58:29,960
我们似乎正处在一个这样的时代。

1201
00:58:30,440 --> 00:58:40,480
额外的投资和培训资金也使这些技术变得越来越强大。 

1202
00:58:40,860 --> 00:58:42,640
我不确定这种情况会持续多久，

1203
00:58:43,120 --> 00:58:51,360
但现在看来，有一些技术已经走进了这个极其丰饶的时期，

1204
00:58:51,360 --> 00:58:54,540
并且带来了积极的反馈和支持。

1205
00:58:54,640 --> 00:58:56,120
我们似乎正处于这样的时代。

1206
00:58:56,680 --> 00:59:00,700
因此，现阶段进入这个领域并接受培训的人，

1207
00:59:00,760 --> 00:59:07,740
往往能够在相对较短的时间内做出相当重大的贡献。 

1208
00:59:08,200 --> 00:59:09,380
我鼓励你们所有人，

1209
00:59:09,400 --> 00:59:10,540
我认为你们现在正在走在正确的道路上。

1210
00:59:11,160 --> 00:59:11,260
好，

1211
00:59:12,060 --> 00:59:13,580
好的，我们再来回答几个问题，然后……好的。

1212
00:59:13,780 --> 00:59:14,340
我们从这边开始，好吗？

1213
00:59:15,220 --> 00:59:18,060
- 并不是每个人都有机会参与到

1214
00:59:18,100 --> 00:59:20,280
关于 AI 的讨论和辩论中来。

1215
00:59:20,720 --> 00:59:25,839
因此，我想了解你对非技术背景利益相关者 AI 素养的看法，不论他们是

1216
00:59:25,840 --> 00:59:27,860
需要做出一定见解判断的政策制定者，

1217
00:59:28,360 --> 00:59:30,820
还是普通大众，比如使用科技产品的用户。

1218
00:59:31,500 --> 00:59:33,740
你觉得在解释技术基础

1219
00:59:34,040 --> 00:59:37,740
和讨论那些看起来很抽象但不一定马上看出答案的影响时，应该怎么平衡呢？ 

1220
00:59:39,160 --> 00:59:39,680
- 这是个难题。

1221
00:59:39,780 --> 00:59:47,760
我必须说，最近在国会和其他地方的人们对这个话题的关注度有了显著提高。

1222
00:59:47,940 --> 00:59:50,320
过去他们对此并不感兴趣，

1223
00:59:50,320 --> 00:59:52,820
现在每个人都在试图更好地理解它。

1224
00:59:53,400 --> 00:59:56,960
我认为有很多领域人们可以做出贡献。

1225
00:59:57,500 --> 00:59:59,980
他们可以在技术层面上做出贡献，

1226
01:00:00,060 --> 01:00:07,280
但如果让我选择，我认为当前更大的瓶颈在于商业和经济层面。

1227
01:00:07,820 --> 01:00:17,260
即便你在技术上做出了重大贡献，要将这些转化为能够影响政策的结果，还有一段距离。

1228
01:00:17,380 --> 01:00:20,910
因此，如果你对政治学感兴趣，或者是一个政治家，

1229
01:00:20,910 --> 01:00:26,680
理解民主、错误信息、权力集中等方面的影响，

1230
01:00:26,800 --> 01:00:28,980
这些问题目前都还没有得到很好的理解。

1231
01:00:29,680 --> 01:00:33,040
我不认为计算机科学家一定是理解这些问题的最佳人选，

1232
01:00:33,140 --> 01:00:39,160
但要了解足够的技术基础， 明确技术可能实现的方面，接下来需要思考这些动态，

1233
01:00:39,380 --> 01:00:42,780
就像亨利·基辛格与埃里克·施密特在书中所做的那样。

1234
01:00:43,560 --> 01:00:45,200
如果你是经济学者，需要去思考

1235
01:00:45,380 --> 01:00:48,000
劳动力市场、集中度、

1236
01:00:48,540 --> 01:00:50,200
不平等、

1237
01:00:50,380 --> 01:00:53,840
就业、生产力以及驱动生产力的各种影响。

1238
01:00:54,200 --> 01:00:57,320
这些都是当前非常值得深入探讨的课题。

1239
01:00:57,380 --> 01:00:59,140
你可以去研究很多不同的领域，

1240
01:00:59,260 --> 01:01:02,580
在那里你可以充分理解这项技术可能的能力，

1241
01:01:02,720 --> 01:01:05,680
然后进一步思考其可能产生的影响。

1242
01:01:06,360 --> 01:01:08,400
我认为这就是我们可以获得最大收益的地方。

1243
01:01:08,480 --> 01:01:11,140
让我给你举一个更具体的例子，

1244
01:01:11,600 --> 01:01:15,260
这是我上周原本打算谈及的一个话题。

1245
01:01:17,120 --> 01:01:21,860
电力也是一种通用技术。通用技术有一个特点，

1246
01:01:21,960 --> 01:01:25,820
它们本身就是一种重要的创新，

1247
01:01:25,940 --> 01:01:29,660
但通用技术真正的力量之一，

1248
01:01:29,740 --> 01:01:36,680
正如我所说的 GPT， 是它们提供了互补性，它们能够激发出互补的创新。

1249
01:01:37,280 --> 01:01:40,839
比如，电力带来了灯泡、计算机和

1250
01:01:40,840 --> 01:01:46,480
电动机，而电动机又推动了压缩机、冰箱和空调的发明。

1251
01:01:46,680 --> 01:01:48,560
你可以从这一项创新中

1252
01:01:48,700 --> 01:01:52,440
引发一系列连锁的创新。

1253
01:01:52,860 --> 01:01:55,320
而大部分的价值来自这些补充性的创新。

1254
01:01:55,920 --> 01:01:57,280
人们常常没有意识到的是，

1255
01:01:57,280 --> 01:02:04,100
一些最重要的互补创新其实是组织创新和人力资本的互补。

1256
01:02:04,620 --> 01:02:09,320
以电力为例，当电力首次引入工厂时，

1257
01:02:10,160 --> 01:02:10,820
密歇根大学的教授 Paul Davis 

1258
01:02:10,840 --> 01:02:13,740
在斯坦福研究了这些工厂的变化，

1259
01:02:13,740 --> 01:02:16,380
令人惊讶的是，

1260
01:02:17,120 --> 01:02:19,900
工厂电气化后，

1261
01:02:19,900 --> 01:02:24,660
他们的生产力并没有比之前由蒸汽机驱动的工厂有显著提高。

1262
01:02:25,100 --> 01:02:25,800
他觉得这很奇怪，

1263
01:02:25,840 --> 01:02:28,580
因为电力看起来像是非常重要的技术。

1264
01:02:28,700 --> 01:02:29,440
这只是一种风潮吗？

1265
01:02:29,580 --> 01:02:30,300
显然并不是。

1266
01:02:31,860 --> 01:02:34,200
使用电力之前的工厂是由蒸汽机驱动的。

1267
01:02:34,320 --> 01:02:34,740
他们通常会

1268
01:02:34,780 --> 01:02:37,820
在工厂中心位置放置一个大蒸汽机，

1269
01:02:37,820 --> 01:02:40,820
然后通过曲轴和皮带驱动所有设备。

1270
01:02:40,820 --> 01:02:45,260
并尽可能地让设备靠近蒸汽机，

1271
01:02:45,260 --> 01:02:46,860
因为如果曲轴过长，

1272
01:02:47,140 --> 01:02:49,460
可能会因为扭力而断裂。

1273
01:02:50,080 --> 01:02:53,320
当他们引入电力时，他发现在一个又一个的工厂里，

1274
01:02:53,740 --> 01:02:55,140
他们会拆除蒸汽机，

1275
01:02:55,140 --> 01:02:58,420
然后找到最大的电动机，

1276
01:02:58,460 --> 01:03:03,140
放在蒸汽机原来的位置，然后启动它。

1277
01:03:03,260 --> 01:03:06,220
但这并没有真正改变整体的生产效率。

1278
01:03:06,400 --> 01:03:07,460
很明显这并不是一个巨大的改进。

1279
01:03:08,420 --> 01:03:11,500
于是他们开始在新的地点从零开始建设全新的工厂。

1280
01:03:12,040 --> 01:03:12,960
那些新工厂长得怎么样？

1281
01:03:14,600 --> 01:03:15,480
跟旧的工厂一模一样。

1282
01:03:16,020 --> 01:03:19,080
他们会采用同样的模型，一些工程师会画出蓝图，

1283
01:03:19,340 --> 01:03:23,420
 在应该放蒸汽机的地方打上一个大大的 X，说："不，不，这里应该放一个电动机，"

1284
01:03:23,460 --> 01:03:24,700
然后他们就开始建设全新的工厂。

1285
01:03:25,460 --> 01:03:27,640
然而，这并没有带来显著的生产率提高。

1286
01:03:28,180 --> 01:03:30,060
大约 30 年后，

1287
01:03:30,060 --> 01:03:32,740
我们才看到一种根本不同的工厂模式。

1288
01:03:33,120 --> 01:03:36,040
这种工厂没有中央电源，也就是说，

1289
01:03:36,080 --> 01:03:37,060
没有在中间放一架大型电动机，

1290
01:03:37,540 --> 01:03:41,680
而是采用分散式电力供应，因为电机，如你们所了解的，

1291
01:03:41,860 --> 01:03:44,040
可以做得很大，也可以做得中等，也可以做得非常非常小，

1292
01:03:44,600 --> 01:03:46,800
你可以用各种方式将它们连接起来。

1293
01:03:47,420 --> 01:03:49,540
于是，他们开始让每一件设备都有

1294
01:03:49,560 --> 01:03:53,620
自己独立的电机， 而不是依赖一个大电动机。

1295
01:03:53,740 --> 01:03:55,040
他们把它称为单元驱动，

1296
01:03:55,060 --> 01:03:55,880
而不是组驱动。

1297
01:03:56,220 --> 01:04:01,820
我在哈佛商学院的贝克图书馆读过 1914 年的一些书，

1298
01:04:01,820 --> 01:04:05,820
当时关于单元驱动和组驱动的讨论非常激烈。

1299
01:04:06,320 --> 01:04:09,160
当他们开始使用单元驱动时，他们就建立了一系列新的工厂

1300
01:04:09,660 --> 01:04:11,600
工厂通常只有一层，

1301
01:04:11,600 --> 01:04:15,540
设备的排列不再基于动力需求，

1302
01:04:15,620 --> 01:04:19,080
而是基于其他因素

1303
01:04:19,160 --> 01:04:20,480
如物料的流动

1304
01:04:21,080 --> 01:04:22,820
于是流水线系统开始形成

1305
01:04:24,040 --> 01:04:25,360
这带来了生产力的大幅度提升

1306
01:04:25,460 --> 01:04:29,960
比如生产力提高一倍，有时甚至三倍

1307
01:04:30,559 --> 01:04:36,300
所以，教训不是说电力是一种短暂的风潮，或者是失败的、被过度炒作的。

1308
01:04:37,080 --> 01:04:39,080
电力是一种非常有价值的基础技术

1309
01:04:39,640 --> 01:04:42,820
但只有在他们进行了流程创新

1310
01:04:43,040 --> 01:04:46,480
和组织创新，重新思考生产方式后，

1311
01:04:47,040 --> 01:04:48,040
才真正实现了巨大的回报

1312
01:04:49,320 --> 01:04:50,740
这样的故事很多

1313
01:04:50,800 --> 01:04:51,800
我只讲了一个。

1314
01:04:51,960 --> 01:04:53,340
我们时间有限，我还可以告诉你其他的例子。

1315
01:04:53,420 --> 01:04:55,600
但在我一些书和文章中

1316
01:04:56,120 --> 01:04:59,940
如果你看看蒸汽机和其他技术，

1317
01:04:59,940 --> 01:05:01,604
会发现类似的代际滞后期，

1318
01:05:01,604 --> 01:05:03,480
经过几代人的努力，人们在数十年后才意识到

1319
01:05:03,480 --> 01:05:05,060
这项技术可以让你做的事情

1320
01:05:05,060 --> 01:05:06,780
完全不同于你过去常做的事情

1321
01:05:07,423 --> 01:05:09,860
我认为 AI 在某些方面也有点类似，

1322
01:05:09,860 --> 01:05:11,980
将会出现很多的组织创新

1323
01:05:12,100 --> 01:05:13,220
会有新的商业模式

1324
01:05:13,860 --> 01:05:17,020
以及我们之前从未想到过的经济组织方式。

1325
01:05:17,560 --> 01:05:19,240
目前，人们大多是在进行技术改进。

1326
01:05:19,740 --> 01:05:22,800
我可以列举一系列与技术互补的技能变革。

1327
01:05:23,480 --> 01:05:24,460
虽然我不知道所有的变革是什么，

1328
01:05:25,600 --> 01:05:27,260
需要创造性地去思考这些问题，

1329
01:05:27,620 --> 01:05:28,900
但这就是当前的差距所在。

1330
01:05:29,320 --> 01:05:30,840
以早期的电脑为例，

1331
01:05:31,600 --> 01:05:39,440
组织资本和人力资本的投资实际上比硬件和软件高出 10 倍，

1332
01:05:39,580 --> 01:05:44,000
如果你看看硬件和软件的投资规模。

1333
01:05:44,600 --> 01:05:45,160
这是一个非常大的问题。

1334
01:05:46,340 --> 01:05:46,900
话虽如此，

1335
01:05:47,980 --> 01:05:55,460
我愿意稍微调整一下我的看法，因为像 ChatGPT 和其他一些工具，

1336
01:05:56,240 --> 01:06:01,420
它们被迅速地采用，并且在短时间内改变了很多事情，

1337
01:06:01,420 --> 01:06:06,120
部分原因是你不需要像过去那样深入学习 Python。

1338
01:06:06,320 --> 01:06:09,420
你只需要用自然语言就能完成很多事情，

1339
01:06:09,740 --> 01:06:14,660
通过将这些工具应用到现有的组织中，可以获得很大的价值。

1340
01:06:15,140 --> 01:06:16,620
因此，某些方面的变化确实发生得更快了。

1341
01:06:17,340 --> 01:06:20,500
在你可能读到的一些论文中，

1342
01:06:20,980 --> 01:06:25,940
我们看到生产率在短时间内提高了15%、20%、甚至30%。

1343
01:06:27,020 --> 01:06:29,960
但我怀疑，一旦我们找到这些互补的创新，

1344
01:06:29,960 --> 01:06:32,100
生产率的提升会更大。

1345
01:06:32,320 --> 01:06:33,800
这就是我对你问题的长篇回复。

1346
01:06:34,020 --> 01:06:35,980
这不仅仅是技术技能的问题，

1347
01:06:35,980 --> 01:06:39,480
还包括重新思考所有其他相关问题的方式。

1348
01:06:39,600 --> 01:06:42,440
因此，对于那些在商学院或经济学领域的人来说， 

1349
01:06:42,880 --> 01:06:48,720
有很多机会可以重新思考你们的领域，

1350
01:06:48,820 --> 01:06:51,080
因为你们现在手中有了强大的技术。

1351
01:06:51,520 --> 01:06:51,900
好的，请提问。

1352
01:06:54,240 --> 01:07:01,560
你似乎比 Eric 对转型速度更为谨慎，

1353
01:07:02,960 --> 01:07:03,860
我理解的对吗？

1354
01:07:03,900 --> 01:07:07,580
- 嗯，我会在两件事情之间做区分。

1355
01:07:08,040 --> 01:07:11,060
我会听取他和其他人对技术的观点。

1356
01:07:11,200 --> 01:07:12,900
我们将从其他几位专家那里听到他们的看法，

1357
01:07:13,460 --> 01:07:17,560
有些人像他一样乐观，

1358
01:07:17,560 --> 01:07:19,580
甚至对技术更乐观。

1359
01:07:19,680 --> 01:07:21,640
当然，也有一些人对此并不那么乐观。

1360
01:07:23,080 --> 01:07:26,180
但仅有技术是不足以创造生产力的，

1361
01:07:26,180 --> 01:07:29,920
你可能拥有一项极其出色的技术，

1362
01:07:29,920 --> 01:07:31,620
但由于种种原因，

1363
01:07:31,780 --> 01:07:34,540
A，可能是因为人们找不出有效的使用方式。

1364
01:07:35,100 --> 01:07:36,640
另一种可能是受到了监管的限制。

1365
01:07:36,760 --> 01:07:39,640
我有一些计算机科学的同事，他们开发了

1366
01:07:39,700 --> 01:07:43,480
更适合读取医学图像的放射科系统。

1367
01:07:44,120 --> 01:07:47,220
但因为文化原因，它们没有被采用，

1368
01:07:47,240 --> 01:07:48,460
人们不愿接受它们。

1369
01:07:48,580 --> 01:07:50,560
还有安全方面的考虑。

1370
01:07:51,540 --> 01:07:54,480
当我分析哪些任务 AI 可以提供最大帮助，

1371
01:07:54,480 --> 01:07:56,540
以及哪些职业受影响最大时， 

1372
01:07:56,660 --> 01:07:59,720
我惊讶地发现飞行员的排名竟然靠前。

1373
01:08:00,160 --> 01:08:02,760
但我认为很多人

1374
01:08:02,760 --> 01:08:05,420
并不会愿意乘坐无人驾驶的飞机，

1375
01:08:06,280 --> 01:08:08,820
 他们更喜欢有人类飞行员在飞机上。

1376
01:08:09,920 --> 01:08:14,620
因此，有许多不同的因素可能会显著减缓这个过程，

1377
01:08:14,720 --> 01:08:19,720
我认为这是我们需要意识到的。

1378
01:08:19,760 --> 01:08:21,200
如果我们能解决这些瓶颈问题，

1379
01:08:21,620 --> 01:08:26,080
可能对生产力的帮助会超过单纯改进技术。

1380
01:08:27,319 --> 01:08:27,920
好，请提问。

1381
01:08:29,140 --> 01:08:33,540
- Eric 对大学和数据中心的观点很有意思。

1382
01:08:33,779 --> 01:08:35,680
这引发了一个更宏大的问题，

1383
01:08:35,680 --> 01:08:37,420
- 我本来想问他，为什么不直接捐钱？

1384
01:08:39,859 --> 01:08:41,319
有人问过他这个问题。

1385
01:08:41,840 --> 01:08:46,200
- 这就像是在探讨大学在生态系统中应扮演什么角色？

1386
01:08:46,899 --> 01:08:50,939
显然，这里有更大的背景，我确信所有的计算机科学教授都意识到了这一点。

1387
01:08:50,939 --> 01:08:53,720
- 我来回答这个吧，我认为如果有更多的资金支持会更好。

1388
01:08:53,859 --> 01:08:56,160
联邦政府有一个叫“国家AI资源”的项目，

1389
01:08:56,540 --> 01:08:59,300
虽然它提供了一些帮助，

1390
01:08:59,479 --> 01:09:02,399
但资金规模只有几百万美元，顶多几千万美元，

1391
01:09:02,560 --> 01:09:05,180
而不是几十亿美元，更别说是几千亿美元了。

1392
01:09:05,700 --> 01:09:08,200
尽管 Eric 在课前告诉我，

1393
01:09:08,200 --> 01:09:11,060
他们正在推动一个可能更大的项目。 

1394
01:09:11,200 --> 01:09:12,460
他正在推动一个更大的项目。

1395
01:09:12,540 --> 01:09:13,160
我不确定这能否成功。

1396
01:09:15,140 --> 01:09:16,779
这个项目是为了训练这些非常大的模型。

1397
01:09:17,319 --> 01:09:20,000
我曾与杰夫·辛顿进行过一次非常有趣的对话。

1398
01:09:20,120 --> 01:09:24,300
大家都知道，杰夫·辛顿是深度学习的教父之一。

1399
01:09:24,300 --> 01:09:30,140
我问他在工作中，他认为哪种硬件最有用。

1400
01:09:30,979 --> 01:09:34,500
他坐在他的笔记本电脑旁边，轻轻地拍了拍他的 MacBook。

1401
01:09:35,240 --> 01:09:36,880
这让我想到，

1402
01:09:36,880 --> 01:09:39,140
大学或许在另一类研究中有竞争优势，

1403
01:09:39,140 --> 01:09:46,000
这类研究不是训练价值数十亿的模型， 而是创新新的算法，

1404
01:09:46,439 --> 01:09:50,040
比如那些可能超越 Transformer 的算法，

1405
01:09:50,300 --> 01:09:54,420
还有很多其他方式可以让人们做出贡献。

1406
01:09:54,580 --> 01:09:56,680
所以或许这里存在劳动分工。

1407
01:09:56,720 --> 01:10:01,820
我完全支持并赞成我的同事申请更多的 GPU 预算。

1408
01:10:02,340 --> 01:10:07,200
但学术界的贡献未必总是在这里。

1409
01:10:07,440 --> 01:10:08,920
有些贡献来自于新的想法、

1410
01:10:08,920 --> 01:10:13,360
不同的视角和新的方法。 

1411
01:10:14,360 --> 01:10:16,760
这可能是我们的优势所在。

1412
01:10:16,980 --> 01:10:20,340
我上周与 Sendhil Mullainathan 一起吃晚餐。

1413
01:10:21,420 --> 01:10:23,380
他刚从芝加哥搬到了麻省理工学院。

1414
01:10:24,760 --> 01:10:25,700
 他是一名研究员。

1415
01:10:26,060 --> 01:10:28,320
我们在谈论大学的相对优势是什么。

1416
01:10:29,120 --> 01:10:31,700
他认为，其中之一就是耐心。

1417
01:10:32,120 --> 01:10:35,820
在大学里，有些人专注于非常长期的项目，

1418
01:10:36,000 --> 01:10:37,360
像是有人在研究核聚变。

1419
01:10:37,440 --> 01:10:39,060
研究核聚变的人已经工作了很长时间，

1420
01:10:39,460 --> 01:10:40,960
不是因为他们会在

1421
01:10:40,960 --> 01:10:45,320
今年或十年后从建造核聚变电站中赚到很多钱，

1422
01:10:45,320 --> 01:10:46,360
甚至也许二十年后都不会。

1423
01:10:46,400 --> 01:10:48,300
我不知道核聚变需要多长时间。

1424
01:10:48,840 --> 01:10:54,160
但这是一件他们愿意长期投入的事情， 即使时间线更长。

1425
01:10:55,180 --> 01:10:58,360
对公司来说，承受这样长时间线的项目要困难得多。 

1426
01:10:58,480 --> 01:11:03,700
因此，大学在这方面或许有一定的相对优势或者说分工。

1427
01:11:04,660 --> 01:11:05,920
我们只剩下几分钟了。

1428
01:11:06,020 --> 01:11:07,040
挺有意思的讨论。

1429
01:11:07,140 --> 01:11:08,420
那我们只回答一个或两个问题。

1430
01:11:08,540 --> 01:11:10,020
然后我想聊聊项目的事情。

1431
01:11:10,120 --> 01:11:10,200
好的。

1432
01:11:10,760 --> 01:11:10,900
请开始。

1433
01:11:11,100 --> 01:11:11,380
- 好的。

1434
01:11:11,560 --> 01:11:12,380
我是凯文。

1435
01:11:12,680 --> 01:11:16,560
我对 AI 的涌现能力感到好奇。

1436
01:11:16,840 --> 01:11:17,000
好的。

1437
01:11:17,340 --> 01:11:21,040
Eric 似乎更倾向于

1438
01:11:21,240 --> 01:11:25,080
讨论架构差异和设计更好的模型，

1439
01:11:25,140 --> 01:11:26,520
而不是上次课我们讨论的规模定律。

1440
01:11:26,940 --> 01:11:28,140
我想知道你怎么看-

1441
01:11:28,280 --> 01:11:29,020
- 嗯，他提到了全部三个。

1442
01:11:29,260 --> 01:11:31,740
你们还记得规模定律吗？

1443
01:11:31,880 --> 01:11:33,220
它有三个部分。

1444
01:11:33,320 --> 01:11:35,760
我记得我提到了 Dario 和他的团队的规模定律，

1445
01:11:36,600 --> 01:11:38,820
要有更多的算力，更多的数据，

1446
01:11:38,820 --> 01:11:41,400
以及算法的改进，例如增加参数。

1447
01:11:41,760 --> 01:11:45,000
所有这三个部分 - 我认为我听到 Eric 说所有这三个部分-

1448
01:11:45,000 --> 01:11:45,380
都很重要。

1449
01:11:45,480 --> 01:11:49,720
但是不要忽视最后一个部分，新的架构，

1450
01:11:50,800 --> 01:11:51,540
所有这三个部分，

1451
01:11:51,660 --> 01:11:52,780
我认为，都很重要。

1452
01:11:53,220 --> 01:11:54,980
我想我听到你问了一个问题。

1453
01:11:55,540 --> 01:11:55,820
你问了吗？

1454
01:11:55,840 --> 01:12:01,380
- 我们离拥有通用人工智能类型的系统，像这些脱离实际曲线的模型，有多近呢？

1455
01:12:01,920 --> 01:12:02,300
这个问题可以吗？

1456
01:12:02,980 --> 01:12:07,880
- Eric 并不认为我们离拥有通用人工智能类型的系统很近，

1457
01:12:08,060 --> 01:12:08,500
虽然我不认为

1458
01:12:08,520 --> 01:12:09,660
这是一个明确的定义。

1459
01:12:10,160 --> 01:12:11,960
实际上，这也是我本来想问他的问题之一，

1460
01:12:12,000 --> 01:12:12,620
但时间不够了。

1461
01:12:13,960 --> 01:12:15,740
如果能听他详细描述一下就好了。

1462
01:12:16,140 --> 01:12:22,220
但当我与他交谈时，发现这个概念并不是那么明确的。

1463
01:12:22,560 --> 01:12:24,180
在某种程度上，通用人工智能已经出现了。

1464
01:12:24,320 --> 01:12:27,540
Peter Norvig 写了一篇名为《AGI 已经出现》的文章。

1465
01:12:28,220 --> 01:12:29,260
我不知道这篇文章是否在阅读材料里。

1466
01:12:29,360 --> 01:12:31,080
如果没有，我会把它加入。

1467
01:12:31,480 --> 01:12:37,560
这是一篇与 Blaise Agüera y Arcas 合作的有趣的小文章。

1468
01:12:38,760 --> 01:12:44,740
许多二十年前人们认为 通用人工智能应该具备的能力，

1469
01:12:45,660 --> 01:12:47,200
现在的大语言模型已经实现了。

1470
01:12:47,280 --> 01:12:52,020
虽然可能没有做得那么完美，但它确实在以一种更通用的方式解决问题。

1471
01:12:52,960 --> 01:12:57,160
另一方面，显然目前有很多事情它们做得不如人类。

1472
01:12:57,780 --> 01:13:03,600
令人意外的是，物理任务是人类目前具备比较优势的领域。

1473
01:13:03,920 --> 01:13:08,060
你们可能知道 Moravec 的悖论，

1474
01:13:08,440 --> 01:13:11,380
Hans Moravec 指出，

1475
01:13:11,380 --> 01:13:14,780
通常三岁或四岁的孩子能做的事情，

1476
01:13:14,860 --> 01:13:18,420
比如扣纽扣或上楼梯，

1477
01:13:18,560 --> 01:13:20,440
对机器来说却很难。

1478
01:13:21,020 --> 01:13:23,720
然而，很多博士都觉得困难的事情，

1479
01:13:23,840 --> 01:13:24,600
比如解决凸优化问题，

1480
01:13:24,800 --> 01:13:28,800
机器却往往能做得很好。

1481
01:13:28,920 --> 01:13:31,860
所以，这并不是一个非黑即白的情况......

1482
01:13:32,400 --> 01:13:33,460
人类觉得简单的事情

1483
01:13:33,460 --> 01:13:34,940
电脑却觉得困难

1484
01:13:34,940 --> 01:13:36,360
反过来，

1485
01:13:37,240 --> 01:13:38,640
电脑觉得简单的事情，人类却可能觉得困难。

1486
01:13:38,640 --> 01:13:40,260
这两者并不是在一个相同的尺度上。

1487
01:13:40,260 --> 01:13:43,540
下周，我们将有幸邀请到

1488
01:13:44,540 --> 01:13:46,820
OpenAI 的首席技术官 Mira Marotti

1489
01:13:47,760 --> 01:13:49,560
她曾短暂担任过 OpenAI 的 CEO。

1490
01:13:50,600 --> 01:13:52,020
请带着您的问题来参加我们的讨论。

1491
01:13:52,280 --> 01:13:52,600
期待下次与您见面。
