1
00:00:00,033 --> 00:00:01,866
嗨，我是Sanjana Reddy，

2
00:00:01,866 --> 00:00:05,300
我在谷歌高级解决方案实验室担任机器学习工程师。

3
00:00:06,000 --> 00:00:12,933
最近，生成式AI和相关的新技术引起了大家的关注，包括新的Vertex AI功能，

5
00:00:13,266 --> 00:00:17,733
如Gen AI、Studio Model Garden、Gen AI API等。

6
00:00:18,166 --> 00:00:27,433
在这个简短的课程中，我们的目标是让您能了解Gen AI背后的一些基本概念，打下一个坚实的基础。

8
00:00:28,166 --> 00:00:31,833
今天我要谈谈注意力机制，

9
00:00:32,000 --> 00:00:37,433
它是所有Transformer模型背后推动力，也是LEM模型的核心。

10
00:00:38,400 --> 00:00:44,833
假设您想把英语句子“the cat ate the mouse”翻译成法语。

12
00:00:44,900 --> 00:00:47,033
你可以使用编码器解码器。

13
00:00:47,433 --> 00:00:51,366
这是一种常用的翻译句子的模型。

14
00:00:51,966 --> 00:00:55,533
编码解码器每次只处理一个单词，

15
00:00:55,533 --> 00:00:57,700
然后在每个时间步骤进行翻译。

16
00:00:58,266 --> 00:01:05,233
然而，有时源语言中的单词并不对应目标语言中的单词。

18
00:01:05,633 --> 00:01:07,133
这里有一个例子。

19
00:01:07,133 --> 00:01:10,100
这个句子是“Black cat ate the mouse”。

20
00:01:10,666 --> 00:01:13,833
在这个例子中，英文中的第一个词是“black”，

21
00:01:14,433 --> 00:01:16,500
但在翻译成法文的时候，

22
00:01:16,500 --> 00:01:21,300
第一个词是“chat”，意思是英文中的“cat”。

23
00:01:21,866 --> 00:01:31,700
那么如何训练一个模型，让它在第一步时更多地关注“cat”这个词，而不是“black”这个词，以提高翻译的准确度呢？

26
00:01:31,733 --> 00:01:36,666
你可以在编码解码器中加入所谓的注意力机制。

27
00:01:37,466 --> 00:01:45,100
注意力机制是一种让神经网络能够专注于输入序列特定部分的技术，

29
00:01:45,933 --> 00:01:50,500
它通过为输入序列的不同部分分配权重来实现，

30
00:01:50,833 --> 00:01:54,900
最重要的部分会获得最高的权重。

31
00:01:55,766 --> 00:02:00,166
传统的基于RNN的编码解码器看起来是这样的：

32
00:02:00,766 --> 00:02:03,700
模型一次接收一个单词作为输入，

33
00:02:04,266 --> 00:02:08,433
更新隐藏状态，并将其传递到下一个时间步骤。

34
00:02:09,933 --> 00:02:14,766
在结束时，只有最后的隐藏状态会传递给解码器。

36
00:02:15,933 --> 00:02:23,466
解码器使用最后的隐藏状态进行处理，并将其翻译为目标语言。

38
00:02:24,433 --> 00:02:29,700
注意力模型与传统的序列到序列模型在两个方面有所不同。

40
00:02:30,333 --> 00:02:34,733
首先，编码器向解码器传递更多的数据。

41
00:02:35,400 --> 00:02:45,000
所以，编码器不只是将最后的隐藏状态3传递给解码器，它会将每个时间步骤的所有隐藏状态都传递给解码器。

44
00:02:46,033 --> 00:02:50,966
这让解码器在只有最后隐藏状态的基础上获得了更多的上下文信息。

46
00:02:51,666 --> 00:02:56,400
解码器使用所有隐藏状态的信息来翻译句子。

47
00:02:56,800 --> 00:03:05,500
注意力机制带来的第二个变化是，在产生输出之前，注意力解码器增加了一个额外的步骤。

49
00:03:06,600 --> 00:03:13,433
让我们看看这些步骤是怎么让解码器只关注输入中最相关的部分的。

52
00:03:13,800 --> 00:03:17,100
解码器执行以下操作：

53
00:03:17,100 --> 00:03:21,933
首先，它会观察到它收到的编码器状态集合，

54
00:03:22,933 --> 00:03:28,266
每个编码器的隐藏状态都与输入句子中的某个词有关。

56
00:03:28,733 --> 00:03:31,533
其次，它会给每个隐藏状态一个分数。

57
00:03:32,400 --> 00:03:38,700
然后，它会将每个隐藏状态乘以它的softmax分数，如图所示。

59
00:03:38,700 --> 00:03:46,333
这样就可以增强得分高的隐藏状态，降低得分低的隐藏状态。

61
00:03:47,133 --> 00:03:52,500
如果我们把所有这些元素都连接起来，我们就可以看到注意力网络是如何工作的。

63
00:03:53,033 --> 00:03:57,266
我们继续之前，让我们来定义一下这个幻灯片上的一些符号。

64
00:03:57,933 --> 00:04:01,766
这里的α表示每个时间步骤的注意力率，

65
00:04:02,366 --> 00:04:09,000
H表示编码器RNN在每个时间步骤的隐藏状态，

67
00:04:09,000 --> 00:04:12,466
H下标b表示解码器RNN在

68
00:04:12,466 --> 00:04:16,833
每个时间步骤的隐藏状态。有了注意力机制

69
00:04:17,000 --> 00:04:24,166
"Black Cat"的翻译倒置就在注意力图中清晰可见，

71
00:04:24,533 --> 00:04:27,000
而"ate"在法语中翻译为两个词，"a mange"。

72
00:04:27,266 --> 00:04:33,933
我们可以看到注意力网络在两个时间步骤内都集中在"ate"这个词上。

74
00:04:34,933 --> 00:04:45,100
在注意力步骤中，我们使用编码器隐藏状态和H4向量来计算这个时间步骤的上下文向量a4，

77
00:04:45,666 --> 00:04:48,133
这是一个加权和。

78
00:04:48,133 --> 00:04:52,966
然后，我们将H4和a4合并成一个向量。

80
00:04:54,000 --> 00:04:58,300
这个合并的向量被送入一个前馈神经网络中，

81
00:04:58,733 --> 00:05:04,166
与模型一起训练，以预测下一个词。

83
00:05:04,166 --> 00:05:10,000
这个前馈神经网络的输出代表了这个时间步骤的输出词。

85
00:05:10,500 --> 00:05:17,166
这个过程会一直持续，直到解码器生成句子结束的标记。

87
00:05:17,166 --> 00:05:24,466
这就是你可以如何使用注意力机制来提高传统编码解码架构性能的方式。

89
00:05:25,233 --> 00:05:28,300
非常感谢你的聆听。
