嗨，我是Sanjana Reddy，
我在谷歌高级解决方案实验室担任机器学习工程师。
最近，生成式AI和相关的新技术引起了大家的关注，包括新的Vertex AI功能，

如Gen AI、Studio Model Garden、Gen AI API等。
在这个简短的课程中，我们的目标是让您能了解Gen AI背后的一些基本概念，打下一个坚实的基础。

今天我要谈谈注意力机制，
它是所有Transformer模型背后推动力，也是LEM模型的核心。
假设您想把英语句子“the cat ate the mouse”翻译成法语。

你可以使用编码器解码器。
这是一种常用的翻译句子的模型。
编码解码器每次只处理一个单词，
然后在每个时间步骤进行翻译。
然而，有时源语言中的单词并不对应目标语言中的单词。

这里有一个例子。
这个句子是“Black cat ate the mouse”。
在这个例子中，英文中的第一个词是“black”，
但在翻译成法文的时候，
第一个词是“chat”，意思是英文中的“cat”。
那么如何训练一个模型，让它在第一步时更多地关注“cat”这个词，而不是“black”这个词，以提高翻译的准确度呢？


你可以在编码解码器中加入所谓的注意力机制。
注意力机制是一种让神经网络能够专注于输入序列特定部分的技术，

它通过为输入序列的不同部分分配权重来实现，
最重要的部分会获得最高的权重。
传统的基于RNN的编码解码器看起来是这样的：
模型一次接收一个单词作为输入，
更新隐藏状态，并将其传递到下一个时间步骤。
在结束时，只有最后的隐藏状态会传递给解码器。

解码器使用最后的隐藏状态进行处理，并将其翻译为目标语言。

注意力模型与传统的序列到序列模型在两个方面有所不同。

首先，编码器向解码器传递更多的数据。
所以，编码器不只是将最后的隐藏状态3传递给解码器，它会将每个时间步骤的所有隐藏状态都传递给解码器。


这让解码器在只有最后隐藏状态的基础上获得了更多的上下文信息。

解码器使用所有隐藏状态的信息来翻译句子。
注意力机制带来的第二个变化是，在产生输出之前，注意力解码器增加了一个额外的步骤。

让我们看看这些步骤是怎么让解码器只关注输入中最相关的部分的。


解码器执行以下操作：
首先，它会观察到它收到的编码器状态集合，
每个编码器的隐藏状态都与输入句子中的某个词有关。

其次，它会给每个隐藏状态一个分数。
然后，它会将每个隐藏状态乘以它的softmax分数，如图所示。

这样就可以增强得分高的隐藏状态，降低得分低的隐藏状态。

如果我们把所有这些元素都连接起来，我们就可以看到注意力网络是如何工作的。

我们继续之前，让我们来定义一下这个幻灯片上的一些符号。
这里的α表示每个时间步骤的注意力率，
H表示编码器RNN在每个时间步骤的隐藏状态，

H下标b表示解码器RNN在
每个时间步骤的隐藏状态。有了注意力机制
"Black Cat"的翻译倒置就在注意力图中清晰可见，

而"ate"在法语中翻译为两个词，"a mange"。
我们可以看到注意力网络在两个时间步骤内都集中在"ate"这个词上。

在注意力步骤中，我们使用编码器隐藏状态和H4向量来计算这个时间步骤的上下文向量a4，


这是一个加权和。
然后，我们将H4和a4合并成一个向量。

这个合并的向量被送入一个前馈神经网络中，
与模型一起训练，以预测下一个词。

这个前馈神经网络的输出代表了这个时间步骤的输出词。

这个过程会一直持续，直到解码器生成句子结束的标记。

这就是你可以如何使用注意力机制来提高传统编码解码架构性能的方式。

非常感谢你的聆听。