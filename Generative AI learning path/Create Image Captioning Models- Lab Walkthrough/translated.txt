大家好，我是\NGoogle Advanced Solutions Lab\N的机器学习工程师，Takumi。

这是图像标注课程的后半部分。
如果你还没看过前半部分，我建议你先看一下。
在这个视频中，我将带你详细介绍整个代码Notebook，帮助你理解如何创建一个非常简单的生成模型。

所有的设置信息都写在ASL的GitHub Repo里。

你可以在幻灯片中或者这个视频的描述下面找到链接。

设置好Vertex AI工作台环境并clone Repo后，

按照说明你可以找到图像标注Notebook，它位于"asl-ml-immersion/notebooks/multi_modal/solutions"下面。



看，这里就是图像标注的.ipynb文件。

所以请打开这个文件，在这里你可以看到所有的流程和指示来构建和使用图像标注模型，这些我们在上一个视频中已经讨论过了。



让我们从第一个单元格开始看起。
在第一个单元格里，我们自然要安装所有的依赖，包括tensorflow和keras。


在这里，我们可以找到tensorflow.keras.layers，
并安装我们需要的所有图像标注模型的layers，
包括GRU、Add层、Attention层、Dense层、Embedding层和LayerNormalization层。


让我们逐一运行。
在下一个单元格里，我们定义了一些超参数，包括词汇表大小，


这意味着我们将使用多少个词汇来进行图像标注。
或者你可以找到一个特征提取器，这意味着我们想要在编码器模型中使用什么样的模型。

所以在这种情况下，正如我们在之前的视频中讨论的，我们指定的是InceptionResNetV2，这是一个非常经典的基于CNN的模型。

所有下面的定义，包括图像、高度、宽度通道和特征形状都来自InceptionResNetV2的定义，特别是这个特征形状。


8, 8, 1536就是InceptionResNetV2输出的形状。

那我们就按照这种方式定义吧。

酷，
在下一个单元格中，我们将从tfds加载数据，也就是TensorFlow数据集。

所以 TensorFlow 数据集以“coco_captions”这个名字托管这个标注数据集

所以我们可以指定这个名字来加载数据。
加载数据后，我们可以传递一些预处理函数，

比如获取图像级别，这在这里定义了，获取图像级别。
在这里我们可以找到一些非常基础的预处理，包括改变图像的大小或者图像的比例，并返回图像张量和文字说明同时。



所以让我们按照相同的方式运行。
让我们看一些例子。

这里我们可以看到，例如，一个随机的例子，每一对图像和文本对我来说都是有意义的。


例如，这个图像的标注是“一个装有烤三明治、薯片和炸薯条的宽盘子”，以及另一个图像的标注。


我们有很多图像。如果你想看另一个例子，你可以再运行这个单元格，你会看到另一个例子。


那么让我们继续。
由于我们有文本数据，我们需要以一种标准的方式预处理那些文本数据。

在这个单元格里，我们添加了开始和结束的特殊标记，这也是我们在幻灯片里讨论过的。

通过添加这些，我们可以把这个标记处理成一种特殊的符号，这个开始标记意味着句子的开始。


同样，结束标记意味着句子的结束。
我们可以用一个函数来添加开始标记和结束标记，并将这个函数传入trainds.map。

让我们继续。
这是一个非常重要的预处理。
现在我们有了文本数据，标注数据。
所以我们要创建分词器。
通过创建分词器，我们可以将像开始标记、猫、狗这样的词进行分词到某个索引。


在 TensorFlow 中，这非常简单。
你可以只用这个TextVectorization模块，然后通过传递所有的数据或标注数据到这个TextVectorization层，



这需要一些时间，在我的环境中大约需要5分钟。
让我们等待它完成。
现在已经完成。
现在，让我们尝试一下这个分词器，通过传递一些样本句子，“<start> This is a sentence <end>”。


所以现在你可以看到，它是以这种方式被分词的。

你可以在这里找到很多的填充，通过改变这个最大标注长度（MAX_CAPTION_LEN），你可以控制这个填充的长度。


但在这个案例中，我们指定的是64。
所以所有的标注都将以这种方式被填充，直到达到这个最大标注长度。


同样，你可以看到这个分词器的行为，这非常有用。


一旦你创建了分词器，你可以在不同的标注中应用这个分词器，并将文本数据转化为适当的标记。

在此时创建转换器是非常好的。
所以在这里，我们可以找到StringLookup层，并且创建了转换器，从词到索引，还有从索引到词。

我们稍后将使用这些模块。
所以这非常有用，然后我们可以创建最终的数据集。

这是一个非常重要的部分。
我们有trainds。
我们要添加额外的create_ds_fn函数，这个函数，
如你所见，它返回img_tensor、caption，这是一个元组，img_tensor将进入编码器，caption将进入解码器。



同时，我们还创建了target，即标签。
在这个函数中，你可以发现这个target是从caption中创建的，只需将caption移动一个单词。


好的。
通过这样做，我们将创建一个移位的caption，也就是下一个单词，并且我们将用这个作为target。


所以让我们定义并应用这个函数，并创建一个批处理指定批处理大小，一切都已经准备就绪。


那么让我们看一些数据集。
给你。
你可以看到Image shape、Caption shape，以及与Caption相同的Label shape，因为我们只是做了移位。


所以我们用零值填充了移位部分，看起来不错。

接下来是模型。
大部分模型代码已经在之前的视频中解释过了，所以我会很快地过一遍。

但是，如果你对此不是很熟悉或者不是很有信心，
那么你可以回到之前的幻灯片，查看编码器和解码器内部发生了什么。

在这个视频里，我们来快速运行这些东西。

这是编码器，你可以看到我们只是将inception_resnet_v2应用到图像数据上。

请注意，在这种情况下，我们冻结了这个卷积神经网络（CNN）的大部分内容，因为我们不需要训练。

这个模型基本上是使用大型数据集（在这种情况下是 ImageNet 数据集）预先训练的。

当然，如果你想要再次进行微调，那是可能的，但在这种情况下，我们只想保留预训练的权重。


接下来让我们继续讨论解码器。
正如我们讨论的，它有点复杂。
在这里，您可以找到关于注意力层的许多说明，以及我们在上一个视频中讨论过的解码器的步骤。


在这里，我们可以找到一些定义，你可以找到Embedding层来创建Embedding和第一个GRU层，然后是Attention层、Add层、LayerNormalization层和最后的Dense层。



让我们按这种方式定义。
模型看起来是这样的，Embedding层，GRU，Attention，Add，LayerNormalization，然后是Dense。


它有很多参数，在定义解码器和编码器之后，我们可以创建最终的 TF Keras 模型，并定义输入和输出。



你可以看到，它有两个输入，image_input进入编码器，word_input进入解码器，输出应该是decoder_output。




现在模型已经准备好了，但在运行训练之前，我们需要像往常一样定义损失函数。

在损失方面，我们的模型基本上是一个分类模型，因为解码器为每个类别、每个单词类别、每个词汇生成了很多概率。



所以我们可以像往常一样使用SparseCategoricalCrossentropy来解决分类问题。

但在这种情况下，我们的数据是填充的，所以它有很多零值和很多没有意义的值。

所以我们想要去掉那部分。
为了做到这一点，我们正在定义这个自定义损失函数，然后一切都已经准备就绪。


让我们编译模型，然后可以运行训练。

在训练方面，使用一块T4 GPU进行一轮训练需要 15 到 20 分钟。

所以，如果你想增加额外的轮次，这是可以的。

你可以这样做，我认为你可以得到稍微好一点的结果。
但是一个轮次就足够了，只是为了检查它是如何工作的。

所以让我们保持一个轮次并进行训练，等待15到20分钟，直到训练完成。


现在训练已经完成，所以让我们用它生成图片的文字说明，但在此之前，我们需要为推理重建解码器，以便手动控制增长状态。


正如我们在之前的视频中讨论的。
所以在这个单元格中，通过重用训练过的层，我们正在创建一个用于推理的模型。

所以这里你可以找到训练decoder_gru，训练decoder_attention等。

与训练模型相比，我们添加GRU状态到输入输出。

对于输入，我们添加gru_state_input，对于输出，我们添加gru_state作为输出。


这样，我们可以在推理过程中控制GRU状态。

好的，让我们用这个自定义推理循环函数生成文本。

我们已经在之前的视频中讨论了它应该具有哪些组件，但让我们简要回顾一下。

首先，我们初始化GRU状态，这种情况下只是简单地用零向量初始化。

然后这里我们获取图像，再对图像进行预处理并将其传递给编码器，当然，是训练的编码器。


我们可以获得图像特征.
然后在将其传递给我们的解码器之前,
所以我们也初始化这个，这个开始标记作为第一个词.

然后我们将重复这个整个循环，一次又一次地生成文本。


所以步骤看起来是这样的，编码解码器，当然，然后它返回很多预测的词的概率。


所以有很多方法可以从词概率的列表中挑选出实际的词，最后的词，最后的选择。

但在这种情况下，我们是以某种随机方式挑选词，以引入一些随机性。

这些代码行正在执行此操作，最终选择一些单词，

然后使用分词器将它们从单词Token还原回单词，并追加到列表中。


所以最后我们应该得到一些图片的文字说明。
让我们看看结果。
定义了这个函数，然后调用它。
在这里，你可以看到这张图片的文字说明样本。

这个样本图片位于这个目录中。
只需传递JPEG和它返回五条文字说明。

看起来像是一个棒球运动员站在球棒旁边一个接球员在棒球场上打棒球，或者类似的东西。


虽然语法不完美，
但你仍然可以看到它在生成文本，生成多个文本和生成有意义的文本。

我们还可以看到我们的模型捕捉到了重要的信息，如棒球、接球员或一个人站在另一个人旁边，或棒球场之类的。




虽然不是很完美，但它确实在生成非常有意义的文本。

令人惊讶，不是吗？
这个模型非常简单。
我们只是将编码器和解码器堆叠在一起，然后将图像数据传递给编码器，解码器以自回归的方式逐个生成图像的文字说明。


通过堆叠编码器和解码器，我们可以创建这种非常小的生成模型。

好的。
目前市面上有很多生成性的大语言模型。
当然，它们具有更复杂、更大的网络，并在更大的数据集上进行训练。

但是，这个简单模型的架构可能与它们类似。
非常感谢观看这个视频。
希望你喜欢。
如果你喜欢这个演示，你可以在我们的ASL Github Repo中找到90多个机器学习相关的Notebooks，

如果你觉得有用，请不要忘记给GitHub Repo加星。
