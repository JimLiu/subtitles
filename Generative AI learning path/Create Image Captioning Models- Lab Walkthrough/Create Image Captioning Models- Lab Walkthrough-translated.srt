1
00:00:00,400 --> 00:00:05,633
大家好，我是Google Advanced Solutions Lab的机器学习工程师，Takumi。

3
00:00:06,865 --> 00:00:09,631
这是图像标注课程的后半部分。

4
00:00:10,199 --> 00:00:14,098
如果你还没看过前半部分，我建议你先看一下。

5
00:00:15,733 --> 00:00:24,032
在这个视频中，我将带你详细介绍整个代码Notebook，帮助你理解如何创建一个非常简单的生成模型。

7
00:00:25,865 --> 00:00:29,932
所有的设置信息都写在ASL的GitHub Repo里。

9
00:00:30,899 --> 00:00:35,566
你可以在幻灯片中或者这个视频的描述下面找到链接。

11
00:00:37,265 --> 00:00:42,065
设置好Vertex AI工作台环境并clone Repo后，

13
00:00:42,066 --> 00:00:56,932
按照说明你可以找到图像标注Notebook，它位于"asl-ml-immersion/notebooks/multi_modal/solutions"下面。

17
00:00:57,399 --> 00:01:01,499
看，这里就是图像标注的.ipynb文件。

19
00:01:01,832 --> 00:01:14,733
所以请打开这个文件，在这里你可以看到所有的流程和指示来构建和使用图像标注模型，这些我们在上一个视频中已经讨论过了。

23
00:01:15,900 --> 00:01:19,500
让我们从第一个单元格开始看起。

24
00:01:19,500 --> 00:01:26,432
在第一个单元格里，我们自然要安装所有的依赖，包括tensorflow和keras。

27
00:01:27,165 --> 00:01:30,431
在这里，我们可以找到tensorflow.keras.layers，

28
00:01:30,733 --> 00:01:34,132
并安装我们需要的所有图像标注模型的layers，

29
00:01:34,766 --> 00:01:45,098
包括GRU、Add层、Attention层、Dense层、Embedding层和LayerNormalization层。

32
00:01:45,332 --> 00:01:48,532
让我们逐一运行。

33
00:01:49,832 --> 00:01:58,265
在下一个单元格里，我们定义了一些超参数，包括词汇表大小，

36
00:01:58,266 --> 00:02:03,365
这意味着我们将使用多少个词汇来进行图像标注。

37
00:02:03,365 --> 00:02:10,365
或者你可以找到一个特征提取器，这意味着我们想要在编码器模型中使用什么样的模型。

39
00:02:10,900 --> 00:02:19,366
所以在这种情况下，正如我们在之前的视频中讨论的，我们指定的是InceptionResNetV2，这是一个非常经典的基于CNN的模型。

41
00:02:20,866 --> 00:02:31,799
所有下面的定义，包括图像、高度、宽度通道和特征形状都来自InceptionResNetV2的定义，特别是这个特征形状。

44
00:02:31,832 --> 00:02:39,564
8, 8, 1536就是InceptionResNetV2输出的形状。

46
00:02:41,265 --> 00:02:44,631
那我们就按照这种方式定义吧。

48
00:02:48,566 --> 00:02:49,200
酷，

49
00:02:49,466 --> 00:02:56,100
在下一个单元格中，我们将从tfds加载数据，也就是TensorFlow数据集。

51
00:02:58,165 --> 00:03:03,198
所以 TensorFlow 数据集以“coco_captions”这个名字托管这个标注数据集

53
00:03:03,199 --> 00:03:06,131
所以我们可以指定这个名字来加载数据。

54
00:03:07,733 --> 00:03:12,631
加载数据后，我们可以传递一些预处理函数，

56
00:03:14,032 --> 00:03:18,165
比如获取图像级别，这在这里定义了，获取图像级别。

57
00:03:18,199 --> 00:03:37,566
在这里我们可以找到一些非常基础的预处理，包括改变图像的大小或者图像的比例，并返回图像张量和文字说明同时。

61
00:03:37,566 --> 00:03:39,666
所以让我们按照相同的方式运行。

62
00:03:40,733 --> 00:03:44,066
让我们看一些例子。

64
00:03:51,765 --> 00:04:01,631
这里我们可以看到，例如，一个随机的例子，每一对图像和文本对我来说都是有意义的。

67
00:04:01,633 --> 00:04:11,098
例如，这个图像的标注是“一个装有烤三明治、薯片和炸薯条的宽盘子”，以及另一个图像的标注。

70
00:04:11,932 --> 00:04:21,399
我们有很多图像。如果你想看另一个例子，你可以再运行这个单元格，你会看到另一个例子。

73
00:04:21,399 --> 00:04:23,265
那么让我们继续。

74
00:04:24,132 --> 00:04:30,600
由于我们有文本数据，我们需要以一种标准的方式预处理那些文本数据。

76
00:04:31,533 --> 00:04:41,198
在这个单元格里，我们添加了开始和结束的特殊标记，这也是我们在幻灯片里讨论过的。

78
00:04:41,966 --> 00:04:54,133
通过添加这些，我们可以把这个标记处理成一种特殊的符号，这个开始标记意味着句子的开始。

81
00:04:54,766 --> 00:05:00,033
同样，结束标记意味着句子的结束。

82
00:05:01,000 --> 00:05:07,333
我们可以用一个函数来添加开始标记和结束标记，并将这个函数传入trainds.map。

84
00:05:07,333 --> 00:05:14,699
让我们继续。

85
00:05:14,699 --> 00:05:17,965
这是一个非常重要的预处理。

86
00:05:18,466 --> 00:05:21,566
现在我们有了文本数据，标注数据。

87
00:05:22,000 --> 00:05:25,432
所以我们要创建分词器。

88
00:05:25,432 --> 00:05:35,599
通过创建分词器，我们可以将像开始标记、猫、狗这样的词进行分词到某个索引。

91
00:05:35,600 --> 00:05:38,065
在 TensorFlow 中，这非常简单。

92
00:05:38,065 --> 00:05:51,399
你可以只用这个TextVectorization模块，然后通过传递所有的数据或标注数据到这个TextVectorization层，

96
00:05:52,699 --> 00:05:56,532
这需要一些时间，在我的环境中大约需要5分钟。

97
00:05:56,733 --> 00:06:01,899
让我们等待它完成。

98
00:06:01,899 --> 00:06:03,399
现在已经完成。

99
00:06:03,899 --> 00:06:15,265
现在，让我们尝试一下这个分词器，通过传递一些样本句子，“<start> This is a sentence <end>”。

102
00:06:16,500 --> 00:06:21,599
所以现在你可以看到，它是以这种方式被分词的。

104
00:06:21,600 --> 00:06:31,865
你可以在这里找到很多的填充，通过改变这个最大标注长度（MAX_CAPTION_LEN），你可以控制这个填充的长度。

107
00:06:31,865 --> 00:06:34,631
但在这个案例中，我们指定的是64。

108
00:06:34,899 --> 00:06:47,432
所以所有的标注都将以这种方式被填充，直到达到这个最大标注长度。

111
00:06:47,432 --> 00:06:55,300
同样，你可以看到这个分词器的行为，这非常有用。

114
00:06:55,300 --> 00:07:06,599
一旦你创建了分词器，你可以在不同的标注中应用这个分词器，并将文本数据转化为适当的标记。

116
00:07:07,899 --> 00:07:11,765
在此时创建转换器是非常好的。

117
00:07:12,600 --> 00:07:21,899
所以在这里，我们可以找到StringLookup层，并且创建了转换器，从词到索引，还有从索引到词。

119
00:07:22,300 --> 00:07:25,099
我们稍后将使用这些模块。

120
00:07:25,100 --> 00:07:31,365
所以这非常有用，然后我们可以创建最终的数据集。

122
00:07:32,432 --> 00:07:34,799
这是一个非常重要的部分。

123
00:07:34,800 --> 00:07:36,865
我们有trainds。

124
00:07:37,466 --> 00:07:41,533
我们要添加额外的create_ds_fn函数，这个函数，

125
00:07:42,300 --> 00:07:55,700
如你所见，它返回img_tensor、caption，这是一个元组，img_tensor将进入编码器，caption将进入解码器。

129
00:07:56,466 --> 00:08:02,265
同时，我们还创建了target，即标签。

130
00:08:02,266 --> 00:08:15,100
在这个函数中，你可以发现这个target是从caption中创建的，只需将caption移动一个单词。

133
00:08:16,500 --> 00:08:16,733
好的。

134
00:08:16,733 --> 00:08:32,232
通过这样做，我们将创建一个移位的caption，也就是下一个单词，并且我们将用这个作为target。

137
00:08:32,232 --> 00:08:43,666
所以让我们定义并应用这个函数，并创建一个批处理指定批处理大小，一切都已经准备就绪。

140
00:08:43,666 --> 00:08:49,732
那么让我们看一些数据集。

141
00:08:49,732 --> 00:08:50,199
给你。

142
00:08:50,200 --> 00:09:00,432
你可以看到Image shape、Caption shape，以及与Caption相同的Label shape，因为我们只是做了移位。

145
00:09:00,432 --> 00:09:08,232
所以我们用零值填充了移位部分，看起来不错。

147
00:09:08,666 --> 00:09:12,100
接下来是模型。

148
00:09:12,100 --> 00:09:17,598
大部分模型代码已经在之前的视频中解释过了，所以我会很快地过一遍。

150
00:09:17,966 --> 00:09:22,299
但是，如果你对此不是很熟悉或者不是很有信心，

151
00:09:22,299 --> 00:09:28,266
那么你可以回到之前的幻灯片，查看编码器和解码器内部发生了什么。

153
00:09:28,899 --> 00:09:33,299
在这个视频里，我们来快速运行这些东西。

155
00:09:33,432 --> 00:09:43,531
这是编码器，你可以看到我们只是将inception_resnet_v2应用到图像数据上。

157
00:09:43,533 --> 00:09:51,433
请注意，在这种情况下，我们冻结了这个卷积神经网络（CNN）的大部分内容，因为我们不需要训练。

159
00:09:52,299 --> 00:10:00,233
这个模型基本上是使用大型数据集（在这种情况下是 ImageNet 数据集）预先训练的。

161
00:10:00,832 --> 00:10:14,566
当然，如果你想要再次进行微调，那是可能的，但在这种情况下，我们只想保留预训练的权重。

164
00:10:14,566 --> 00:10:17,332
接下来让我们继续讨论解码器。

165
00:10:18,700 --> 00:10:20,732
正如我们讨论的，它有点复杂。

166
00:10:20,732 --> 00:10:32,066
在这里，您可以找到关于注意力层的许多说明，以及我们在上一个视频中讨论过的解码器的步骤。

169
00:10:32,066 --> 00:10:48,899
在这里，我们可以找到一些定义，你可以找到Embedding层来创建Embedding和第一个GRU层，然后是Attention层、Add层、LayerNormalization层和最后的Dense层。

173
00:10:48,899 --> 00:10:52,198
让我们按这种方式定义。

174
00:10:52,200 --> 00:11:00,232
模型看起来是这样的，Embedding层，GRU，Attention，Add，LayerNormalization，然后是Dense。

177
00:11:01,500 --> 00:11:17,866
它有很多参数，在定义解码器和编码器之后，我们可以创建最终的 TF Keras 模型，并定义输入和输出。

181
00:11:18,832 --> 00:11:37,232
你可以看到，它有两个输入，image_input进入编码器，word_input进入解码器，输出应该是decoder_output。

186
00:11:37,232 --> 00:11:45,831
现在模型已经准备好了，但在运行训练之前，我们需要像往常一样定义损失函数。

188
00:11:45,832 --> 00:11:58,065
在损失方面，我们的模型基本上是一个分类模型，因为解码器为每个类别、每个单词类别、每个词汇生成了很多概率。

192
00:11:58,500 --> 00:12:05,732
所以我们可以像往常一样使用SparseCategoricalCrossentropy来解决分类问题。

194
00:12:05,732 --> 00:12:14,799
但在这种情况下，我们的数据是填充的，所以它有很多零值和很多没有意义的值。

196
00:12:15,000 --> 00:12:18,365
所以我们想要去掉那部分。

197
00:12:18,365 --> 00:12:26,899
为了做到这一点，我们正在定义这个自定义损失函数，然后一切都已经准备就绪。

200
00:12:26,899 --> 00:12:34,066
让我们编译模型，然后可以运行训练。

202
00:12:34,066 --> 00:12:44,099
在训练方面，使用一块T4 GPU进行一轮训练需要 15 到 20 分钟。

204
00:12:45,500 --> 00:12:49,099
所以，如果你想增加额外的轮次，这是可以的。

206
00:12:49,332 --> 00:12:53,032
你可以这样做，我认为你可以得到稍微好一点的结果。

207
00:12:53,600 --> 00:13:00,299
但是一个轮次就足够了，只是为了检查它是如何工作的。

209
00:13:00,299 --> 00:13:10,499
所以让我们保持一个轮次并进行训练，等待15到20分钟，直到训练完成。

212
00:13:11,200 --> 00:13:23,766
现在训练已经完成，所以让我们用它生成图片的文字说明，但在此之前，我们需要为推理重建解码器，以便手动控制增长状态。

215
00:13:24,033 --> 00:13:28,066
正如我们在之前的视频中讨论的。

216
00:13:28,066 --> 00:13:37,599
所以在这个单元格中，通过重用训练过的层，我们正在创建一个用于推理的模型。

218
00:13:37,600 --> 00:13:44,333
所以这里你可以找到训练decoder_gru，训练decoder_attention等。

220
00:13:45,732 --> 00:13:53,600
与训练模型相比，我们添加GRU状态到输入输出。

222
00:13:55,732 --> 00:14:03,999
对于输入，我们添加gru_state_input，对于输出，我们添加gru_state作为输出。

225
00:14:04,000 --> 00:14:10,966
这样，我们可以在推理过程中控制GRU状态。

227
00:14:10,966 --> 00:14:17,399
好的，让我们用这个自定义推理循环函数生成文本。

229
00:14:18,765 --> 00:14:26,132
我们已经在之前的视频中讨论了它应该具有哪些组件，但让我们简要回顾一下。

231
00:14:27,466 --> 00:14:33,499
首先，我们初始化GRU状态，这种情况下只是简单地用零向量初始化。

233
00:14:34,799 --> 00:14:44,832
然后这里我们获取图像，再对图像进行预处理并将其传递给编码器，当然，是训练的编码器。

236
00:14:45,265 --> 00:14:47,999
我们可以获得图像特征.

237
00:14:49,500 --> 00:14:51,633
然后在将其传递给我们的解码器之前,

238
00:14:52,033 --> 00:14:58,999
所以我们也初始化这个，这个开始标记作为第一个词.

240
00:14:59,533 --> 00:15:08,533
然后我们将重复这个整个循环，一次又一次地生成文本。

243
00:15:09,232 --> 00:15:19,764
所以步骤看起来是这样的，编码解码器，当然，然后它返回很多预测的词的概率。

246
00:15:20,299 --> 00:15:29,531
所以有很多方法可以从词概率的列表中挑选出实际的词，最后的词，最后的选择。

248
00:15:29,932 --> 00:15:38,832
但在这种情况下，我们是以某种随机方式挑选词，以引入一些随机性。

250
00:15:38,832 --> 00:15:45,064
这些代码行正在执行此操作，最终选择一些单词，

252
00:15:45,299 --> 00:15:58,165
然后使用分词器将它们从单词Token还原回单词，并追加到列表中。

255
00:15:58,166 --> 00:16:00,799
所以最后我们应该得到一些图片的文字说明。

256
00:16:01,100 --> 00:16:03,466
让我们看看结果。

257
00:16:03,466 --> 00:16:06,166
定义了这个函数，然后调用它。

258
00:16:10,432 --> 00:16:17,365
在这里，你可以看到这张图片的文字说明样本。

260
00:16:17,365 --> 00:16:22,031
这个样本图片位于这个目录中。

261
00:16:22,332 --> 00:16:27,799
只需传递JPEG和它返回五条文字说明。

263
00:16:29,000 --> 00:16:39,032
看起来像是一个棒球运动员站在球棒旁边一个接球员在棒球场上打棒球，或者类似的东西。

266
00:16:39,832 --> 00:16:42,364
虽然语法不完美，

267
00:16:42,566 --> 00:16:50,898
但你仍然可以看到它在生成文本，生成多个文本和生成有意义的文本。

269
00:16:51,399 --> 00:17:07,365
我们还可以看到我们的模型捕捉到了重要的信息，如棒球、接球员或一个人站在另一个人旁边，或棒球场之类的。

274
00:17:08,333 --> 00:17:16,432
虽然不是很完美，但它确实在生成非常有意义的文本。

276
00:17:16,432 --> 00:17:18,632
令人惊讶，不是吗？

277
00:17:18,633 --> 00:17:20,833
这个模型非常简单。

278
00:17:21,032 --> 00:17:32,931
我们只是将编码器和解码器堆叠在一起，然后将图像数据传递给编码器，解码器以自回归的方式逐个生成图像的文字说明。

281
00:17:33,965 --> 00:17:39,065
通过堆叠编码器和解码器，我们可以创建这种非常小的生成模型。

283
00:17:40,299 --> 00:17:41,099
好的。

284
00:17:41,099 --> 00:17:45,565
目前市面上有很多生成性的大语言模型。

285
00:17:45,566 --> 00:17:51,266
当然，它们具有更复杂、更大的网络，并在更大的数据集上进行训练。

287
00:17:52,133 --> 00:17:55,500
但是，这个简单模型的架构可能与它们类似。

288
00:17:56,766 --> 00:17:58,833
非常感谢观看这个视频。

289
00:17:58,833 --> 00:18:00,599
希望你喜欢。

290
00:18:00,766 --> 00:18:08,400
如果你喜欢这个演示，你可以在我们的ASL Github Repo中找到90多个机器学习相关的Notebooks，

292
00:18:10,133 --> 00:18:14,398
如果你觉得有用，请不要忘记给GitHub Repo加星。
