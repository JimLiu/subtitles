嗨，我是Sanjana Reddy，我在谷歌的高级解决方案实验室担任机器学习工程师。

最近，生成式AI以及相关的新的技术，包括即将出现的新的顶点AI特性，如Gen AI Studio, Model Garden, Gen AI API等，引起了大量的关注和热议。


在这个简短的课程中，我们的目标是让你对构成所有Gen AI神奇的基本概念有一个扎实的理解。


今天，我将要谈论的是Transformer模型和BERT模型。

语言建模技术在过去几年中发生了很大的变化。
过去十年的最新突破包括使用神经网络来表示文本，

例如2013年的Word2vec和N-grams。
2014年，序列到序列模型的发展，
如RNN和LSTM，
帮助提高了机器学习模型在自然语言处理任务上的性能，
如翻译和文本分类。
2015年，随着注意力机制的出现及其基于该机制建立的模型，如Transformer和Bert模型，引起了人们的关注。


在这个课程中，我们将重点讨论Transformer。
Transformer的基础是2017年的一篇论文，名为《Attention As All You Need》。

尽管Transformer之前的所有模型都能将单词表示为向量，
但这些向量并不包含上下文，
而单词的使用会根据上下文而变化。
例如，"bank"和"river bank"与"bank robber"中的"bank"在注意力机制出现之前可能具有相同的向量表示。


Transformer是一个使用注意力机制的编码解码模型。

它可以利用复数化和同时处理大量数据，因为它的模型架构，注意力机制有助于提高机器翻译应用的性能。




Transformer模型是使用注意力机制为核心构建的。

一个Transformer模型包括编码器和解码器。

编码器对输入序列进行编码，并将其传递给解码器，解码器解码表示以用于相关任务。

编码组件是一个由相同数量的编码器堆叠而成的栈。

引入Transformer的研究论文将六个编码器堆叠在一起。

六并不是一个神奇的数字，它只是一个超参数（Hyperparameter）。

这些编码器在结构上都是相同的，但权重不同。
每个编码器都可以分解成两个子层。
第一层称为自注意力。
编码器的输入首先流经一个自注意力层，这有助于编码或查看单词的相关部分，因为它编码输入句子中的中心词。


第二层被称为前馈层。
自注意力层的输出被送入前馈神经网络。

相同的前馈神经网络独立应用于每个位置。

解码器既有自注意力层，也有前馈层，但在它们之间是编码器解码器注意力层，帮助解码器关注输入句子的相关部分。




在对输入序列中的单词进行嵌入（Embedding）后，每个嵌入向量都会流经编码器的两层。

每个位置的单词都经过一个自注意力过程。
然后它通过一个前馈神经网络，每个向量分别流过相同的网络。

在这个自注意力层中，这些路径之间存在依赖关系。

然而，前馈层没有这些依赖关系，因此各种路径可以在它们流经前馈层时并行执行。


在自注意力层，输入嵌入被分解成查询向量、键向量和值向量。

这些向量是通过Transformer在训练过程中学到的权重计算的。

所有这些计算都是在模型中以矩阵计算的形式并行进行的。

一旦我们有了查询键和值向量，下一步就是将每个值向量乘以"Softmax"分数，以便将它们加起来。


这里的目的是保持你想要关注的单词的值不变，并通过将它们乘以很小的数字（例如0.001）来排除不相关的单词。



接下来，我们需要对加权值向量求和，从而得到这个位置的自注意力层的输出。

对于第一个单词，你可以将结果向量发送到前馈神经网络。

总结一下这个过程，我们采取以下步骤来获得最终的嵌入。

我们从自然语言句子开始，嵌入句子中的每个单词。

然后，我们在这种情况下执行8次多头注意力，并将这个嵌入的单词与相应的加权矩阵相乘。


我们首先使用结果Q K.V矩阵计算注意力。
然后，我们可以将矩阵连接起来生成输出矩阵，这与该层最初获得的最终矩阵具有相同的维度。


现在有多种Transformer的变体。
有些使用原始架构中的编码器和解码器部分。
有些只使用编码器，有些只使用解码器。

一个流行的只有编码器架构是Bert。
Bert是一种训练过的Transformer模型。
Bert代表双向编码器表示，来自Transformer，由谷歌于2018年开发。

从那时起，已经建立了多种Bert的变体。

如今，Bert为谷歌搜索提供动力。
你可以看到Bert为同一搜索查询提供的结果有多么不同。

在此之前和之后，Bert分为两种变体进行训练。

一个模型包含Bert Base，它有12个Transformer的堆栈，大约有1.1亿个参数，


另一个是Bert Large，有24层Transformer，大约有3.4亿个参数。

Bert模型之所以强大，是因为它可以处理长输入上下文。
它在整个维基百科语料库和书籍语料库上进行了训练。
Bert模型经过了100万步的训练。
Bert在不同任务上进行了训练，这意味着它具有多任务目标。

这使得Bert非常强大，因为它接受了各种任务的训练。

它既可以在句子级别上工作，也可以在Token级别上工作。
这是最初发布的两个不同版本的Bert。

一个是Bert Base，它有12层，而Bert Large有24层。

与原始的Transformer相比，它只有六层。
Bert的工作方式是在两个不同的任务上进行训练。
任务一称为掩码语言模型，其中句子被掩盖，模型被训练来预测被掩盖的单词。

如果要从头开始训练Bert，你将不得不掩盖语料库中的一定百分比的单词。

建议的掩盖百分比是15%。
掩盖百分比实现了过少和过多掩盖之间的平衡。

太少的遮罩使训练过程非常昂贵，太多的遮罩则去除了模型所需的上下文。

第二个任务是预测下一句话。
例如，模型给出了两组句子。
Bert的目标是学习句子之间的关系，并根据第一句预测下一句。

例如，句子A可能是一个人去了商店，句子B是他买了一加仑牛奶。


Bert负责分类句子B是否在句子A之后的下一句。

这是一个二分类任务。
这帮助Bert在句子级别上表现良好，为了训练Bert，
你需要为输入句子向模型提供三种不同类型的嵌入，

分别是Token嵌入、段落嵌入和位置嵌入。

Token嵌入是输入句子中每个Token的表示。

单词被转换为特定维度的向量表示。

Bert可以解决涉及文本分类的自然语言处理任务。

一个例子是，对于两句话“我的狗很可爱”和“他喜欢玩耍”，我们需要分类这两句话在语义上是否相似。

将输入文本对简单地连接起来并输入模型。
那么Bert如何区分给定对中的输入呢？
答案是使用段落嵌入。
有一个由SEP表示的特殊标记，用于分隔句子的两个不同部分。

另一个问题是学习句子中单词的顺序。

众所周知，Bert由一堆Transformer组成。
Bert设计用于处理最长为512的输入序列。
输入序列的顺序被整合到位置嵌入中。
这使得Bert能够为每个位置学习一个向量表示。
Bert可用于不同的下游任务。
尽管Bert是在大规模语言建模和单句分类上进行训练的，但它可以用于流行的NLP任务，如单句分类、句子对分类、问答和单句标记任务。




感谢你的聆听。