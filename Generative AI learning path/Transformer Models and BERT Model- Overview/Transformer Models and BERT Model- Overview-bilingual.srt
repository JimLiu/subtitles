1
00:00:00,100 --> 00:00:05,764
嗨，我是Sanjana Reddy，我在谷歌的高级解决方案实验室担任机器学习工程师。
Hi. I'm Sanjana Reddy, a machine learning engineer at Google's Advanced Solutions Lab.

3
00:00:06,533 --> 00:00:20,165
最近，生成式AI以及相关的新的技术，包括即将出现的新的顶点AI特性，如Gen AI Studio, Model Garden, Gen AI API等，引起了大量的关注和热议。
There's been a lot of excitement around generative AI and all the new advancements, including new vertex AI features that are coming up, such as Gen AI Studio, Model Garden, Gen AI API.

6
00:00:20,166 --> 00:00:30,032
在这个简短的课程中，我们的目标是让你对构成所有Gen AI神奇的基本概念有一个扎实的理解。
Our objective in this short session is to give you a solid footing on some of the underlying concepts that make all the Gen AI magic possible.

9
00:00:30,899 --> 00:00:35,964
今天，我将要谈论的是Transformer模型和BERT模型。
Today I'm going to talk about transformer models and the BERT model.

11
00:00:37,066 --> 00:00:39,732
语言建模技术在过去几年中发生了很大的变化。
Language modeling has evolved over the years.

12
00:00:40,432 --> 00:00:47,365
过去十年的最新突破包括使用神经网络来表示文本，
The recent breakthroughs in the past ten years include the usage of neural networks to represent text,

14
00:00:47,466 --> 00:00:50,999
例如2013年的Word2vec和N-grams。
such as Word2vec an N-grams in 2013.

15
00:00:51,799 --> 00:00:55,832
2014年，序列到序列模型的发展，
In 2014, the development of sequence to sequence models

16
00:00:56,200 --> 00:00:58,466
如RNN和LSTM，
such as RNN's and LSTM’s

17
00:00:58,865 --> 00:01:03,331
帮助提高了机器学习模型在自然语言处理任务上的性能，
helped improve the performance of ML models on NLP tasks

18
00:01:03,533 --> 00:01:06,766
如翻译和文本分类。
such as translation and text classification.

19
00:01:07,500 --> 00:01:16,964
2015年，随着注意力机制的出现及其基于该机制建立的模型，如Transformer和Bert模型，引起了人们的关注。
In 2015, the excitement came with attention mechanisms and the models built based on it, such as Transformers and the Bert model.

22
00:01:17,665 --> 00:01:20,999
在这个课程中，我们将重点讨论Transformer。
In this presentation we'll focus on Transformers.

23
00:01:22,000 --> 00:01:28,533
Transformer的基础是2017年的一篇论文，名为《Attention As All You Need》。
Transformers is based on a 2017 paper named Attention As All You Need.

25
00:01:29,233 --> 00:01:34,165
尽管Transformer之前的所有模型都能将单词表示为向量，
Although all the models before Transformers were able to represent words

26
00:01:34,465 --> 00:01:38,931
但这些向量并不包含上下文，
as vectors, these vectors did not contain the context

27
00:01:39,599 --> 00:01:42,965
而单词的使用会根据上下文而变化。
and the usage of words changes based on the context.

28
00:01:43,433 --> 00:01:54,564
例如，"bank"和"river bank"与"bank robber"中的"bank"在注意力机制出现之前可能具有相同的向量表示。
For example, bank and river bank versus bank in bank robber might have the same vector representation before attention mechanisms came about.

31
00:01:55,132 --> 00:02:01,465
Transformer是一个使用注意力机制的编码解码模型。
A transformer is an encoder decoder model that uses the attention mechanism.

33
00:02:01,465 --> 00:02:17,132
它可以利用复数化和同时处理大量数据，因为它的模型架构，注意力机制有助于提高机器翻译应用的性能。
It can take advantage of pluralization and also process a large amount of data at the same time. because of its model architecture, attention mechanism helps improve the performance of machine translation applications.

38
00:02:17,733 --> 00:02:22,365
Transformer模型是使用注意力机制为核心构建的。
Transformer models were built using attention mechanisms at the core.

40
00:02:23,699 --> 00:02:28,066
一个Transformer模型包括编码器和解码器。
A transformer model consists of encoder and decoder.

42
00:02:28,932 --> 00:02:39,132
编码器对输入序列进行编码，并将其传递给解码器，解码器解码表示以用于相关任务。
The encoder encodes the input sequence and passes it to the decoder and the decoder decodes the representation for a relevant task.

44
00:02:40,265 --> 00:02:44,700
编码组件是一个由相同数量的编码器堆叠而成的栈。
The encoding component is a stack of encoders of the same number.

46
00:02:45,300 --> 00:02:50,465
引入Transformer的研究论文将六个编码器堆叠在一起。
The research paper that introduced Transformers stack six encoders on top of each other.

48
00:02:51,300 --> 00:02:56,099
六并不是一个神奇的数字，它只是一个超参数（Hyperparameter）。
Six is not a magical number. It's just a hyperparameter.

50
00:02:56,099 --> 00:03:00,365
这些编码器在结构上都是相同的，但权重不同。
The encoders are all identical in structure, but with different weights.

51
00:03:01,032 --> 00:03:04,931
每个编码器都可以分解成两个子层。
Each encoder can be broken down into two sub layers.

52
00:03:05,633 --> 00:03:08,532
第一层称为自注意力。
The first layer is called self attention.

53
00:03:09,300 --> 00:03:21,099
编码器的输入首先流经一个自注意力层，这有助于编码或查看单词的相关部分，因为它编码输入句子中的中心词。
The input of the encode are first flows through a self attention layer, which helps to encode or look at relevant parts of the words as it encodes a central word in the input sentence.

56
00:03:22,265 --> 00:03:25,398
第二层被称为前馈层。
And the second layer is called a feedforward layer.

57
00:03:25,932 --> 00:03:30,600
自注意力层的输出被送入前馈神经网络。
The output of the self attention layer is fed to the feedforward neural network.

59
00:03:31,699 --> 00:03:37,233
相同的前馈神经网络独立应用于每个位置。
The exact same feedforward neural network is independently applied to each position.

61
00:03:38,800 --> 00:03:53,399
解码器既有自注意力层，也有前馈层，但在它们之间是编码器解码器注意力层，帮助解码器关注输入句子的相关部分。
The decoder has both the self attention and the feedforward layer, but between them is the encoder decoder, attention layer that helps a decoder focus on relevant parts of the input sentence.

66
00:03:53,400 --> 00:04:00,266
在对输入序列中的单词进行嵌入（Embedding）后，每个嵌入向量都会流经编码器的两层。
After embedding the words in the input sequence, each of the embedding vector flows through the two layers of the encoder.

68
00:04:01,066 --> 00:04:05,500
每个位置的单词都经过一个自注意力过程。
The word at each position passes through a self attention process.

69
00:04:06,066 --> 00:04:14,131
然后它通过一个前馈神经网络，每个向量分别流过相同的网络。
Then it passes through a feedforward neural network, the exact same network with each vector flowing through it separately.

71
00:04:15,165 --> 00:04:21,565
在这个自注意力层中，这些路径之间存在依赖关系。
Dependencies exist between these paths in this self attention layer.

73
00:04:21,565 --> 00:04:33,032
然而，前馈层没有这些依赖关系，因此各种路径可以在它们流经前馈层时并行执行。
However, the feedforward layer does not have these dependencies and therefore various paths can be executed in parallel while they flow through the feedforward layer.

76
00:04:34,165 --> 00:04:41,533
在自注意力层，输入嵌入被分解成查询向量、键向量和值向量。
In the self attention layer, the input embedding is broken up into query, key, and value vectors.

78
00:04:42,165 --> 00:04:48,765
这些向量是通过Transformer在训练过程中学到的权重计算的。
These vectors are computed using weights that the transformer learns during the training process.

80
00:04:49,800 --> 00:04:55,300
所有这些计算都是在模型中以矩阵计算的形式并行进行的。
All of these computations happen in parallel in the model, in the form of matrix computation.

82
00:04:56,466 --> 00:05:07,565
一旦我们有了查询键和值向量，下一步就是将每个值向量乘以"Softmax"分数，以便将它们加起来。
Once we have the query key and value vectors, the next step is to multiply each value vector by the soft max score in preparation to sum them up.

85
00:05:08,266 --> 00:05:21,632
这里的目的是保持你想要关注的单词的值不变，并通过将它们乘以很小的数字（例如0.001）来排除不相关的单词。
The intention here is to keep intact the values of the words you want to focus on and leave out a irrelevant words by multiplying them by tiny numbers like 0.001, for example.

89
00:05:22,800 --> 00:05:31,033
接下来，我们需要对加权值向量求和，从而得到这个位置的自注意力层的输出。
Next, we have to sum up the weighted value vectors which produces the output of the self attention layer at this position.

91
00:05:31,065 --> 00:05:37,333
对于第一个单词，你可以将结果向量发送到前馈神经网络。
For the first word, you can send along the resulting vector to the feedforward neural network.

93
00:05:38,665 --> 00:05:43,833
总结一下这个过程，我们采取以下步骤来获得最终的嵌入。
To sum up this process of getting the final embeddings, these are the steps that we take.

95
00:05:44,833 --> 00:05:50,800
我们从自然语言句子开始，嵌入句子中的每个单词。
We start with the natural language sentence embed each word in the sentence.

97
00:05:51,899 --> 00:06:02,265
然后，我们在这种情况下执行8次多头注意力，并将这个嵌入的单词与相应的加权矩阵相乘。
After that, we perform multi-headed attention eight times in this case and multiply this embedded word with the respective weighted matrices.

100
00:06:02,932 --> 00:06:07,832
我们首先使用结果Q K.V矩阵计算注意力。
We then calculate the attention using the resulting Q K.V.

101
00:06:08,033 --> 00:06:18,732
然后，我们可以将矩阵连接起来生成输出矩阵，这与该层最初获得的最终矩阵具有相同的维度。
matrices. Finally, we can concatenate the matrices to produce the output matrix, which is the same dimension as the final matrix

104
00:06:18,733 --> 00:06:21,132
现在有多种Transformer的变体。
that this layer initially got.

105
00:06:22,065 --> 00:06:25,265
有些使用原始架构中的编码器和解码器部分。
There's multiple variations of transformers out there now.

106
00:06:26,100 --> 00:06:35,400
有些只使用编码器，有些只使用解码器。
Some use both the encoder and the decoder component from the original architecture. Some use only the encoder and some use only the decoder.

108
00:06:36,365 --> 00:06:39,699
一个流行的只有编码器架构是Bert。
A popular encoder only architecture is Bert.

109
00:06:40,500 --> 00:06:43,665
Bert是一种训练过的Transformer模型。
Bert is one of the trained transformer models.

110
00:06:44,266 --> 00:06:52,633
Bert代表双向编码器表示，来自Transformer，由谷歌于2018年开发。
Bert stands for bidirectional encoder representations from transformers and was developed by Google in 2018.

112
00:06:54,365 --> 00:06:57,899
从那时起，已经建立了多种Bert的变体。
Since then, multiple variations of Bert have been built.

114
00:06:58,365 --> 00:07:00,965
如今，Bert为谷歌搜索提供动力。
Today, Bert Powers Google Search.

115
00:07:01,865 --> 00:07:06,665
你可以看到Bert为同一搜索查询提供的结果有多么不同。
You can see how different the results provided by Bert are for the same search query.

117
00:07:06,899 --> 00:07:12,100
在此之前和之后，Bert分为两种变体进行训练。
Before and after, Bert was trained in two variations.

119
00:07:12,565 --> 00:07:21,065
一个模型包含Bert Base，它有12个Transformer的堆栈，大约有1.1亿个参数，
One model contains Bert Base, which had 12 stock of Transformers with approximately 110 million

122
00:07:21,065 --> 00:07:29,465
另一个是Bert Large，有24层Transformer，大约有3.4亿个参数。
parameters, and the other Bert Large with 24 layers of transformers with about 340 million parameters.

124
00:07:30,466 --> 00:07:35,499
Bert模型之所以强大，是因为它可以处理长输入上下文。
The Bert model is powerful because it can handle long input context.

125
00:07:36,165 --> 00:07:40,299
它在整个维基百科语料库和书籍语料库上进行了训练。
It was trained on the entire Wikipedia corpus and books corpus.

126
00:07:41,300 --> 00:07:44,099
Bert模型经过了100万步的训练。
The Bert model was trained for 1 million steps.

127
00:07:44,800 --> 00:07:49,864
Bert在不同任务上进行了训练，这意味着它具有多任务目标。
Bert is trained on different tasks, which means it has multi-task objective.

129
00:07:50,466 --> 00:07:55,633
这使得Bert非常强大，因为它接受了各种任务的训练。
This makes Bert very powerful because of the kind of tasks it was trained on.

131
00:07:55,665 --> 00:07:59,632
它既可以在句子级别上工作，也可以在Token级别上工作。
It works at both a sentence level and at a token level.

132
00:08:00,699 --> 00:08:04,833
这是最初发布的两个不同版本的Bert。
These are the two different versions of Bert that were originally released.

134
00:08:05,300 --> 00:08:11,431
一个是Bert Base，它有12层，而Bert Large有24层。
One is Bert Base, which had 12 layers, whereas Bert Large had 24 layers.

136
00:08:11,699 --> 00:08:15,299
与原始的Transformer相比，它只有六层。
And compared to the original transformer, which had six layers.

137
00:08:16,333 --> 00:08:20,833
Bert的工作方式是在两个不同的任务上进行训练。
The way that Bert works is that it was trained on two different tasks.

138
00:08:21,132 --> 00:08:30,633
任务一称为掩码语言模型，其中句子被掩盖，模型被训练来预测被掩盖的单词。
Task one is called a masked language model, where the sentences are masked and the model is trained to predict the masked words.

140
00:08:31,199 --> 00:08:38,233
如果要从头开始训练Bert，你将不得不掩盖语料库中的一定百分比的单词。
If you were to train Bert from scratch, you would have to mask a certain percentage of the words in your corpus.

142
00:08:38,799 --> 00:08:42,231
建议的掩盖百分比是15%。
The recommended percentage for masking is 15%.

143
00:08:42,899 --> 00:08:47,731
掩盖百分比实现了过少和过多掩盖之间的平衡。
The masking percentage achieves a balance between too little and too much masking.

145
00:08:48,365 --> 00:08:57,265
太少的遮罩使训练过程非常昂贵，太多的遮罩则去除了模型所需的上下文。
Too little masking makes the training process extremely expensive, and too much masking removes the context of the model requires.

147
00:08:58,133 --> 00:09:01,733
第二个任务是预测下一句话。
The second task is to predict the next sentence.

148
00:09:02,399 --> 00:09:06,132
例如，模型给出了两组句子。
For example, the model is given two sets of sentences.

149
00:09:06,600 --> 00:09:12,833
Bert的目标是学习句子之间的关系，并根据第一句预测下一句。
Bert aims to learn the relationships between sentences and predict the next sentence given the first one.

151
00:09:13,932 --> 00:09:21,864
例如，句子A可能是一个人去了商店，句子B是他买了一加仑牛奶。
For example, sentence A could be a man went to the store and sentence B is he bought a gallon of milk.

154
00:09:22,966 --> 00:09:29,733
Bert负责分类句子B是否在句子A之后的下一句。
Bert is responsible for classifying if sentence B is in next sentence after sentence A.

156
00:09:30,166 --> 00:09:32,633
这是一个二分类任务。
This is a binary classification task.

157
00:09:33,600 --> 00:09:38,699
这帮助Bert在句子级别上表现良好，为了训练Bert，
This helps Bert perform at a sentence level in order to train Bert.

158
00:09:38,899 --> 00:09:45,231
你需要为输入句子向模型提供三种不同类型的嵌入，
You need to feed three different kinds of embeddings to the model for the input sentence.

160
00:09:45,265 --> 00:09:50,533
分别是Token嵌入、段落嵌入和位置嵌入。
You get three different embeddings token segment and position embeddings.

162
00:09:51,832 --> 00:09:58,832
Token嵌入是输入句子中每个Token的表示。
The token embeddings is a representation of each token as an embedding in the input sentence.

164
00:09:59,399 --> 00:10:04,500
单词被转换为特定维度的向量表示。
The words are transformed into vector representations of certain dimensions.

166
00:10:05,600 --> 00:10:10,199
Bert可以解决涉及文本分类的自然语言处理任务。
Bert can solve NLP tasks that involve tex classification as well.

168
00:10:10,799 --> 00:10:18,600
一个例子是，对于两句话“我的狗很可爱”和“他喜欢玩耍”，我们需要分类这两句话在语义上是否相似。
An example is to classify whether two sentences say my dog is cute and he likes playing are semantically similar.

170
00:10:18,932 --> 00:10:23,465
将输入文本对简单地连接起来并输入模型。
The pairs of input texts are simply concatenated and fed into the model.

171
00:10:23,899 --> 00:10:27,532
那么Bert如何区分给定对中的输入呢？
How does Bert distinguish the input in a given pair?

172
00:10:28,033 --> 00:10:30,832
答案是使用段落嵌入。
The answer is to use segment embeddings.

173
00:10:31,566 --> 00:10:38,833
有一个由SEP表示的特殊标记，用于分隔句子的两个不同部分。
There is a special token represented by SEP that separates the two different splits of the sentence.

175
00:10:39,799 --> 00:10:43,666
另一个问题是学习句子中单词的顺序。
Another problem is to learn the order of the words in the sentence.

177
00:10:44,799 --> 00:10:48,331
众所周知，Bert由一堆Transformer组成。
As you know, Bert consists of a stack of transformers.

178
00:10:48,899 --> 00:10:53,998
Bert设计用于处理最长为512的输入序列。
Bert is designed to process input sequences up to a length of 512.

179
00:10:54,832 --> 00:10:59,965
输入序列的顺序被整合到位置嵌入中。
The order of the input sequence is incorporated into the position embeddings.

180
00:11:00,399 --> 00:11:05,565
这使得Bert能够为每个位置学习一个向量表示。
This allows Bert to learn a vector representation for each position.

181
00:11:06,500 --> 00:11:09,932
Bert可用于不同的下游任务。
Bert can be used for different downstream tasks.

182
00:11:10,533 --> 00:11:28,231
尽管Bert是在大规模语言建模和单句分类上进行训练的，但它可以用于流行的NLP任务，如单句分类、句子对分类、问答和单句标记任务。
Although Bert was trained on mass language modeling and single sentence classification, it can be used for popular NLP tasks like single sentence classification. Sentence Pair Classification. Question Answering and single sentence tagging tasks.

187
00:11:28,865 --> 00:11:31,898
感谢你的聆听。
Thank you for listening.
