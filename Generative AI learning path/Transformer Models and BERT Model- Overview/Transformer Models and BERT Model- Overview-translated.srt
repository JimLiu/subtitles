1
00:00:00,100 --> 00:00:05,764
嗨，我是Sanjana Reddy，我在谷歌的高级解决方案实验室担任机器学习工程师。

3
00:00:06,533 --> 00:00:20,165
最近，生成式AI以及相关的新的技术，包括即将出现的新的顶点AI特性，如Gen AI Studio, Model Garden, Gen AI API等，引起了大量的关注和热议。

6
00:00:20,166 --> 00:00:30,032
在这个简短的课程中，我们的目标是让你对构成所有Gen AI神奇的基本概念有一个扎实的理解。

9
00:00:30,899 --> 00:00:35,964
今天，我将要谈论的是Transformer模型和BERT模型。

11
00:00:37,066 --> 00:00:39,732
语言建模技术在过去几年中发生了很大的变化。

12
00:00:40,432 --> 00:00:47,365
过去十年的最新突破包括使用神经网络来表示文本，

14
00:00:47,466 --> 00:00:50,999
例如2013年的Word2vec和N-grams。

15
00:00:51,799 --> 00:00:55,832
2014年，序列到序列模型的发展，

16
00:00:56,200 --> 00:00:58,466
如RNN和LSTM，

17
00:00:58,865 --> 00:01:03,331
帮助提高了机器学习模型在自然语言处理任务上的性能，

18
00:01:03,533 --> 00:01:06,766
如翻译和文本分类。

19
00:01:07,500 --> 00:01:16,964
2015年，随着注意力机制的出现及其基于该机制建立的模型，如Transformer和Bert模型，引起了人们的关注。

22
00:01:17,665 --> 00:01:20,999
在这个课程中，我们将重点讨论Transformer。

23
00:01:22,000 --> 00:01:28,533
Transformer的基础是2017年的一篇论文，名为《Attention As All You Need》。

25
00:01:29,233 --> 00:01:34,165
尽管Transformer之前的所有模型都能将单词表示为向量，

26
00:01:34,465 --> 00:01:38,931
但这些向量并不包含上下文，

27
00:01:39,599 --> 00:01:42,965
而单词的使用会根据上下文而变化。

28
00:01:43,433 --> 00:01:54,564
例如，"bank"和"river bank"与"bank robber"中的"bank"在注意力机制出现之前可能具有相同的向量表示。

31
00:01:55,132 --> 00:02:01,465
Transformer是一个使用注意力机制的编码解码模型。

33
00:02:01,465 --> 00:02:17,132
它可以利用复数化和同时处理大量数据，因为它的模型架构，注意力机制有助于提高机器翻译应用的性能。

38
00:02:17,733 --> 00:02:22,365
Transformer模型是使用注意力机制为核心构建的。

40
00:02:23,699 --> 00:02:28,066
一个Transformer模型包括编码器和解码器。

42
00:02:28,932 --> 00:02:39,132
编码器对输入序列进行编码，并将其传递给解码器，解码器解码表示以用于相关任务。

44
00:02:40,265 --> 00:02:44,700
编码组件是一个由相同数量的编码器堆叠而成的栈。

46
00:02:45,300 --> 00:02:50,465
引入Transformer的研究论文将六个编码器堆叠在一起。

48
00:02:51,300 --> 00:02:56,099
六并不是一个神奇的数字，它只是一个超参数（Hyperparameter）。

50
00:02:56,099 --> 00:03:00,365
这些编码器在结构上都是相同的，但权重不同。

51
00:03:01,032 --> 00:03:04,931
每个编码器都可以分解成两个子层。

52
00:03:05,633 --> 00:03:08,532
第一层称为自注意力。

53
00:03:09,300 --> 00:03:21,099
编码器的输入首先流经一个自注意力层，这有助于编码或查看单词的相关部分，因为它编码输入句子中的中心词。

56
00:03:22,265 --> 00:03:25,398
第二层被称为前馈层。

57
00:03:25,932 --> 00:03:30,600
自注意力层的输出被送入前馈神经网络。

59
00:03:31,699 --> 00:03:37,233
相同的前馈神经网络独立应用于每个位置。

61
00:03:38,800 --> 00:03:53,399
解码器既有自注意力层，也有前馈层，但在它们之间是编码器解码器注意力层，帮助解码器关注输入句子的相关部分。

66
00:03:53,400 --> 00:04:00,266
在对输入序列中的单词进行嵌入（Embedding）后，每个嵌入向量都会流经编码器的两层。

68
00:04:01,066 --> 00:04:05,500
每个位置的单词都经过一个自注意力过程。

69
00:04:06,066 --> 00:04:14,131
然后它通过一个前馈神经网络，每个向量分别流过相同的网络。

71
00:04:15,165 --> 00:04:21,565
在这个自注意力层中，这些路径之间存在依赖关系。

73
00:04:21,565 --> 00:04:33,032
然而，前馈层没有这些依赖关系，因此各种路径可以在它们流经前馈层时并行执行。

76
00:04:34,165 --> 00:04:41,533
在自注意力层，输入嵌入被分解成查询向量、键向量和值向量。

78
00:04:42,165 --> 00:04:48,765
这些向量是通过Transformer在训练过程中学到的权重计算的。

80
00:04:49,800 --> 00:04:55,300
所有这些计算都是在模型中以矩阵计算的形式并行进行的。

82
00:04:56,466 --> 00:05:07,565
一旦我们有了查询键和值向量，下一步就是将每个值向量乘以"Softmax"分数，以便将它们加起来。

85
00:05:08,266 --> 00:05:21,632
这里的目的是保持你想要关注的单词的值不变，并通过将它们乘以很小的数字（例如0.001）来排除不相关的单词。

89
00:05:22,800 --> 00:05:31,033
接下来，我们需要对加权值向量求和，从而得到这个位置的自注意力层的输出。

91
00:05:31,065 --> 00:05:37,333
对于第一个单词，你可以将结果向量发送到前馈神经网络。

93
00:05:38,665 --> 00:05:43,833
总结一下这个过程，我们采取以下步骤来获得最终的嵌入。

95
00:05:44,833 --> 00:05:50,800
我们从自然语言句子开始，嵌入句子中的每个单词。

97
00:05:51,899 --> 00:06:02,265
然后，我们在这种情况下执行8次多头注意力，并将这个嵌入的单词与相应的加权矩阵相乘。

100
00:06:02,932 --> 00:06:07,832
我们首先使用结果Q K.V矩阵计算注意力。

101
00:06:08,033 --> 00:06:18,732
然后，我们可以将矩阵连接起来生成输出矩阵，这与该层最初获得的最终矩阵具有相同的维度。

104
00:06:18,733 --> 00:06:21,132
现在有多种Transformer的变体。

105
00:06:22,065 --> 00:06:25,265
有些使用原始架构中的编码器和解码器部分。

106
00:06:26,100 --> 00:06:35,400
有些只使用编码器，有些只使用解码器。

108
00:06:36,365 --> 00:06:39,699
一个流行的只有编码器架构是Bert。

109
00:06:40,500 --> 00:06:43,665
Bert是一种训练过的Transformer模型。

110
00:06:44,266 --> 00:06:52,633
Bert代表双向编码器表示，来自Transformer，由谷歌于2018年开发。

112
00:06:54,365 --> 00:06:57,899
从那时起，已经建立了多种Bert的变体。

114
00:06:58,365 --> 00:07:00,965
如今，Bert为谷歌搜索提供动力。

115
00:07:01,865 --> 00:07:06,665
你可以看到Bert为同一搜索查询提供的结果有多么不同。

117
00:07:06,899 --> 00:07:12,100
在此之前和之后，Bert分为两种变体进行训练。

119
00:07:12,565 --> 00:07:21,065
一个模型包含Bert Base，它有12个Transformer的堆栈，大约有1.1亿个参数，

122
00:07:21,065 --> 00:07:29,465
另一个是Bert Large，有24层Transformer，大约有3.4亿个参数。

124
00:07:30,466 --> 00:07:35,499
Bert模型之所以强大，是因为它可以处理长输入上下文。

125
00:07:36,165 --> 00:07:40,299
它在整个维基百科语料库和书籍语料库上进行了训练。

126
00:07:41,300 --> 00:07:44,099
Bert模型经过了100万步的训练。

127
00:07:44,800 --> 00:07:49,864
Bert在不同任务上进行了训练，这意味着它具有多任务目标。

129
00:07:50,466 --> 00:07:55,633
这使得Bert非常强大，因为它接受了各种任务的训练。

131
00:07:55,665 --> 00:07:59,632
它既可以在句子级别上工作，也可以在Token级别上工作。

132
00:08:00,699 --> 00:08:04,833
这是最初发布的两个不同版本的Bert。

134
00:08:05,300 --> 00:08:11,431
一个是Bert Base，它有12层，而Bert Large有24层。

136
00:08:11,699 --> 00:08:15,299
与原始的Transformer相比，它只有六层。

137
00:08:16,333 --> 00:08:20,833
Bert的工作方式是在两个不同的任务上进行训练。

138
00:08:21,132 --> 00:08:30,633
任务一称为掩码语言模型，其中句子被掩盖，模型被训练来预测被掩盖的单词。

140
00:08:31,199 --> 00:08:38,233
如果要从头开始训练Bert，你将不得不掩盖语料库中的一定百分比的单词。

142
00:08:38,799 --> 00:08:42,231
建议的掩盖百分比是15%。

143
00:08:42,899 --> 00:08:47,731
掩盖百分比实现了过少和过多掩盖之间的平衡。

145
00:08:48,365 --> 00:08:57,265
太少的遮罩使训练过程非常昂贵，太多的遮罩则去除了模型所需的上下文。

147
00:08:58,133 --> 00:09:01,733
第二个任务是预测下一句话。

148
00:09:02,399 --> 00:09:06,132
例如，模型给出了两组句子。

149
00:09:06,600 --> 00:09:12,833
Bert的目标是学习句子之间的关系，并根据第一句预测下一句。

151
00:09:13,932 --> 00:09:21,864
例如，句子A可能是一个人去了商店，句子B是他买了一加仑牛奶。

154
00:09:22,966 --> 00:09:29,733
Bert负责分类句子B是否在句子A之后的下一句。

156
00:09:30,166 --> 00:09:32,633
这是一个二分类任务。

157
00:09:33,600 --> 00:09:38,699
这帮助Bert在句子级别上表现良好，为了训练Bert，

158
00:09:38,899 --> 00:09:45,231
你需要为输入句子向模型提供三种不同类型的嵌入，

160
00:09:45,265 --> 00:09:50,533
分别是Token嵌入、段落嵌入和位置嵌入。

162
00:09:51,832 --> 00:09:58,832
Token嵌入是输入句子中每个Token的表示。

164
00:09:59,399 --> 00:10:04,500
单词被转换为特定维度的向量表示。

166
00:10:05,600 --> 00:10:10,199
Bert可以解决涉及文本分类的自然语言处理任务。

168
00:10:10,799 --> 00:10:18,600
一个例子是，对于两句话“我的狗很可爱”和“他喜欢玩耍”，我们需要分类这两句话在语义上是否相似。

170
00:10:18,932 --> 00:10:23,465
将输入文本对简单地连接起来并输入模型。

171
00:10:23,899 --> 00:10:27,532
那么Bert如何区分给定对中的输入呢？

172
00:10:28,033 --> 00:10:30,832
答案是使用段落嵌入。

173
00:10:31,566 --> 00:10:38,833
有一个由SEP表示的特殊标记，用于分隔句子的两个不同部分。

175
00:10:39,799 --> 00:10:43,666
另一个问题是学习句子中单词的顺序。

177
00:10:44,799 --> 00:10:48,331
众所周知，Bert由一堆Transformer组成。

178
00:10:48,899 --> 00:10:53,998
Bert设计用于处理最长为512的输入序列。

179
00:10:54,832 --> 00:10:59,965
输入序列的顺序被整合到位置嵌入中。

180
00:11:00,399 --> 00:11:05,565
这使得Bert能够为每个位置学习一个向量表示。

181
00:11:06,500 --> 00:11:09,932
Bert可用于不同的下游任务。

182
00:11:10,533 --> 00:11:28,231
尽管Bert是在大规模语言建模和单句分类上进行训练的，但它可以用于流行的NLP任务，如单句分类、句子对分类、问答和单句标记任务。

187
00:11:28,865 --> 00:11:31,898
感谢你的聆听。
