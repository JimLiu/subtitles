大家好，我叫Benoit Dherin。
我是谷歌高级解决方案实验室的机器学习工程师。
如果你想了解更多关于高级解决方案实验室的信息，请点击下面的描述框中的链接。

目前，围绕生成性AI以及包括新的Vertex AI特性（如GenAI Studio、Model Garden、Gen AI API）在内的新进展，大家都非常兴奋。

我们在这些短期课程中的目标是让你对生成式AI的一些基本概念有一个扎实的了解。

今天，我将讲解编码器-解码器架构，这是大语言模型的核心。

首先，我们将简要介绍架构。
然后我会讲解如何训练这些模型。
最后，我们将了解如何在服务时间从训练好的模型生成文本。
首先，编码器-解码器架构是一种序列到序列的架构。
这意味着它接受一系列单词作为输入，例如英语句子“The cat ate the mouse”

然后输出，例如法语翻译“Le chat a mange la souris”。
编码器-解码器架构是一种消耗序列并输出序列的机器。
另一个输入示例是形成发送给大语言模型的Prompt的单词序列。

然后输出就是大语言模型对这个Prompt的回应。
现在我们知道编码器-解码器架构的作用了。
但是它是如何实现的呢？
通常，编码器-解码器架构分为两个阶段。
首先是编码器阶段，生成输入句子的向量表示。
然后是解码器阶段，生成序列输出。
编码器和解码器都可以用不同的内部架构实现。
内部机制可以是这个幻灯片中展示的循环神经网络，或者
更复杂的Transformer模块，就像我们现在看到的超强大的语言模型一样。

循环神经网络编码器一次处理输入序列中的一个Token，
并生成一个状态，这个状态代表了这个Token以及之前处理过的Token。
然后将此状态与下一个Token一起用作下一编码步骤的输入，
生成下一个状态。
一旦你完成了将所有的输入Token输入到循环神经网络（RNN）中，输出一个表示整个输入句子的向量。

这就是编码器的全部内容。
那解码器部分呢？
解码器接收输入句子的向量表示，并从该表示中生成输出句子。

在RNN解码器中，它是分步进行的，
利用当前状态和已解码的内容逐个解码输出Token。
好了，现在我们对编码器-解码器架构有了高层次的理解，那我们如何训练它呢？

这就是训练阶段。
要训练一个模型，你需要一个数据集，也就是你希望你的模型模仿的输入/输出对的集合。

然后，你可以将这个数据集提供给模型，模型在训练过程中
会根据它在数据集中给定输入产生的错误来修正它的权重。
这个误差本质上是神经网络在给定输入序列时产生的输出与数据集中真实输出序列之间的差异。

好的。
那么如何生成这个数据集呢？
在编码器-解码器架构中，这比典型的预测模型更复杂。

首先，你需要一组输入和输出文本。
对应到翻译的例子，那就是一个句子对，一个句子是源语言，另一个是目标语言。

你将源语言句子输入编码器，然后计算解码器生成的内容和实际翻译之间的错误。

然而，这里有个问题。
解码器在训练时也需要自己的输入！
你需要给解码器正确的前一个翻译Token作为输入来生成下一个Token，而不是到目前为止解码器生成的内容。

这种训练方法被称为教师强制，因为你强迫解码器从正确的前一个Token生成下一个Token。

这意味着在你的代码中，你需要准备两个输入句子，一个是原始的输入给编码器的句子，还有一个是你将提供给解码器的向左移动的原始句子。


另一个微妙之处是，解码器在每一步只生成每个词汇表中的Token是下一个Token的概率。

根据这些概率，你需要选择一个词。
有几种方法可以做到这一点。
最简单的一种，称为贪婪搜索，是生成概率最高的Token。

一种效果更好的方法叫做集束搜索（Beam Search）。
在这种情况下，你用解码器生成的概率来评估句子块的概率，而不是单个词。

并且你在每个步骤中保留最可能生成的块。
这就是训练的方式。
现在让我们转向服务。
在训练后，当你想生成新的翻译或对某个Prompt的新回应时，你会首先将Prompt的编码器表示连同像“GO”这样的特殊Token一起输入到解码器中。


这会促使解码器生成第一个单词。
让我们更详细地了解生成阶段发生了什么。
首先，开始Token需要通过嵌入层表示为一个向量。
然后，循环层将会更新编码器生成的先前状态，使其成为新的状态。
这个状态将被传递到一个密集的 softmax 层来产生单词概率。最后
通过贪婪搜索或者束搜索取概率最高的Token或者最高概率的块来生成词语。

在这个阶段，你需要为生成的第二个单词重复这个过程。
然后是第三个单词，直到完成！
接下来是什么呢？
嗯。
我们刚刚学习的架构和大型语言模型中的架构之间的差异在于编码器和解码器块中的内容。

简单的RRN网络被替换为Transformer模块，这是一种在Google发现的基于注意力机制的架构。

如果你对这些话题感兴趣，我们还有另外两门概述课程：
“注意力机制：概述”，以及“Transformer模型和BERT模型：概述”。
另外，如果你喜欢今天的课程，请看“编码器-解码器架构：实验演示”，
在那里，我将向你展示如何使用我们在这节课中学到的概念来在代码中生成诗歌。

感谢你的时间！
祝你有美好的一天！