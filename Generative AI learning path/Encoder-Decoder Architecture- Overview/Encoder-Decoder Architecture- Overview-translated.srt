1
00:00:00,269 --> 00:00:02,269
大家好，我叫Benoit Dherin。

2
00:00:02,270 --> 00:00:06,710
我是谷歌高级解决方案实验室的机器学习工程师。

3
00:00:06,710 --> 00:00:11,848
如果你想了解更多关于高级解决方案实验室的信息，请点击下面的描述框中的链接。

5
00:00:11,849 --> 00:00:24,379
目前，围绕生成性AI以及包括新的Vertex AI特性（如GenAI Studio、Model Garden、Gen AI API）在内的新进展，大家都非常兴奋。

7
00:00:24,379 --> 00:00:32,379
我们在这些短期课程中的目标是让你对生成式AI的一些基本概念有一个扎实的了解。

9
00:00:32,380 --> 00:00:39,450
今天，我将讲解编码器-解码器架构，这是大语言模型的核心。

11
00:00:39,450 --> 00:00:41,929
首先，我们将简要介绍架构。

12
00:00:41,929 --> 00:00:45,458
然后我会讲解如何训练这些模型。

13
00:00:45,460 --> 00:00:51,919
最后，我们将了解如何在服务时间从训练好的模型生成文本。

14
00:00:51,920 --> 00:00:56,448
首先，编码器-解码器架构是一种序列到序列的架构。

15
00:00:56,448 --> 00:01:04,608
这意味着它接受一系列单词作为输入，例如英语句子“The cat ate the mouse”

17
00:01:04,609 --> 00:01:11,818
然后输出，例如法语翻译“Le chat a mange la souris”。

18
00:01:11,819 --> 00:01:18,078
编码器-解码器架构是一种消耗序列并输出序列的机器。

19
00:01:18,079 --> 00:01:24,090
另一个输入示例是形成发送给大语言模型的Prompt的单词序列。

21
00:01:24,090 --> 00:01:29,349
然后输出就是大语言模型对这个Prompt的回应。

22
00:01:29,349 --> 00:01:32,328
现在我们知道编码器-解码器架构的作用了。

23
00:01:32,328 --> 00:01:34,969
但是它是如何实现的呢？

24
00:01:34,969 --> 00:01:39,598
通常，编码器-解码器架构分为两个阶段。

25
00:01:39,599 --> 00:01:45,969
首先是编码器阶段，生成输入句子的向量表示。

26
00:01:45,969 --> 00:01:51,318
然后是解码器阶段，生成序列输出。

27
00:01:51,319 --> 00:01:57,219
编码器和解码器都可以用不同的内部架构实现。

28
00:01:57,219 --> 00:02:01,839
内部机制可以是这个幻灯片中展示的循环神经网络，或者

29
00:02:01,840 --> 00:02:07,918
更复杂的Transformer模块，就像我们现在看到的超强大的语言模型一样。

31
00:02:07,920 --> 00:02:14,150
循环神经网络编码器一次处理输入序列中的一个Token，

32
00:02:14,150 --> 00:02:21,848
并生成一个状态，这个状态代表了这个Token以及之前处理过的Token。

33
00:02:21,848 --> 00:02:28,788
然后将此状态与下一个Token一起用作下一编码步骤的输入，

34
00:02:28,789 --> 00:02:31,219
生成下一个状态。

35
00:02:31,219 --> 00:02:39,749
一旦你完成了将所有的输入Token输入到循环神经网络（RNN）中，输出一个表示整个输入句子的向量。

37
00:02:39,750 --> 00:02:42,049
这就是编码器的全部内容。

38
00:02:42,050 --> 00:02:44,100
那解码器部分呢？

39
00:02:45,000 --> 00:02:54,548
解码器接收输入句子的向量表示，并从该表示中生成输出句子。

41
00:02:54,550 --> 00:03:00,550
在RNN解码器中，它是分步进行的，

42
00:03:00,550 --> 00:03:05,509
利用当前状态和已解码的内容逐个解码输出Token。

43
00:03:05,509 --> 00:03:13,078
好了，现在我们对编码器-解码器架构有了高层次的理解，那我们如何训练它呢？

45
00:03:13,080 --> 00:03:15,930
这就是训练阶段。

46
00:03:15,930 --> 00:03:24,119
要训练一个模型，你需要一个数据集，也就是你希望你的模型模仿的输入/输出对的集合。

48
00:03:24,120 --> 00:03:30,360
然后，你可以将这个数据集提供给模型，模型在训练过程中

49
00:03:30,360 --> 00:03:36,719
会根据它在数据集中给定输入产生的错误来修正它的权重。

50
00:03:36,719 --> 00:03:48,150
这个误差本质上是神经网络在给定输入序列时产生的输出与数据集中真实输出序列之间的差异。

52
00:03:48,150 --> 00:03:49,150
好的。

53
00:03:49,150 --> 00:03:52,188
那么如何生成这个数据集呢？

54
00:03:52,188 --> 00:03:59,608
在编码器-解码器架构中，这比典型的预测模型更复杂。

56
00:03:59,610 --> 00:04:03,400
首先，你需要一组输入和输出文本。

57
00:04:03,400 --> 00:04:12,539
对应到翻译的例子，那就是一个句子对，一个句子是源语言，另一个是目标语言。

59
00:04:12,539 --> 00:04:21,740
你将源语言句子输入编码器，然后计算解码器生成的内容和实际翻译之间的错误。

61
00:04:21,740 --> 00:04:24,199
然而，这里有个问题。

62
00:04:24,199 --> 00:04:27,769
解码器在训练时也需要自己的输入！

63
00:04:27,769 --> 00:04:38,970
你需要给解码器正确的前一个翻译Token作为输入来生成下一个Token，而不是到目前为止解码器生成的内容。

65
00:04:38,970 --> 00:04:50,468
这种训练方法被称为教师强制，因为你强迫解码器从正确的前一个Token生成下一个Token。

67
00:04:50,470 --> 00:05:05,289
这意味着在你的代码中，你需要准备两个输入句子，一个是原始的输入给编码器的句子，还有一个是你将提供给解码器的向左移动的原始句子。

70
00:05:05,290 --> 00:05:14,599
另一个微妙之处是，解码器在每一步只生成每个词汇表中的Token是下一个Token的概率。

72
00:05:14,600 --> 00:05:17,139
根据这些概率，你需要选择一个词。

73
00:05:17,139 --> 00:05:19,879
有几种方法可以做到这一点。

74
00:05:19,879 --> 00:05:25,850
最简单的一种，称为贪婪搜索，是生成概率最高的Token。

76
00:05:25,850 --> 00:05:31,379
一种效果更好的方法叫做集束搜索（Beam Search）。

77
00:05:31,379 --> 00:05:41,929
在这种情况下，你用解码器生成的概率来评估句子块的概率，而不是单个词。

79
00:05:41,930 --> 00:05:46,259
并且你在每个步骤中保留最可能生成的块。

80
00:05:46,259 --> 00:05:48,439
这就是训练的方式。

81
00:05:48,439 --> 00:05:51,209
现在让我们转向服务。

82
00:05:51,209 --> 00:06:09,769
在训练后，当你想生成新的翻译或对某个Prompt的新回应时，你会首先将Prompt的编码器表示连同像“GO”这样的特殊Token一起输入到解码器中。

85
00:06:09,769 --> 00:06:14,347
这会促使解码器生成第一个单词。

86
00:06:14,348 --> 00:06:19,088
让我们更详细地了解生成阶段发生了什么。

87
00:06:19,089 --> 00:06:25,257
首先，开始Token需要通过嵌入层表示为一个向量。

88
00:06:25,259 --> 00:06:32,850
然后，循环层将会更新编码器生成的先前状态，使其成为新的状态。

89
00:06:32,850 --> 00:06:40,029
这个状态将被传递到一个密集的 softmax 层来产生单词概率。最后

90
00:06:40,029 --> 00:06:48,129
通过贪婪搜索或者束搜索取概率最高的Token或者最高概率的块来生成词语。

92
00:06:48,129 --> 00:06:51,478
在这个阶段，你需要为生成的第二个单词重复这个过程。

93
00:06:51,478 --> 00:06:56,529
然后是第三个单词，直到完成！

94
00:06:56,529 --> 00:06:58,839
接下来是什么呢？

95
00:06:58,839 --> 00:06:59,839
嗯。

96
00:06:59,839 --> 00:07:09,370
我们刚刚学习的架构和大型语言模型中的架构之间的差异在于编码器和解码器块中的内容。

98
00:07:09,370 --> 00:07:21,160
简单的RRN网络被替换为Transformer模块，这是一种在Google发现的基于注意力机制的架构。

100
00:07:21,160 --> 00:07:25,499
如果你对这些话题感兴趣，我们还有另外两门概述课程：

101
00:07:25,500 --> 00:07:32,899
“注意力机制：概述”，以及“Transformer模型和BERT模型：概述”。

102
00:07:32,899 --> 00:07:38,328
另外，如果你喜欢今天的课程，请看“编码器-解码器架构：实验演示”，

103
00:07:38,329 --> 00:07:45,519
在那里，我将向你展示如何使用我们在这节课中学到的概念来在代码中生成诗歌。

105
00:07:45,519 --> 00:07:46,817
感谢你的时间！

106
00:07:46,819 --> 00:07:47,490
祝你有美好的一天！
