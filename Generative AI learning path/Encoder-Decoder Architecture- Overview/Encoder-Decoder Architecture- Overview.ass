[Script Info]

Title: Encoder-Decoder Architecture- Overview
ScriptType: v4.00+
WrapStyle: 0
Collisions: Reverse
PlayResX: 384
PlayResY: 288
Timer: 100.0000
ScaledBorderAndShadow: no
Last Style Storage: Default
Video Aspect Ratio: 0
Video Zoom: 6
Video Position: 0

[V4+ Styles]
Format: Name,Fontname,Fontsize,PrimaryColour,SecondaryColour,OutlineColour,BackColour,Bold,Italic,Underline,StrikeOut,ScaleX,ScaleY,Spacing,Angle,BorderStyle,Outline,Shadow,Alignment,MarginL,MarginR,MarginV,Encoding
Style: Default,LXGW WenKai,20,&H0080FFFF,&H000000FF,&H00000000,&H00000000,-1,0,0,0,100,100,0,0,1,2,2,2,1,1,6,1
Style: Secondary,Helvetica,12,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,-1,0,0,0,100,100,0,0,1,2,2,2,1,1,6,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 0,0:00:05.0,0:00:10.0,Secondary,,0,0,0,,{\an7\fs12\pos(9,11)\fad(300,1000)}{\1c&H00FFFFFF&\2c&H0000FF&}翻译：{\1c&H80FFFF&\2c&H0000FF&}宝玉 + GPT-4
Dialogue: 0,0:00:00.27,0:00:02.27,Secondary,,0,0,0,,Hello everybody, my name Benoit Dherin.
Dialogue: 0,0:00:02.27,0:00:06.71,Secondary,,0,0,0,,I am machine learning engineer at the Google Advanced Solutions Lab.
Dialogue: 0,0:00:06.71,0:00:10.62,Secondary,,0,0,0,,If you want to know more about the Advanced Solutions Lab, please follow the link in the
Dialogue: 0,0:00:11.85,0:00:17.62,Secondary,,0,0,0,,There is lots of excitement currently around Generative AI and new advancements including
Dialogue: 0,0:00:24.38,0:00:29.13,Secondary,,0,0,0,,Our objective in these short courses is to give you a solid footing on some of the underlying
Dialogue: 0,0:00:32.38,0:00:37.95,Secondary,,0,0,0,,Today, I am going to talk about the encoder-decoder architecture, which is at the core of large
Dialogue: 0,0:00:39.45,0:00:41.93,Secondary,,0,0,0,,We will start with a brief overview of the architecture.
Dialogue: 0,0:00:41.93,0:00:45.46,Secondary,,0,0,0,,Then I’ll go over how we train these models.
Dialogue: 0,0:00:45.46,0:00:51.92,Secondary,,0,0,0,,And at last, we will see how to produce text from a trained model at serving time.
Dialogue: 0,0:00:51.92,0:00:56.45,Secondary,,0,0,0,,To begin with, the encoder-decoder architecture is a sequence-to-sequence architecture.
Dialogue: 0,0:00:56.45,0:01:03.52,Secondary,,0,0,0,,This means it takes, say, a sequence of words as input, like the sentence in english “The
Dialogue: 0,0:01:04.61,0:01:11.82,Secondary,,0,0,0,,and it outputs, say, the translation in French “Le chat a mange la souris” The encoder-decoder
Dialogue: 0,0:01:11.82,0:01:18.8,Secondary,,0,0,0,,architecture is machine that consumes sequences and spits out sequences.
Dialogue: 0,0:01:18.8,0:01:23.9,Secondary,,0,0,0,,Another input example is the sequence of words forming the prompt sent to a large language
Dialogue: 0,0:01:24.9,0:01:29.35,Secondary,,0,0,0,,Then the output is the response of the large language model to this prompt.
Dialogue: 0,0:01:29.35,0:01:32.33,Secondary,,0,0,0,,Now we know what a encoder-decoder architecture does.
Dialogue: 0,0:01:32.33,0:01:34.97,Secondary,,0,0,0,,But how does it do it?
Dialogue: 0,0:01:34.97,0:01:39.60,Secondary,,0,0,0,,Typically, the encoder-decoder architecture has two stages.
Dialogue: 0,0:01:39.60,0:01:45.97,Secondary,,0,0,0,,First, an encoder stage that produces a vector representation of the input sentence.
Dialogue: 0,0:01:45.97,0:01:51.32,Secondary,,0,0,0,,Then this encoder stage is followed by a decoder stage that creates the sequence output .
Dialogue: 0,0:01:51.32,0:01:57.22,Secondary,,0,0,0,,Both the encoder and the decoder can be implemented with different internal architectures.
Dialogue: 0,0:01:57.22,0:02:01.84,Secondary,,0,0,0,,The internal mechanism can be a recurrent neural network as shown in this slide or a
Dialogue: 0,0:02:01.84,0:02:06.24,Secondary,,0,0,0,,more complex transformer block as in the case of the super powerful language
Dialogue: 0,0:02:07.92,0:02:14.15,Secondary,,0,0,0,,A recurrent neural network encoder takes each token in the input sequence one at a time,
Dialogue: 0,0:02:14.15,0:02:21.85,Secondary,,0,0,0,,and produces a state representing this token as well as the previously ingested tokens
Dialogue: 0,0:02:21.85,0:02:28.79,Secondary,,0,0,0,,Then this state in used in the next encoding step as input along with the next token to
Dialogue: 0,0:02:28.79,0:02:31.22,Secondary,,0,0,0,,produce the next state.
Dialogue: 0,0:02:31.22,0:02:38.13,Secondary,,0,0,0,,Once you are done ingesting all the the input tokens into the Recurrent Neural Networks (RNN), you output a vector that
Dialogue: 0,0:02:39.75,0:02:42.5,Secondary,,0,0,0,,That’s it for the encoder.
Dialogue: 0,0:02:42.5,0:02:44.10,Secondary,,0,0,0,,What about the decoder part?
Dialogue: 0,0:02:45.0,0:02:51.38,Secondary,,0,0,0,,The decoder takes the vector representation of the input sentence and produces an output
Dialogue: 0,0:02:54.55,0:03:00.55,Secondary,,0,0,0,,In the case of a RNN decoder it does it in steps, decoding the output one token at a
Dialogue: 0,0:03:00.55,0:03:05.51,Secondary,,0,0,0,,time using the current state and what has been decoded so far.
Dialogue: 0,0:03:05.51,0:03:11.70,Secondary,,0,0,0,,Okay, now that we have a high level understanding of the encoder-decoder architecture, how do
Dialogue: 0,0:03:13.8,0:03:15.93,Secondary,,0,0,0,,That’s the training phase.
Dialogue: 0,0:03:15.93,0:03:21.7,Secondary,,0,0,0,,To train a model, you need a dataset, that is a collection of input/ouput pairs that
Dialogue: 0,0:03:24.12,0:03:30.36,Secondary,,0,0,0,,You can then feed this dataset to the model, which will correct its weights during training
Dialogue: 0,0:03:30.36,0:03:36.72,Secondary,,0,0,0,,on the basis of the error it produces on a given input in the dataset.
Dialogue: 0,0:03:36.72,0:03:42.78,Secondary,,0,0,0,,This error is essentially the difference between what the neural network generates given an
Dialogue: 0,0:03:48.15,0:03:49.15,Secondary,,0,0,0,,Okay.
Dialogue: 0,0:03:49.15,0:03:52.19,Secondary,,0,0,0,,But then how do you produce this dataset?
Dialogue: 0,0:03:52.19,0:03:57.70,Secondary,,0,0,0,,In the case of the encoder-decoder architecture this is more complicated than for typical
Dialogue: 0,0:03:59.61,0:04:03.40,Secondary,,0,0,0,,First of all you need a collection of input and output texts.
Dialogue: 0,0:04:03.40,0:04:08.96,Secondary,,0,0,0,,In the case of translation that would be sentence pairs where one sentence is in the source
Dialogue: 0,0:04:12.54,0:04:17.45,Secondary,,0,0,0,,You’ll feed the source language sentence to the encoder and then compute the error
Dialogue: 0,0:04:21.74,0:04:24.20,Secondary,,0,0,0,,However, there is a catch.
Dialogue: 0,0:04:24.20,0:04:27.77,Secondary,,0,0,0,,The decoder also needs it own input at training time!
Dialogue: 0,0:04:27.77,0:04:33.85,Secondary,,0,0,0,,You’ll need to give the decoder the correct previous translated token as input to generate
Dialogue: 0,0:04:38.97,0:04:46.9,Secondary,,0,0,0,,This method of training is called teacher forcing, because you force the decoder to
Dialogue: 0,0:04:50.47,0:04:56.91,Secondary,,0,0,0,,This means that in your code you’ll have to prepare two input sentences, the original
Dialogue: 0,0:05:05.29,0:05:10.69,Secondary,,0,0,0,,Another subtle point is that the decoder generates at each step only the probability that each
Dialogue: 0,0:05:14.60,0:05:17.14,Secondary,,0,0,0,,Using these probabilities, you’ll have to select a word.
Dialogue: 0,0:05:17.14,0:05:19.88,Secondary,,0,0,0,,And there are several approaches for that.
Dialogue: 0,0:05:19.88,0:05:24.85,Secondary,,0,0,0,,The simplest one, called greedy search, is to generate the token that has the highest
Dialogue: 0,0:05:25.85,0:05:31.38,Secondary,,0,0,0,,A better approach that produces better results is called beam search.
Dialogue: 0,0:05:31.38,0:05:37.41,Secondary,,0,0,0,,In that case you use the probabilities generated by the decoder to evaluate the probability
Dialogue: 0,0:05:41.93,0:05:46.26,Secondary,,0,0,0,,And you keep at each step the most likely generated chunk.
Dialogue: 0,0:05:46.26,0:05:48.44,Secondary,,0,0,0,,That’s how training is done.
Dialogue: 0,0:05:48.44,0:05:51.21,Secondary,,0,0,0,,Now let’s move onto serving.
Dialogue: 0,0:05:51.21,0:05:58.77,Secondary,,0,0,0,,After training, at serving time, when you want to say generate a new translation or
Dialogue: 0,0:06:09.77,0:06:14.35,Secondary,,0,0,0,,This will prompt the decoder to generate the first word.
Dialogue: 0,0:06:14.35,0:06:19.9,Secondary,,0,0,0,,Let’s see in more detail what happens during the generation stage.
Dialogue: 0,0:06:19.9,0:06:25.26,Secondary,,0,0,0,,First of all the start token needs to be represented by a vector using an embedding layer.
Dialogue: 0,0:06:25.26,0:06:32.85,Secondary,,0,0,0,,Then the recurrent layer will update the previous state produced by the encoder into a new state.
Dialogue: 0,0:06:32.85,0:06:40.3,Secondary,,0,0,0,,This state will be passed to a dense softmax layer to produce the word probabilities Finally
Dialogue: 0,0:06:40.3,0:06:45.15,Secondary,,0,0,0,,the word is generated by taking the highest probability token with greedy search or the
Dialogue: 0,0:06:48.13,0:06:51.48,Secondary,,0,0,0,,At this point you repeat this procedure for the second word to be generated.
Dialogue: 0,0:06:51.48,0:06:56.53,Secondary,,0,0,0,,And for the third word Until you're done!
Dialogue: 0,0:06:56.53,0:06:58.84,Secondary,,0,0,0,,So what’s next?
Dialogue: 0,0:06:58.84,0:06:59.84,Secondary,,0,0,0,,Well.
Dialogue: 0,0:06:59.84,0:07:03.68,Secondary,,0,0,0,,The difference between the architecture we just learned about and the ones in the large
Dialogue: 0,0:07:09.37,0:07:15.95,Secondary,,0,0,0,,The simple RRN network is replaced by transformer blocks which is an architecture discovered
Dialogue: 0,0:07:21.16,0:07:25.50,Secondary,,0,0,0,,If you are interested in knowing more about these topics, we have two more overview courses
Dialogue: 0,0:07:25.50,0:07:32.90,Secondary,,0,0,0,,in that series: Attention Mechanism: Overview, and Transformer Models and BERT Model: Overview.
Dialogue: 0,0:07:32.90,0:07:38.33,Secondary,,0,0,0,,Also, if you liked this the course today, have a look at Encoder-Decoder Architecture:
Dialogue: 0,0:07:38.33,0:07:42.82,Secondary,,0,0,0,,Lab Walkthrough Where I’ll show you how to generate poetry in code using the concepts
Dialogue: 0,0:07:45.52,0:07:46.82,Secondary,,0,0,0,,Thanks for your time!
Dialogue: 0,0:07:46.82,0:07:47.49,Secondary,,0,0,0,,Have a great day!
Dialogue: 0,0:00:00.27,0:00:02.27,Default,,0,0,0,,大家好，我叫Benoit Dherin。
Dialogue: 0,0:00:02.27,0:00:06.71,Default,,0,0,0,,我是谷歌高级解决方案实验室的机器学习工程师。
Dialogue: 0,0:00:06.71,0:00:11.85,Default,,0,0,0,,如果你想了解更多关于高级解决方案实验室\N的信息，请点击下面的描述框中的链接。
Dialogue: 0,0:00:11.85,0:00:24.38,Default,,0,0,0,,目前，围绕生成性AI以及包括新的Vertex AI特性（如GenAI Studio\N、Model Garden、Gen AI API）在内的新进展，大家都非常兴奋。
Dialogue: 0,0:00:24.38,0:00:32.38,Default,,0,0,0,,我们在这些短期课程中的目标是让你对生成\N式AI的一些基本概念有一个扎实的了解。
Dialogue: 0,0:00:32.38,0:00:39.45,Default,,0,0,0,,今天，我将讲解编码器-解码器架构，这是大语言模型的核心。
Dialogue: 0,0:00:39.45,0:00:41.93,Default,,0,0,0,,首先，我们将简要介绍架构。
Dialogue: 0,0:00:41.93,0:00:45.46,Default,,0,0,0,,然后我会讲解如何训练这些模型。
Dialogue: 0,0:00:45.46,0:00:51.92,Default,,0,0,0,,最后，我们将了解如何在服务时间从训练好的模型生成文本。
Dialogue: 0,0:00:51.92,0:00:56.45,Default,,0,0,0,,首先，编码器-解码器架构是一种序列到序列的架构。
Dialogue: 0,0:00:56.45,0:01:04.61,Default,,0,0,0,,这意味着它接受一系列单词作为输入，例\N如英语句子“The cat ate the mouse”
Dialogue: 0,0:01:04.61,0:01:11.82,Default,,0,0,0,,然后输出，例如法语翻译“Le chat a mange la souris”。
Dialogue: 0,0:01:11.82,0:01:18.8,Default,,0,0,0,,编码器-解码器架构是一种消耗序列并输出序列的机器。
Dialogue: 0,0:01:18.8,0:01:24.9,Default,,0,0,0,,另一个输入示例是形成发送给大语言模型的Prompt的单词序列。
Dialogue: 0,0:01:24.9,0:01:29.35,Default,,0,0,0,,然后输出就是大语言模型对这个Prompt的回应。
Dialogue: 0,0:01:29.35,0:01:32.33,Default,,0,0,0,,现在我们知道编码器-解码器架构的作用了。
Dialogue: 0,0:01:32.33,0:01:34.97,Default,,0,0,0,,但是它是如何实现的呢？
Dialogue: 0,0:01:34.97,0:01:39.60,Default,,0,0,0,,通常，编码器-解码器架构分为两个阶段。
Dialogue: 0,0:01:39.60,0:01:45.97,Default,,0,0,0,,首先是编码器阶段，生成输入句子的向量表示。
Dialogue: 0,0:01:45.97,0:01:51.32,Default,,0,0,0,,然后是解码器阶段，生成序列输出。
Dialogue: 0,0:01:51.32,0:01:57.22,Default,,0,0,0,,编码器和解码器都可以用不同的内部架构实现。
Dialogue: 0,0:01:57.22,0:02:01.84,Default,,0,0,0,,内部机制可以是这个幻灯片中展示的循环神经网络，或者
Dialogue: 0,0:02:01.84,0:02:07.92,Default,,0,0,0,,更复杂的Transformer模块，就像我们现\N在看到的超强大的语言模型一样。
Dialogue: 0,0:02:07.92,0:02:14.15,Default,,0,0,0,,循环神经网络编码器一次处理输入序列中的一个Token，
Dialogue: 0,0:02:14.15,0:02:21.85,Default,,0,0,0,,并生成一个状态，这个状态代表了这个Token以及之前处理过的Token。
Dialogue: 0,0:02:21.85,0:02:28.79,Default,,0,0,0,,然后将此状态与下一个Token一起用作下一编码步骤的输入，
Dialogue: 0,0:02:28.79,0:02:31.22,Default,,0,0,0,,生成下一个状态。
Dialogue: 0,0:02:31.22,0:02:39.75,Default,,0,0,0,,一旦你完成了将所有的输入Token输入到循环神经网\N络（RNN）中，输出一个表示整个输入句子的向量。
Dialogue: 0,0:02:39.75,0:02:42.5,Default,,0,0,0,,这就是编码器的全部内容。
Dialogue: 0,0:02:42.5,0:02:44.10,Default,,0,0,0,,那解码器部分呢？
Dialogue: 0,0:02:45.0,0:02:54.55,Default,,0,0,0,,解码器接收输入句子的向量表示，并从该表示中生成输出句子。
Dialogue: 0,0:02:54.55,0:03:00.55,Default,,0,0,0,,在RNN解码器中，它是分步进行的，
Dialogue: 0,0:03:00.55,0:03:05.51,Default,,0,0,0,,利用当前状态和已解码的内容逐个解码输出Token。
Dialogue: 0,0:03:05.51,0:03:13.8,Default,,0,0,0,,好了，现在我们对编码器-解码器架构有了\N高层次的理解，那我们如何训练它呢？
Dialogue: 0,0:03:13.8,0:03:15.93,Default,,0,0,0,,这就是训练阶段。
Dialogue: 0,0:03:15.93,0:03:24.12,Default,,0,0,0,,要训练一个模型，你需要一个数据集，也就是你\N希望你的模型模仿的输入/输出对的集合。
Dialogue: 0,0:03:24.12,0:03:30.36,Default,,0,0,0,,然后，你可以将这个数据集提供给模型，模型在训练过程中
Dialogue: 0,0:03:30.36,0:03:36.72,Default,,0,0,0,,会根据它在数据集中给定输入产生的错误来修正它的权重。
Dialogue: 0,0:03:36.72,0:03:48.15,Default,,0,0,0,,这个误差本质上是神经网络在给定输入序列时产生\N的输出与数据集中真实输出序列之间的差异。
Dialogue: 0,0:03:48.15,0:03:49.15,Default,,0,0,0,,好的。
Dialogue: 0,0:03:49.15,0:03:52.19,Default,,0,0,0,,那么如何生成这个数据集呢？
Dialogue: 0,0:03:52.19,0:03:59.61,Default,,0,0,0,,在编码器-解码器架构中，这比典型的预测模型更复杂。
Dialogue: 0,0:03:59.61,0:04:03.40,Default,,0,0,0,,首先，你需要一组输入和输出文本。
Dialogue: 0,0:04:03.40,0:04:12.54,Default,,0,0,0,,对应到翻译的例子，那就是一个句子对，一\N个句子是源语言，另一个是目标语言。
Dialogue: 0,0:04:12.54,0:04:21.74,Default,,0,0,0,,你将源语言句子输入编码器，然后计算解码\N器生成的内容和实际翻译之间的错误。
Dialogue: 0,0:04:21.74,0:04:24.20,Default,,0,0,0,,然而，这里有个问题。
Dialogue: 0,0:04:24.20,0:04:27.77,Default,,0,0,0,,解码器在训练时也需要自己的输入！
Dialogue: 0,0:04:27.77,0:04:38.97,Default,,0,0,0,,你需要给解码器正确的前一个翻译Token作为输入来生成\N下一个Token，而不是到目前为止解码器生成的内容。
Dialogue: 0,0:04:38.97,0:04:50.47,Default,,0,0,0,,这种训练方法被称为教师强制，因为你强迫解码\N器从正确的前一个Token生成下一个Token。
Dialogue: 0,0:04:50.47,0:05:05.29,Default,,0,0,0,,这意味着在你的代码中，你需要准备两个输入句子\N，一个是原始的输入给编码器的句子，还有一个是\N你将提供给解码器的向左移动的原始句子。
Dialogue: 0,0:05:05.29,0:05:14.60,Default,,0,0,0,,另一个微妙之处是，解码器在每一步只生成每\N个词汇表中的Token是下一个Token的概率。
Dialogue: 0,0:05:14.60,0:05:17.14,Default,,0,0,0,,根据这些概率，你需要选择一个词。
Dialogue: 0,0:05:17.14,0:05:19.88,Default,,0,0,0,,有几种方法可以做到这一点。
Dialogue: 0,0:05:19.88,0:05:25.85,Default,,0,0,0,,最简单的一种，称为贪婪搜索，是生成概率最高的Token。
Dialogue: 0,0:05:25.85,0:05:31.38,Default,,0,0,0,,一种效果更好的方法叫做集束搜索（Beam Search）。
Dialogue: 0,0:05:31.38,0:05:41.93,Default,,0,0,0,,在这种情况下，你用解码器生成的概率来\N评估句子块的概率，而不是单个词。
Dialogue: 0,0:05:41.93,0:05:46.26,Default,,0,0,0,,并且你在每个步骤中保留最可能生成的块。
Dialogue: 0,0:05:46.26,0:05:48.44,Default,,0,0,0,,这就是训练的方式。
Dialogue: 0,0:05:48.44,0:05:51.21,Default,,0,0,0,,现在让我们转向服务。
Dialogue: 0,0:05:51.21,0:06:09.77,Default,,0,0,0,,在训练后，当你想生成新的翻译或对某个Prompt的\N新回应时，你会首先将Prompt的编码器表示连同像\N“GO”这样的特殊Token一起输入到解码器中。
Dialogue: 0,0:06:09.77,0:06:14.35,Default,,0,0,0,,这会促使解码器生成第一个单词。
Dialogue: 0,0:06:14.35,0:06:19.9,Default,,0,0,0,,让我们更详细地了解生成阶段发生了什么。
Dialogue: 0,0:06:19.9,0:06:25.26,Default,,0,0,0,,首先，开始Token需要通过嵌入层表示为一个向量。
Dialogue: 0,0:06:25.26,0:06:32.85,Default,,0,0,0,,然后，循环层将会更新编码器生成的先前状态，使其成为新的状态。
Dialogue: 0,0:06:32.85,0:06:40.3,Default,,0,0,0,,这个状态将被传递到一个密集的 softmax 层来产生单词概率。最后
Dialogue: 0,0:06:40.3,0:06:48.13,Default,,0,0,0,,通过贪婪搜索或者束搜索取概率最高的\NToken或者最高概率的块来生成词语。
Dialogue: 0,0:06:48.13,0:06:51.48,Default,,0,0,0,,在这个阶段，你需要为生成的第二个单词重复这个过程。
Dialogue: 0,0:06:51.48,0:06:56.53,Default,,0,0,0,,然后是第三个单词，直到完成！
Dialogue: 0,0:06:56.53,0:06:58.84,Default,,0,0,0,,接下来是什么呢？
Dialogue: 0,0:06:58.84,0:06:59.84,Default,,0,0,0,,嗯。
Dialogue: 0,0:06:59.84,0:07:09.37,Default,,0,0,0,,我们刚刚学习的架构和大型语言模型中的架构之\N间的差异在于编码器和解码器块中的内容。
Dialogue: 0,0:07:09.37,0:07:21.16,Default,,0,0,0,,简单的RRN网络被替换为Transformer模块，这是\N一种在Google发现的基于注意力机制的架构。
Dialogue: 0,0:07:21.16,0:07:25.50,Default,,0,0,0,,如果你对这些话题感兴趣，我们还有另外两门概述课程：
Dialogue: 0,0:07:25.50,0:07:32.90,Default,,0,0,0,,“注意力机制：概述”，以及“Transformer模型和BERT模型：概述”。
Dialogue: 0,0:07:32.90,0:07:38.33,Default,,0,0,0,,另外，如果你喜欢今天的课程，请看“编码器-解码器架构：实验演示”，
Dialogue: 0,0:07:38.33,0:07:45.52,Default,,0,0,0,,在那里，我将向你展示如何使用我们在这节\N课中学到的概念来在代码中生成诗歌。
Dialogue: 0,0:07:45.52,0:07:46.82,Default,,0,0,0,,感谢你的时间！
Dialogue: 0,0:07:46.82,0:07:47.49,Default,,0,0,0,,祝你有美好的一天！