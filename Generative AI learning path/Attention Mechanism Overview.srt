1
00:00:00,033 --> 00:00:01,866
Hi. I'm Sanjana Reddy,

2
00:00:01,866 --> 00:00:05,300
a machine learning engineer at Google's Advanced Solutions Lab.

3
00:00:06,000 --> 00:00:08,766
There's a lot of excitement currently around generative

4
00:00:08,966 --> 00:00:12,933
AI and new advancements, including new vertex AI features

5
00:00:13,266 --> 00:00:17,733
such as Gen AI, Studio Model Garden, Gen AI API.

6
00:00:18,166 --> 00:00:22,333
Our objective in this short session is to give you a solid footing

7
00:00:22,666 --> 00:00:27,433
on some of the underlying concepts that make all the Gen AI magic possible.

8
00:00:28,166 --> 00:00:31,833
Today I'm going to talk about the attention mechanism

9
00:00:32,000 --> 00:00:37,433
that is behind all the transformer models and which is core to the LEM models.

10
00:00:38,400 --> 00:00:41,000
Let's say you want to translate in English sentence

11
00:00:41,033 --> 00:00:44,833
the cat ate the mouse to French.

12
00:00:44,900 --> 00:00:47,033
You could use an encoder decoder.

13
00:00:47,433 --> 00:00:51,366
This is a popular model that is used to translate sentences.

14
00:00:51,966 --> 00:00:55,533
The encoder decoder takes one word at a time

15
00:00:55,533 --> 00:00:57,700
and translates it at each time step.

16
00:00:58,266 --> 00:01:02,066
However, sometimes the words in the source language

17
00:01:02,066 --> 00:01:05,233
do not align with the words in the target language.

18
00:01:05,633 --> 00:01:07,133
Here's an example.

19
00:01:07,133 --> 00:01:10,100
Take the sentence Black cat ate the mouse.

20
00:01:10,666 --> 00:01:13,833
In this example, the first English word is black.

21
00:01:14,433 --> 00:01:16,500
However, in the translation,

22
00:01:16,500 --> 00:01:21,300
the first French word is chat, which means cat in English.

23
00:01:21,866 --> 00:01:24,933
So how can you train a model to focus more on

24
00:01:24,933 --> 00:01:27,633
the word cat instead of the word black?

25
00:01:27,933 --> 00:01:31,700
Add the first time step to improve the translation.

26
00:01:31,733 --> 00:01:36,666
You can add what is called the attention mechanism to the encoder decoder.

27
00:01:37,466 --> 00:01:41,333
Attention mechanism is a technique that allows the neural network

28
00:01:41,466 --> 00:01:45,100
to focus on specific parts of an input sequence.

29
00:01:45,933 --> 00:01:50,500
This is done by assigning weights to different parts of the input sequence

30
00:01:50,833 --> 00:01:54,900
with the most important parts receiving the highest weights.

31
00:01:55,766 --> 00:02:00,166
This is what a traditional RNN based encoder decoder looks like.

32
00:02:00,766 --> 00:02:03,700
The model takes one word at a time as input

33
00:02:04,266 --> 00:02:08,433
updates the hidden state and passes it on to the next time step.

34
00:02:09,933 --> 00:02:11,166
In the end,

35
00:02:11,166 --> 00:02:14,766
only the final hidden state is passed on to the decoder.

36
00:02:15,933 --> 00:02:19,100
The decoder works with the final hidden state

37
00:02:19,400 --> 00:02:23,466
for processing and translates it to the target language.

38
00:02:24,433 --> 00:02:27,700
An attention model differs from the traditional sequence

39
00:02:27,700 --> 00:02:29,700
to sequence model in two ways.

40
00:02:30,333 --> 00:02:34,733
First, the encoder passes a lot more data to the decoder.

41
00:02:35,400 --> 00:02:39,100
So instead of just passing the final hidden state number

42
00:02:39,100 --> 00:02:42,533
three to the decoder, the encoder passes

43
00:02:42,533 --> 00:02:45,000
all the hidden states from each time step.

44
00:02:46,033 --> 00:02:48,533
This gives the decoder more context

45
00:02:48,733 --> 00:02:50,966
beyond just the final hidden state.

46
00:02:51,666 --> 00:02:56,400
The decoder uses all the hidden state information to translate the sentence.

47
00:02:56,800 --> 00:03:01,166
The second change that the attention mechanism brings is adding

48
00:03:01,166 --> 00:03:05,500
an extra step to the attention decoder before producing its output.

49
00:03:06,600 --> 00:03:08,600
Let's take a look at what these steps are

50
00:03:10,133 --> 00:03:10,733
to focus

51
00:03:10,733 --> 00:03:13,433
only on the most relevant parts of the input.

52
00:03:13,800 --> 00:03:17,100
The decoder does the following.

53
00:03:17,100 --> 00:03:21,933
First, it looks at the set of encoder states that it has received.

54
00:03:22,933 --> 00:03:25,566
Each encoder Hidden State is associated

55
00:03:25,566 --> 00:03:28,266
with a certain word in the input sentence.

56
00:03:28,733 --> 00:03:31,533
Second, it gives each hidden state a score.

57
00:03:32,400 --> 00:03:35,566
Third in multiplies each hidden state by its

58
00:03:35,566 --> 00:03:38,700
soft-max score as shown here.

59
00:03:38,700 --> 00:03:42,533
Thus amplifying hidden states with the highest scores

60
00:03:43,133 --> 00:03:46,333
and downsizing hidden states with low scores.

61
00:03:47,133 --> 00:03:49,566
If we connect all of these pieces together,

62
00:03:49,800 --> 00:03:52,500
we're going to see how the Attention Network works.

63
00:03:53,033 --> 00:03:57,266
Before moving on, let's define some of the notations on this slide.

64
00:03:57,933 --> 00:04:01,766
Alpha here represents the attention rate at each time step.

65
00:04:02,366 --> 00:04:05,666
H represents the hidden state of the encoder RNN

66
00:04:05,666 --> 00:04:09,000
at each time step h subscript

67
00:04:09,000 --> 00:04:12,466
B represents the hidden state of the decoder

68
00:04:12,466 --> 00:04:16,833
RNN at each time step. With the attention mechanism

69
00:04:17,000 --> 00:04:21,133
the inversion of the Black Cat translation is clearly visible

70
00:04:21,132 --> 00:04:24,166
in the attention diagram and ate

71
00:04:24,533 --> 00:04:27,000
translates as two words, a mange,

72
00:04:27,266 --> 00:04:30,333
in French. We can see the attention network

73
00:04:30,366 --> 00:04:33,933
staying focused on the word ate for two time steps.

74
00:04:34,933 --> 00:04:35,933
During the attention

75
00:04:35,933 --> 00:04:40,733
step we use the encoder hidden states and the H4 vector

76
00:04:40,733 --> 00:04:45,100
to calculate a context vector a four for this time step.

77
00:04:45,666 --> 00:04:48,133
This is the weighted sum.

78
00:04:48,133 --> 00:04:50,400
We then concatenate H4

79
00:04:50,400 --> 00:04:52,966
and a 4 into one vector.

80
00:04:54,000 --> 00:04:58,300
This concatenated vector is passed through a feedforward neural network.

81
00:04:58,733 --> 00:05:00,833
One train jointly with the model

82
00:05:01,400 --> 00:05:04,166
to predict the next work.

83
00:05:04,166 --> 00:05:06,766
The output of the feedforward neural network

84
00:05:06,900 --> 00:05:10,000
indicates the output word of this time step.

85
00:05:10,500 --> 00:05:14,533
This process continues till the end of sentence token

86
00:05:14,533 --> 00:05:17,166
is generated by the decoder.

87
00:05:17,166 --> 00:05:20,666
This is how you can use an attention mechanism to improve

88
00:05:20,666 --> 00:05:24,466
the performance of a traditional encoder decoder architecture.

89
00:05:25,233 --> 00:05:28,300
Thank you so much for listening.

