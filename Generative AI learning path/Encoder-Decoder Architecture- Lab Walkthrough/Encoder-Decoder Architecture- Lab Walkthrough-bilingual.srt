1
00:00:00,600 --> 00:00:05,432
大家好！我是Google Advanced Solutions Lab的机器学习工程师Benoit Dherin。
Hello, everybody! My name is Benoit Dherin, a machine learning engineer at Google's Advanced Solutions Lab.

4
00:00:05,766 --> 00:00:12,266
如果你想了解更多关于Advanced Solutions Lab的信息，请点击描述框下方的链接。
If you want to know more about what the Advanced Solutions Lab is, please follow the link below in the description box.

6
00:00:13,233 --> 00:00:23,266
围绕着生成性AI和相关的新技术取得了令人兴奋的进展，例如像新的Vertex AI特性如GenAI Studio，Model Garden，GenAI API等。
There is lots of excitement currently around generative AI and new advancements, including new Vertex AI features such as GenAI Studio, Model Garden, GenAI API.

9
00:00:24,199 --> 00:00:31,798
在这个简短的课程中，我们的目标是帮助你了解一些生成式AI的基本概念。
Our objective in this short session is to give you a solid footing on some of the underlying concepts that make all the Gen AI magic possible.

11
00:00:32,200 --> 00:00:39,432
今天，我将讲解与同一系列的“编码器-解码器架构概述”课程相辅助的代码。
Today, I’ll go over the code that’s complementary to the “Encoder-Decoder Architecture Overview” course in the same series.

13
00:00:39,432 --> 00:00:46,066
我们将一起学习如何从头开始使用编码器-解码器架构建立一个诗歌生成器，
We will see together how to build a poetry generator from scratch using the encoder-decoder architecture. Using the angular decoder architecture

16
00:00:46,799 --> 00:00:49,964
你可以在我们的GitHub Repo找到设置指南。
you’d find to set up instructions in our GitHub repository.

17
00:00:50,566 --> 00:00:52,766
好的，现在让我们看一下代码。
Okay, let's now have a look at the code.

18
00:00:53,365 --> 00:00:58,831
要访问我们的Lab，请进入asl-ml-immersion文件夹。
To access our lab, go in the asl-ml-immersion folder.

19
00:00:58,865 --> 00:01:01,065
然后是notebooks文件夹。
Then the notebooks folder.

20
00:01:02,332 --> 00:01:04,432
然后是text_models文件夹。
Then the text_models folder.

21
00:01:05,000 --> 00:01:10,466
在solutions文件夹里你会找到text_generation的Notebook，
And in the solutions from there you'll find the text generation notebook.

22
00:01:10,965 --> 00:01:16,332
这就是我们今天要讲的实验。
That's the lab that we'll cover today.

23
00:01:17,165 --> 00:01:24,100
在这个实验中，我们将基于编码器解码器架构实现一个基于字符的文本生成器。
In this lab we will implement a character based text generator based on the encoder decoder architecture.

25
00:01:24,865 --> 00:01:32,664
基于字符意味着网络消耗和生成的Token是字符而不是单词。
Character based means that the tokens consumed and generated by the network are characters and not words.

27
00:01:33,900 --> 00:01:36,965
我们将使用剧本作为数据集。
We will use plays as a data set.

28
00:01:38,332 --> 00:01:45,132
它们有特殊的结构，就像人们在彼此对话。
They have a special structure which are that of people talking with each other.

30
00:01:45,299 --> 00:01:53,066
你在这里看到的是一个由训练过的神经网络生成的文本示例，
And here you see an example of a piece of text that has been generated by the trained neural network.

32
00:01:53,766 --> 00:01:56,500
即使这些句子没有必然的意义，
When the sentences are not necessarily making sense

33
00:01:56,500 --> 00:01:58,500
也不一定符合语法规则，
nor are grammatically correct.

34
00:01:58,500 --> 00:02:00,965
但仍然有很多值得注意的地方。
This is remarkable in many ways.

35
00:02:00,965 --> 00:02:03,665
首先，记住，它是基于字符的。
First of all, remember, it's character based.

36
00:02:03,665 --> 00:02:07,799
所以它只学习预测最可能的字符。
So it means that it learns to predict only the most probable characters.

37
00:02:08,332 --> 00:02:11,065
尽管如此，它还是学得很好
Despite that, it was able to learn pretty well

38
00:02:11,066 --> 00:02:14,133
像单词被空格分开的概念，
The notion of words separated by blank spaces.

39
00:02:14,432 --> 00:02:19,965
以及剧本的基本结构，角色彼此对话。
And also the basic structure of a play with the characters talking to each other.

40
00:02:21,032 --> 00:02:21,432
所以继续
So going

41
00:02:21,432 --> 00:02:32,733
你将看到的这个非常小的网络是基于RNN 0架构的，并且只在Vertex AIr工作台上训练了30个周期，这是一个非常快的训练时间。
of what is a very small network, as you will see, it's based on the rnn 0 and architecture and only trained for 30 epochs in the vertex air workbench, which is a pretty fast training time.

44
00:02:33,900 --> 00:02:37,033
那么现在我们来看看代码。
So let's look at the code now.

45
00:02:42,099 --> 00:02:47,365
首先要导入我们需要的库。
So the first thing is to import the libraries that we need.

46
00:02:47,665 --> 00:02:56,132
特别是，我们可以使用TensorFlow Keras来导入编码器解码器架构。
In particular, we could or encoder decoder architecture using TensorFlow Keras to impart that.

48
00:02:57,000 --> 00:02:59,299
然后我们下载数据集
Then we download our data set

49
00:02:59,300 --> 00:03:02,099
使用tf.keras.utils.get_file。
using tf.keras.utils.get_file.

50
00:03:02,832 --> 00:03:10,498
现在数据集已经在磁盘上，我们只需要将其加载到变量text中。
So now the dataset is on disk and we just need to load it into a variable called text.

51
00:03:10,500 --> 00:03:18,665
这个text变量现在包含了代表整个莎士比亚剧本数据集的整个字符串。
So the text variable now contains the whole string representing all the all the plays in that Shakespeare dataset.

53
00:03:19,733 --> 00:03:23,066
我可以快速看看它是什么吗？
Can I have a quick look at what it is?

54
00:03:23,066 --> 00:03:27,399
你会看到，如果我们打印出前250个字符，
And you see if we printed the first 250 characters.

55
00:03:27,900 --> 00:03:38,233
有第一位公民对所有人说话，然后所有人对第一位公民说话。
You have the first citizens speaking to everybody and everybody else is speaking to the first citizen.

58
00:03:38,699 --> 00:03:52,064
这个单元计算我们在文本数据集中有多少唯一的字符，我们看到我们有65个唯一的字符，对吧？
The cell computes the number of unique characters that we have in that in the text dataset, and we see that we have 65 unique characters, right?

61
00:03:52,199 --> 00:04:01,199
这些字符将是神经网络在训练和生成过程中将消耗的Token。
These characters would be the tokens that the neural network will consume during training and will generating during this service.

63
00:04:02,066 --> 00:04:06,899
现在的第一步是对文本进行向量化。
So the first step here now is to vectorize the text.

65
00:04:07,233 --> 00:04:08,932
向量化是什么意思呢？
What do we mean by that?

66
00:04:08,932 --> 00:04:17,365
这意味着我们首先需要从实际的字符串序列中提取出字符序列，
It means that first of all, we will need to extract

68
00:04:17,365 --> 00:04:26,198
我们可以用TensorFlow的tf.strings.unicode_split来做到这一点。
from the actual string sequence of characters, which we can do with TensorFlow by using tf.strings.unicode_split.

70
00:04:26,899 --> 00:04:37,531
以现在，例如，文本在这里被转换成了字符序列的列表。
So now, for example, texts here are transformed into a list of sequences of characters.

73
00:04:39,365 --> 00:04:44,898
神经网络不能直接消耗字符，我们需要将它转换成数字。
A neural network cannot consume immediately. The characters. We need to transform that into numbers.

76
00:04:45,300 --> 00:04:49,399
所以我们需要简单地将每个字符映射到一个给定的id。
So we need to simply map each of the characters to a given

77
00:04:50,365 --> 00:05:04,499
为此我们有tf.keras.layers.StringLookup，你只需要将你的词汇表列表传给它。
id. For that we have the tf.keras.layers.StringLookup to which you just need to pass to the list of your vocabulary.

81
00:05:04,500 --> 00:05:17,532
我们在语料库中有的65个唯一字符，然后我们生成一个层，当传入字符时，它会产生相应的id。
The 65 unique character that we have in our corpus and that we produce a layer that when passed the characters will produce corresponding ids.

84
00:05:18,333 --> 00:05:25,932
在那个层中，你有一个已经生成的字符和id之间的映射。
So within that, that layer you have a mapping that has been generated between the characters and the

86
00:05:27,899 --> 00:05:40,300
要获得反向映射，你使用相同的字符串查找层，使用从第一层获取的相同的词汇表。
id. To get the inverse mapping, you use the same layer of string lookup with the exact same vocabulary that you retrieve from the first of the year by using get vocabulary.

90
00:05:41,165 --> 00:05:45,299
但你将参数设置为真，invert等于True，
But you set that parameter to be true, invert, equal equal true,

91
00:05:45,600 --> 00:05:54,699
那将计算反向映射，即从id到字符的映射。
and that will compute the invert mapping, which is the mapping from id to chars Right.

94
00:05:54,699 --> 00:06:07,465
实际上，如果你传递一系列ID给这个映射，它会给你返回对应的字符，
And indeed, if you pass to this mapping sequence of ID’s, the ID’s, it gives you back the corresponding characters.

98
00:06:07,632 --> 00:06:10,599
使用在这个层的内存中存储的映射。
Using the mapping that start in the memory of this layer.

99
00:06:11,733 --> 00:06:14,233
就是这样。
So that's that.

100
00:06:15,165 --> 00:06:15,899
好的。
Okay.

101
00:06:15,899 --> 00:06:20,631
现在我们来准备我们将用来训练神经网络的数据集。
Now let's that's been the dataset that we will train our neural network with.

102
00:06:20,932 --> 00:06:34,799
我们使用了tf.data.Dataset API，它有一个很好的方法叫做“from_tensor_slices”，
For that we are using the tf.data.Dataset API, which has this nice method from tons of slices which will convert.

107
00:06:34,800 --> 00:06:43,965
它将把一个实例数组，代表我们的整个剧本文本语料库的id，转化为一个TF data数据集。
That answer of instance represents or whole corpus of text of plays as id it will store that into it to have data data sets.

109
00:06:44,565 --> 00:06:52,600
此刻，这些数据集的元素仅仅是单个字符。
So at this point, the elements of these datasets are just the individual characters.

111
00:06:52,600 --> 00:06:53,899
这对我们来说并不理想。
So that's not great for us.

112
00:06:53,899 --> 00:07:01,499
我们想要给我们的神经网络提供的是等长的序列，而不仅仅是一个字符。
But we want to feed our neural network with our sequences of the same length but not just one character.

114
00:07:01,565 --> 00:07:03,432
我们需要预测下一个字符。
We need to predict the next character. So

115
00:07:05,432 --> 00:07:06,764
但幸运的是，
but luckily the

116
00:07:06,766 --> 00:07:11,266
数据集API有一个很好的功能叫做“batch”，它能完美地为我们做到这一点。
dataset API has this nice function batch that will do exactly that for us.

117
00:07:11,266 --> 00:07:17,700
所以如果我们在我们的ID数据集上调用“batch”方法，
So if we pass, if we invoke the batch method on our ID dataset,

119
00:07:18,300 --> 00:07:26,399
并传递一个给定的序列长度（这里我们设定为100），
to which we pass a given sequence length, which we said to be 100 here, now the elements,

121
00:07:27,000 --> 00:07:37,665
那么存储在我们的数据集中的数据点就不再是字符，而是100个字符的序列。
the data points that are stored in our dataset are no longer characters, but the sequences of 100 characters.

124
00:07:37,665 --> 00:07:40,564
这里有一个例子。
So here you see an example.

125
00:07:40,565 --> 00:07:47,733
如果我们只取一个元素，那么它们不再是字符，而是百位的字符ID序列，
If we take just one element, they are no longer characters, but sequences of hundreds of their character IDs

127
00:07:47,800 --> 00:07:50,100
你需要的不是字符，而是字符ID。
0 you want not characters, but character IDs.

128
00:07:52,365 --> 00:07:57,698
好的，我们还没完全完成。
Okay, it's not completely we are not completely done here.

130
00:07:58,199 --> 00:08:08,832
我们仍然需要创建我们将传给解码器的输入序列，以及我们想要预测的序列。
We still need to create the input sequences that we were going to pass to the decoder and also the sequences that we want to predict.

133
00:08:08,932 --> 00:08:09,599
对吧？
Right?

134
00:08:09,600 --> 00:08:15,565
那么，这些序列是什么呢？它们只是输入序列中下一个字符的序列。
And what are the sequences that are just the sequences of the next character in the input sequence?

136
00:08:15,565 --> 00:08:19,065
例如，如果我们有序列TensorFlow
So for instance, here, if we have the sequence TensorFlow

137
00:08:20,865 --> 00:08:23,398
并且在开始时有序列TensorFlow，
and the sequence TensorFlow at the beginning,

138
00:08:24,432 --> 00:08:32,364
在序列的开头，我们能从中得到的输入序列是"Tens-or-flow"（省略了最后一个W），
then the input sequence we can do from it is tens-or-flow, We know the W

140
00:08:33,066 --> 00:08:39,066
我们想要预测的目标序列是相同的序列，但是向右移动一位，
and the target sequence that we want to predict is the same sequence, but just shifted by one

142
00:08:39,932 --> 00:08:42,932
所以是"ensor-flow"，
on the right, so ensor-low and

143
00:08:42,932 --> 00:08:52,699
你可以看到E是T的下一个字符，N是E的下一个字符，等等。
you see that E is the next character for Ring T and is the next there for E, etc.

145
00:08:53,100 --> 00:08:55,366
基本上，这个小函数就是实现这一功能的，
So basically this little function does exactly that.

146
00:08:55,700 --> 00:09:15,133
它接受一个原始序列，通过截断这个序列（去掉最后一个字符）创建一个输入序列，目标序列则是从第一个字符开始创建的。
It takes an original sequence, creates an input sequence from that by just truncating that sequence 0 removing the last character and that just the target sequence is created by started at starting add the first character.

150
00:09:15,899 --> 00:09:23,331
我们怎么做呢？我们只需将这个分割输入目标函数映射到我们的序列数据集即可。
So how we do that, we just map the split input target function to our sequence dataset.

152
00:09:24,666 --> 00:09:25,400
好的。
Okay.

153
00:09:26,466 --> 00:09:27,599
这样就完成了。
And it's already does it.

154
00:09:27,600 --> 00:09:29,166
现在让我们看看如何构建模型。
Now let's see how to build the model.

155
00:09:30,600 --> 00:09:34,399
首先，我们设置一些变量：
First off, we set a number of variables

156
00:09:36,332 --> 00:09:46,066
词汇表大小，我们希望表示字符的向量的大小（我认为应该是256），
the vocabulary size, the size of the vectors. We want to represent the characters will I think That would be 256

158
00:09:46,399 --> 00:09:49,732
以及我们的循环层的神经元数量。
and a number of neurons or recurrent layer we'd have.

159
00:09:51,765 --> 00:09:52,799
对于模型本身。
For the model itself.

160
00:09:52,799 --> 00:09:54,831
这是一个相对简单的模型。
It's a relatively simple model.

161
00:09:55,666 --> 00:10:01,499
我们通过使用Keras子类API来创建它。
We create it by using the Keras subclass API.

162
00:10:01,500 --> 00:10:08,731
我们创建了一个新的叫做"MyModel"的类，并从tf.keras.Model中子类化。
We create just a new class called MyModel and we subclass here from tf.keras.Model.

165
00:10:08,732 --> 00:10:17,233
当你这样做的时候，你只需要覆盖两个函数，构造函数和调用函数。
When you do that you only have to override two functions, the constructor and the call function.

167
00:10:17,533 --> 00:10:20,865
那么让我们看看这两个函数分别做什么。
So let's see what each of these function does.

168
00:10:20,865 --> 00:10:26,132
第一个函数主要接收模型的超参数，
The first function takes essentially the hyper parameters of your model,

169
00:10:26,133 --> 00:10:33,299
即词汇表大小，嵌入维度，神经元数量，
the vocabulary size, the embedding dimension, the number of neuron that number of neurons for your recurrent layer,

171
00:10:33,832 --> 00:10:45,864
然后它只是构建你需要的层并将它们存储为类的变量。
and it just constricts the layers you will need and store them as variables of the class.

173
00:10:45,865 --> 00:10:46,565
好的。
Okay.

174
00:10:47,399 --> 00:10:51,931
而这些层是如何连接的，
Now really how these layers are connected,

176
00:10:51,932 --> 00:10:57,700
所有这些都在调用函数中指定，也就是你的网络架构。
all that is specified in the call function, the architecture of your network, if you will.

178
00:10:57,700 --> 00:11:00,799
如果你想的话。
If you want.

179
00:11:00,799 --> 00:11:02,499
让我们看看这个函数主要做什么。
Let's see where to the body does. Here.

180
00:11:02,500 --> 00:11:07,766
它接收输入，也就是代表字符的id序列。
Take the input which are sequences of ids representing the characters.

181
00:11:08,533 --> 00:11:19,032
我们有一个第一层，它会为每一个输入创建一个代表那个输入的向量，也就是训练层。
We have a first layer that we'll create for each of the inits a vector representing that. So that's the training layer.

184
00:11:19,033 --> 00:11:27,199
随着训练的进行，这些代表字符的向量将开始变得越来越有意义，
So as the training progresses, this vector is representing the characters. We'll start to be more and more meaningful.

186
00:11:27,666 --> 00:11:29,699
至少这是我们的想法。
At least that's the idea.

187
00:11:29,700 --> 00:11:39,898
然后，这些静态的字符表示被传递给循环层，它会根据之前看到的内容的上下文来修改这些表示，
Then these static representations of the characters is passed to the recurrent layer that we'll somehow modify these for representation

190
00:11:39,899 --> 00:11:52,032
并生成一个代表之前看到的内容的状态，这将在下一步被重用。
according to the context of what I've seen with what has been seen previously and generate a state of what is seen previously, that would be a reuse in the next step.

193
00:11:52,700 --> 00:12:03,600
最后，我们将循环层的输出传递给一个密集层，它将输出与我们的词汇表中的数量相同的数，
Finally, we pass the output of the recurrent layer to a dense layer that will output as many numbers that we as we have in our vocabulary,

196
00:12:04,000 --> 00:12:20,265
也就是每一个可能的65个字符的一个分数，这个分数代表字符是下一个字符的概率。
which means one score for each of the possible 65 characters and the score represent the probability of the character being the next one.

201
00:12:20,265 --> 00:12:22,765
这就是模型做的所有事情。
So that's all that the model does.

202
00:12:23,299 --> 00:12:29,398
首先，我们实例化了模型。
Then we instantiated.

203
00:12:29,399 --> 00:12:33,132
有了模型后，我们可以通过使用模型摘要来查看模型的结构。
Once we have done that, we can look at the structure of the model

204
00:12:33,133 --> 00:12:44,631
你会看到我们刚刚在模型中实现的嵌入层、循环层和密集层。
using model summary, and you see here you have the I'm building the year, the recurrently year and the dust layer that we just encoded implemented in our, in our model

207
00:12:47,365 --> 00:12:48,232
就是这样。
does that.

208
00:12:48,232 --> 00:12:49,865
接下来，我们要训练模型。
So let's train the model.

209
00:12:49,865 --> 00:12:53,631
在训练模型之前，我们需要一个损失函数，
Before we train the model, we need a loss and that's the loss function

210
00:12:53,633 --> 00:12:58,232
我们将用它来比较模型的输出和真实结果。
that we compared the output of the model with the truth, right?

211
00:12:58,832 --> 00:13:03,332
由于我们的问题本质上是一个多类分类问题，
Since that's essentially a classification problem with many classes

212
00:13:04,365 --> 00:13:14,065
类别就是可能的下一个字符，所以我们会使用损失函数SparseCategoricalCrossentropy。
and the classes being each of the possible characters to be the next, the loss would be the SparseCategoricalCrossentropy loss.

215
00:13:14,832 --> 00:13:24,065
由于神经网络的输出（对数）并不直接对应概率，
And also because the neural network output, the logits are not directly the probability we configure this loss

217
00:13:24,066 --> 00:13:30,299
我们将损失函数配置为从对数分数而不是概率分数计算。
to be computed not from the probability scores, but from the logits scores.

219
00:13:31,299 --> 00:13:31,999
好的，
Okay,

220
00:13:32,332 --> 00:13:36,299
有了损失函数后，我们可以编译模型，
once we have the loss, we can combine our model,

221
00:13:36,299 --> 00:13:41,731
也就是为模型附加损失函数和优化器。
which means that basically we tied to it a loss and also an optimizer.

222
00:13:41,899 --> 00:13:47,699
优化器将在训练过程中更新权重，尽可能地减少损失。
And that will update the weights during training to decrease the loss as much as possible.

224
00:13:48,466 --> 00:14:01,865
此外，我们还设置了一个在训练过程中保存权重的回调函数，这是一个很有用的工具。
Basically, it then here we have a little bit of a callback that we will use and that will save the weights during training, which is a useful item.

229
00:14:01,865 --> 00:14:04,565
至此，我们已经准备好开始训练了。
And we are all set up now to start the training.

230
00:14:04,566 --> 00:14:13,833
我们对数据集执行model.fit，选择想要训练的轮次。
So we do a model.fit on the data set. We choose a number of epochs we want to be trained on.

232
00:14:14,399 --> 00:14:18,599
一轮是对数据集的一次完整遍历。
An epoch is a full pass on the data set.

233
00:14:18,600 --> 00:14:26,999
在这里，我们将数据集遍历了十次，
So here we we have a look at ten, ten times the corpus of plays we have in our text vector

235
00:14:27,832 --> 00:14:35,032
并在训练过程中使用回调函数确保权重被保存。
and we give the callback to make sure that the weights are saved during the the training,

237
00:14:36,466 --> 00:14:37,066
就是这样。
that's it.

238
00:14:37,066 --> 00:14:39,765
这个过程相对简单。我们用数据训练模型，
So that's relatively simple. We train them my data.

239
00:14:39,765 --> 00:14:42,499
现在我们有了训练好的模型，接下来我们该怎么做呢？
We have a train model now what do we do with it?

240
00:14:42,765 --> 00:15:02,098
在编码器-解码器架构中，你不能直接使用你的模型，你需要编写一个解码函数来一步步解码生成的文本。
And that's a bit of a complication in the encoder. Decoder architecture is that you cannot through the immediately use your model, you need to write a sort of a decoding function that's here that will decode the generated text a step at a time using the trained model.

246
00:15:03,533 --> 00:15:03,865
好的。
Okay.

247
00:15:03,865 --> 00:15:12,500
在这个例子中，我们选择将解码函数作为一个Keras模型实现。
So here in this case, we chose to implement this decoding function as a Keras model.

250
00:15:12,500 --> 00:15:15,966
我们从tf.keras.Model子类化。
So we subclass from the tf.keras.Model.

251
00:15:15,966 --> 00:15:21,733
模型中的主要方法是生成一步。
The main method in that model is to generate one step.

253
00:15:22,232 --> 00:15:40,265
这个方法接收输入，即你希望编码器-解码器模型完成、预测、生成新字符的初始字符序列。
It's a quick look to what it does, so it takes the inputs so the input can be to prompt the initial prompt initial the sequence of character you want to the encoder-decoder model to complete, to predict, to generate new new characters.

258
00:15:41,100 --> 00:15:50,999
然后，它将这段文本转换为一串字符，然后再转换为一串id。
So you bypass the input it transform that text into a sequence of character, and then the sequence of characters into a sequence of ids.

261
00:15:51,100 --> 00:15:53,100
使用了ids_from_chars方法。
Using the ids_from_chars.

262
00:15:53,100 --> 00:15:57,465
这里我们之前有一个设置，然后我们调用我们的模型
Here we have a setup previously, and then we call our model

263
00:15:57,600 --> 00:16:00,866
或者之前训练过的编码器-解码器模型。
or encoder-decoder model that has been previously trained.

264
00:16:01,799 --> 00:16:02,699
它做什么呢？
And what does it do?

265
00:16:02,700 --> 00:16:09,965
它会接收id输入并输出预测的对数。
It takes this input of ids and output the predicted logits.

267
00:16:09,966 --> 00:16:14,400
这个对数对应了最可能的Token，也就是最可能的字符，
So this calls for the most probable token the most probable character in this case,

268
00:16:14,600 --> 00:16:18,100
以及一个总结前面看到的内容的状态。
0 along with the state that summarizes what has been seen previously.

269
00:16:20,432 --> 00:16:30,499
从预测的对数中，我们可以选择最可能的Token或字符。
From the predicted logits, we can compute, we can select the most likely tokens or characters.

271
00:16:30,666 --> 00:16:34,199
在做这之前，有一个小技巧，
But before doing that there is a little bit of a trick,

272
00:16:34,600 --> 00:16:38,965
就是我们会将对数除以一个温度(temperature)值。
which is that we divide the logits by a temperature, by a number.

273
00:16:39,533 --> 00:16:43,699
基本上，如果温度是1，就没有任何变化。
So basically if the temperature is one, nothing happens.

274
00:16:43,700 --> 00:17:00,632
如果温度很高，它会使得所有Token的得分变得接近于0，
But if the temperature is very high, what it will do, it will makes the scores associated to each of the token to be predictive. Next will be relatively similar, close to zero.

278
00:17:02,299 --> 00:17:09,164
这意味着这个Token更有可能被选中。
This means that actually this token would be more and more likely to be chosen, right?

280
00:17:09,299 --> 00:17:16,365
所以，如果温度更高，会有更多的多样性，
So there would be more variety, more a more stuff can be predicted if the temperature is higher.

282
00:17:16,365 --> 00:17:20,765
更多的内容可以被预测，这会使得模型变得更有创造性。
So it's a bit more creative If you have a two high temperature,

283
00:17:20,766 --> 00:17:25,299
如果温度太高，神经网络只会预测出无意义的东西。
of course, the neural network would just predict the gibberish.

284
00:17:26,099 --> 00:17:28,164
好的。
Okay.

285
00:17:28,165 --> 00:17:34,200
如果温度非常低，最高的概率得分会被乘以一个非常大的数，
And if you have a true temperature, the highest probability score will be just multiply by a very large number

287
00:17:34,665 --> 00:17:39,565
因为它被一个小数除，这是一个介于0和1之间的数字，
because it's divided by a small number, it's a number between zero and one,

288
00:17:40,099 --> 00:17:56,264
这会使得最高得分的Token有更大的被选中的概率，也就是说，模型的行为会更加确定。
which means that the highest score will be become much, much bigger than the other scores, giving a much higher chance to be selected, which gives you more of the deterministic behavior.

292
00:17:56,266 --> 00:17:57,399
好的，这就是温度。
Okay, that's the temperature.

293
00:17:57,400 --> 00:18:00,100
所以，温度是这种架构中一个重要的参数。
That's an important parameter, as in this type of architecture.

294
00:18:01,532 --> 00:18:01,832
好的。
Okay.

295
00:18:01,833 --> 00:18:03,232
这就是它的功能。
And that's what it does.

296
00:18:03,799 --> 00:18:04,399
好的。
Okay.

297
00:18:04,400 --> 00:18:18,131
有了预测的对数后，我们使用tf.random.categorical从这些概率分数中随机选择下一个最可能的id，
So now we have the predicted logits we use tf.random.categorical to just sample from these probability scores the most likely idea is to be next.

300
00:18:18,865 --> 00:18:22,764
然后将它转换回一个字符，这就是我们返回的结果。
We transform that back to a character and that's what we return.

301
00:18:23,700 --> 00:18:30,133
好的，这就是解码函数的主要工作，大多数解码函数都是这样的结构。
Okay, So that's essentially what the decoding function does and most decoding function at the very same structure.

303
00:18:30,133 --> 00:18:38,800
这个设置“温度”参数的技巧，你也可以应用在大语言模型中。
There is also this temperature trick that you can see as a as a parameter in the case of large language models.

306
00:18:41,299 --> 00:18:44,432
好的，接下来，我们要使用解码函数。
Okay, so let's use our decoding function.

307
00:18:44,432 --> 00:18:46,598
通常，你会在一个循环中使用它。
So typically you use that in the loop.

308
00:18:46,599 --> 00:18:56,598
在这里，我们将通过反复调用解码函数生成1000个字符。
So here we are going to predict 1000 characters by repeatedly making a call to the decoding function generated one step,

310
00:18:57,066 --> 00:19:07,399
你将前一次预测的结果和总结前一次发生的状态一起输入到解码函数中，
to which you feed what has been predicted before, along with the state summarizing what happened before,

313
00:19:07,799 --> 00:19:11,231
它会预测下一个字符和一个新的状态，
and it predict the next character along with a new state.

314
00:19:11,700 --> 00:19:13,700
然后我们就可以开始这个过程了。
And we start the process.

315
00:19:13,700 --> 00:19:17,500
我们以“罗密欧（Romeo）”作为Prompt。
We do sort of a prompt here. That's Romeo.

317
00:19:17,500 --> 00:19:24,599
你打算说什么？然后看看神经网络生成的结果。对吧？
What are you going to say? And then the there are let's let's see what the neuron that generates, right?

319
00:19:24,599 --> 00:19:31,666
生成的是：“no good corona at least take your feetle and if I seem to my love you...”
Says no good corona at least take your feetle and if I seem to my love you...

321
00:19:32,232 --> 00:19:36,299
你会看到，结果并没有完全符合语法规则，
so you see it's not it doesn't make a lot of sense here.

322
00:19:36,566 --> 00:19:50,632
这是因为我只在Vertex AI工作台上用很短的时间训练了这个模型。
0 Remember I've trained it only a few minutes on the work bench, AI work bench in Vertex AI Workbench, which are great by the way, but here

325
00:19:50,700 --> 00:19:54,833
这是一个小实例，只用了一个GPU，所以训练规模很小。
that's a small instance which just one GPU So it was a very small training.

326
00:19:55,133 --> 00:19:57,899
模型的代码只有几行，
The model is written

327
00:19:57,900 --> 00:20:07,900
但你仍然能看到，它真的能从输入数据的结构中捕获到很多东西。
in a few lines, but yet you still see that it can really pick up a lot of things in the structure of the of the input data.

329
00:20:07,900 --> 00:20:11,065
它能够检测到你输入的字符模式。
It detects patterns that you have characters.

330
00:20:11,066 --> 00:20:13,599
比如，“罗密欧”就是我们的输入，
So Romeo, that was our input,

331
00:20:15,266 --> 00:20:15,965
然后神经网络生成了“莱昂特斯（Leontes）”，
but then

332
00:20:15,965 --> 00:20:21,832
以及“莱昂特斯”说的话。
Leontes was generated by the network and then what Leontes says. So

334
00:20:23,532 --> 00:20:26,232
好的，就这样。
okay, that's it.

336
00:20:26,865 --> 00:20:34,099
如果你喜欢这个介绍，你可以在我们的ASL GitHub Repo中找到更多的机器学习Notebooks。
If you like this presentation, you'll find more on our ASL GitHub repository with 90 plus machine learning and notebooks.

338
00:20:34,500 --> 00:20:37,199
如果你觉得这个Repo很有用，请给我们的Repo加星星⭐️。
Don't forget it. If you find it useful, please star our repo.

340
00:20:37,665 --> 00:20:40,331
感谢你花时间来听我讲解。
Thanks for your time.
