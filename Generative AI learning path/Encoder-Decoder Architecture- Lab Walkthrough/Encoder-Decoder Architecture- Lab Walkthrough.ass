[Script Info]

Title: Encoder-Decoder Architecture- Lab Walkthrough
ScriptType: v4.00+
WrapStyle: 0
Collisions: Reverse
PlayResX: 384
PlayResY: 288
Timer: 100.0000
ScaledBorderAndShadow: no
Last Style Storage: Default
Video Aspect Ratio: 0
Video Zoom: 6
Video Position: 0

[V4+ Styles]
Format: Name,Fontname,Fontsize,PrimaryColour,SecondaryColour,OutlineColour,BackColour,Bold,Italic,Underline,StrikeOut,ScaleX,ScaleY,Spacing,Angle,BorderStyle,Outline,Shadow,Alignment,MarginL,MarginR,MarginV,Encoding
Style: Default,LXGW WenKai,20,&H0080FFFF,&H000000FF,&H00000000,&H00000000,-1,0,0,0,100,100,0,0,1,2,2,2,1,1,6,1
Style: Secondary,Helvetica,12,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,-1,0,0,0,100,100,0,0,1,2,2,2,1,1,6,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 9,0:00:05.0,0:00:10.0,Secondary,,0,0,0,,{\an7\fs12\pos(9,11)\fad(300,1000)}{\1c&H00FFFFFF&\2c&H0000FF&}翻译：{\1c&H80FFFF&\2c&H0000FF&}宝玉 + GPT-4
Dialogue: 1,0:00:00.60,0:00:01.40,Secondary,,0,0,0,,Hello, everybody!
Dialogue: 1,0:00:01.40,0:00:01.100,Secondary,,0,0,0,,My name is Benoit
Dialogue: 1,0:00:02.0,0:00:05.43,Secondary,,0,0,0,,Dherin, a machine learning engineer at Google's Advanced Solutions Lab.
Dialogue: 1,0:00:05.77,0:00:09.30,Secondary,,0,0,0,,If you want to know more about what the Advanced Solutions Lab is, please
Dialogue: 1,0:00:09.30,0:00:12.27,Secondary,,0,0,0,,follow the link below in the description box.
Dialogue: 1,0:00:13.23,0:00:17.53,Secondary,,0,0,0,,There is lots of excitement currently around generative AI and new advancements,
Dialogue: 1,0:00:17.53,0:00:20.67,Secondary,,0,0,0,,including new Vertex AI features such as GenAI
Dialogue: 1,0:00:20.67,0:00:23.27,Secondary,,0,0,0,,Studio, Model Garden, GenAI API.
Dialogue: 1,0:00:24.20,0:00:27.40,Secondary,,0,0,0,,Our objective in this short session is to give you a solid footing
Dialogue: 1,0:00:27.70,0:00:31.80,Secondary,,0,0,0,,on some of the underlying concepts that make all the Gen AI magic possible.
Dialogue: 1,0:00:32.20,0:00:34.80,Secondary,,0,0,0,,Today, I’ll go over the code that’s complementary
Dialogue: 1,0:00:34.80,0:00:39.43,Secondary,,0,0,0,,to the “Encoder-Decoder Architecture Overview” course in the same series.
Dialogue: 1,0:00:39.43,0:00:42.53,Secondary,,0,0,0,,We will see together how to build a poetry generator
Dialogue: 1,0:00:42.53,0:00:43.60,Secondary,,0,0,0,,from scratch using the encoder-decoder architecture.
Dialogue: 1,0:00:43.60,0:00:46.7,Secondary,,0,0,0,,Using the angular decoder architecture
Dialogue: 1,0:00:46.80,0:00:49.96,Secondary,,0,0,0,,you’d find to set up instructions in our GitHub repository.
Dialogue: 1,0:00:50.57,0:00:52.77,Secondary,,0,0,0,,Okay, let's now have a look at the code.
Dialogue: 1,0:00:53.37,0:00:58.83,Secondary,,0,0,0,,To access our lab, go in the asl-ml-immersion folder.
Dialogue: 1,0:00:58.87,0:01:01.7,Secondary,,0,0,0,,Then the notebooks folder.
Dialogue: 1,0:01:02.33,0:01:04.43,Secondary,,0,0,0,,Then the text_models folder.
Dialogue: 1,0:01:05.0,0:01:10.47,Secondary,,0,0,0,,And in the solutions from there you'll find the text generation notebook.
Dialogue: 1,0:01:10.97,0:01:16.33,Secondary,,0,0,0,,That's the lab that we'll cover today.
Dialogue: 1,0:01:17.17,0:01:21.3,Secondary,,0,0,0,,In this lab we will implement a character based text generator
Dialogue: 1,0:01:21.40,0:01:24.10,Secondary,,0,0,0,,based on the encoder decoder architecture.
Dialogue: 1,0:01:24.87,0:01:28.47,Secondary,,0,0,0,,Character based means that the tokens consumed
Dialogue: 1,0:01:28.87,0:01:32.66,Secondary,,0,0,0,,and generated by the network are characters and not words.
Dialogue: 1,0:01:33.90,0:01:36.97,Secondary,,0,0,0,,We will use plays as a data set.
Dialogue: 1,0:01:38.33,0:01:40.3,Secondary,,0,0,0,,They have a special structure
Dialogue: 1,0:01:40.3,0:01:45.13,Secondary,,0,0,0,,which are that of people talking with each other.
Dialogue: 1,0:01:45.30,0:01:49.43,Secondary,,0,0,0,,And here you see an example of a piece of text
Dialogue: 1,0:01:49.43,0:01:53.7,Secondary,,0,0,0,,that has been generated by the trained neural network.
Dialogue: 1,0:01:53.77,0:01:56.50,Secondary,,0,0,0,,When the sentences are not necessarily making sense
Dialogue: 1,0:01:56.50,0:01:58.50,Secondary,,0,0,0,,nor are grammatically correct.
Dialogue: 1,0:01:58.50,0:02:00.97,Secondary,,0,0,0,,This is remarkable in many ways.
Dialogue: 1,0:02:00.97,0:02:03.67,Secondary,,0,0,0,,First of all, remember, it's character based.
Dialogue: 1,0:02:03.67,0:02:07.80,Secondary,,0,0,0,,So it means that it learns to predict only the most probable characters.
Dialogue: 1,0:02:08.33,0:02:11.7,Secondary,,0,0,0,,Despite that, it was able to learn pretty well
Dialogue: 1,0:02:11.7,0:02:14.13,Secondary,,0,0,0,,The notion of words separated by blank spaces.
Dialogue: 1,0:02:14.43,0:02:19.97,Secondary,,0,0,0,,And also the basic structure of a play with the characters talking to each other.
Dialogue: 1,0:02:21.3,0:02:21.43,Secondary,,0,0,0,,So going
Dialogue: 1,0:02:21.43,0:02:26.7,Secondary,,0,0,0,,of what is a very small network, as you will see, it's based on the rnn
Dialogue: 1,0:02:26.7,0:02:29.10,Secondary,,0,0,0,,0 and architecture and only trained for 30 epochs in the vertex
Dialogue: 1,0:02:30.0,0:02:32.73,Secondary,,0,0,0,,air workbench, which is a pretty fast training time.
Dialogue: 1,0:02:33.90,0:02:37.3,Secondary,,0,0,0,,So let's look at the code now.
Dialogue: 1,0:02:42.10,0:02:47.37,Secondary,,0,0,0,,So the first thing is to import the libraries that we need.
Dialogue: 1,0:02:47.67,0:02:52.20,Secondary,,0,0,0,,In particular, we could or encoder decoder architecture
Dialogue: 1,0:02:52.67,0:02:56.13,Secondary,,0,0,0,,using TensorFlow Keras to impart that.
Dialogue: 1,0:02:57.0,0:02:59.30,Secondary,,0,0,0,,Then we download our data set
Dialogue: 1,0:02:59.30,0:03:02.10,Secondary,,0,0,0,,using tf.keras.utils.get_file.
Dialogue: 1,0:03:02.83,0:03:10.50,Secondary,,0,0,0,,So now the dataset is on disk and we just need to load it into a variable called text.
Dialogue: 1,0:03:10.50,0:03:14.83,Secondary,,0,0,0,,So the text variable now contains the whole string representing
Dialogue: 1,0:03:14.83,0:03:18.67,Secondary,,0,0,0,,all the all the plays in that Shakespeare dataset.
Dialogue: 1,0:03:19.73,0:03:23.7,Secondary,,0,0,0,,Can I have a quick look at what it is?
Dialogue: 1,0:03:23.7,0:03:27.40,Secondary,,0,0,0,,And you see if we printed the first 250 characters.
Dialogue: 1,0:03:27.90,0:03:30.13,Secondary,,0,0,0,,You have the first citizens speaking to
Dialogue: 1,0:03:31.43,0:03:33.73,Secondary,,0,0,0,,everybody and everybody else is speaking
Dialogue: 1,0:03:34.90,0:03:38.23,Secondary,,0,0,0,,to the first citizen.
Dialogue: 1,0:03:38.70,0:03:43.3,Secondary,,0,0,0,,The cell computes the number of unique characters that we have in that
Dialogue: 1,0:03:44.40,0:03:47.70,Secondary,,0,0,0,,in the text dataset, and we see that we have
Dialogue: 1,0:03:48.27,0:03:52.6,Secondary,,0,0,0,,65 unique characters, right?
Dialogue: 1,0:03:52.20,0:03:56.63,Secondary,,0,0,0,,These characters would be the tokens that the neural network will consume
Dialogue: 1,0:03:57.30,0:04:01.20,Secondary,,0,0,0,,during training and will generating during this service.
Dialogue: 1,0:04:02.7,0:04:04.23,Secondary,,0,0,0,,So the first step here
Dialogue: 1,0:04:04.23,0:04:06.90,Secondary,,0,0,0,,now is to vectorize the text.
Dialogue: 1,0:04:07.23,0:04:08.93,Secondary,,0,0,0,,What do we mean by that?
Dialogue: 1,0:04:08.93,0:04:13.13,Secondary,,0,0,0,,It means that first of all,
Dialogue: 1,0:04:13.13,0:04:17.37,Secondary,,0,0,0,,we will need to extract
Dialogue: 1,0:04:17.37,0:04:21.90,Secondary,,0,0,0,,from the actual string sequence of characters, which we can do
Dialogue: 1,0:04:22.17,0:04:26.20,Secondary,,0,0,0,,with TensorFlow by using tf.strings.unicode_split.
Dialogue: 1,0:04:26.90,0:04:29.40,Secondary,,0,0,0,,So now, for example, texts
Dialogue: 1,0:04:30.33,0:04:34.40,Secondary,,0,0,0,,here are transformed into a list
Dialogue: 1,0:04:34.87,0:04:37.53,Secondary,,0,0,0,,of sequences of characters.
Dialogue: 1,0:04:39.37,0:04:41.93,Secondary,,0,0,0,,A neural network cannot consume immediately.
Dialogue: 1,0:04:41.93,0:04:42.87,Secondary,,0,0,0,,The characters.
Dialogue: 1,0:04:42.87,0:04:44.90,Secondary,,0,0,0,,We need to transform that into numbers.
Dialogue: 1,0:04:45.30,0:04:49.40,Secondary,,0,0,0,,So we need to simply map each of the characters to a given
Dialogue: 1,0:04:50.37,0:04:52.80,Secondary,,0,0,0,,id. For that we have the
Dialogue: 1,0:04:53.3,0:04:56.23,Secondary,,0,0,0,,tf.keras.layers.StringLookup
Dialogue: 1,0:04:57.67,0:05:00.80,Secondary,,0,0,0,,to which you just need to pass to
Dialogue: 1,0:05:02.10,0:05:04.50,Secondary,,0,0,0,,the list of your vocabulary.
Dialogue: 1,0:05:04.50,0:05:08.23,Secondary,,0,0,0,,The 65 unique character that we have in our corpus
Dialogue: 1,0:05:09.3,0:05:14.40,Secondary,,0,0,0,,and that we produce a layer that when passed the characters
Dialogue: 1,0:05:14.77,0:05:17.53,Secondary,,0,0,0,,will produce corresponding ids.
Dialogue: 1,0:05:18.33,0:05:23.10,Secondary,,0,0,0,,So within that, that layer you have a mapping that has been generated
Dialogue: 1,0:05:23.57,0:05:25.93,Secondary,,0,0,0,,between the characters and the
Dialogue: 1,0:05:27.90,0:05:30.90,Secondary,,0,0,0,,id. To get the inverse mapping,
Dialogue: 1,0:05:30.93,0:05:33.63,Secondary,,0,0,0,,you use the same layer of string lookup
Dialogue: 1,0:05:34.77,0:05:37.73,Secondary,,0,0,0,,with the exact same vocabulary that you retrieve
Dialogue: 1,0:05:37.73,0:05:40.30,Secondary,,0,0,0,,from the first of the year by using get vocabulary.
Dialogue: 1,0:05:41.17,0:05:45.30,Secondary,,0,0,0,,But you set that parameter to be true, invert, equal equal true,
Dialogue: 1,0:05:45.60,0:05:50.60,Secondary,,0,0,0,,and that will compute the invert mapping, which is the mapping from
Dialogue: 1,0:05:51.0,0:05:54.20,Secondary,,0,0,0,,id to chars
Dialogue: 1,0:05:54.47,0:05:54.70,Secondary,,0,0,0,,Right.
Dialogue: 1,0:05:54.70,0:05:56.100,Secondary,,0,0,0,,And indeed, if you pass to this mapping
Dialogue: 1,0:05:58.27,0:06:01.23,Secondary,,0,0,0,,sequence of ID’s, the ID’s,
Dialogue: 1,0:06:02.33,0:06:04.87,Secondary,,0,0,0,,it gives you back
Dialogue: 1,0:06:04.87,0:06:07.47,Secondary,,0,0,0,,the corresponding characters.
Dialogue: 1,0:06:07.63,0:06:10.60,Secondary,,0,0,0,,Using the mapping that start in the memory of this layer.
Dialogue: 1,0:06:11.73,0:06:14.23,Secondary,,0,0,0,,So that's that.
Dialogue: 1,0:06:15.17,0:06:15.90,Secondary,,0,0,0,,Okay.
Dialogue: 1,0:06:15.90,0:06:20.63,Secondary,,0,0,0,,Now let's that's been the dataset that we will train our neural network with.
Dialogue: 1,0:06:20.93,0:06:23.46,Secondary,,0,0,0,,For that we are using the
Dialogue: 1,0:06:24.20,0:06:27.3,Secondary,,0,0,0,,tf.data.Dataset API,
Dialogue: 1,0:06:28.3,0:06:30.13,Secondary,,0,0,0,,which has this nice method
Dialogue: 1,0:06:30.27,0:06:33.13,Secondary,,0,0,0,,from tons of slices
Dialogue: 1,0:06:33.60,0:06:34.80,Secondary,,0,0,0,,which will convert.
Dialogue: 1,0:06:34.80,0:06:39.87,Secondary,,0,0,0,,That answer of instance represents or whole corpus of text of plays
Dialogue: 1,0:06:40.37,0:06:43.97,Secondary,,0,0,0,,as id it will store that into it to have data data sets.
Dialogue: 1,0:06:44.57,0:06:49.16,Secondary,,0,0,0,,So at this point, the elements of these datasets
Dialogue: 1,0:06:49.77,0:06:52.60,Secondary,,0,0,0,,are just the individual characters.
Dialogue: 1,0:06:52.60,0:06:53.90,Secondary,,0,0,0,,So that's not great for us.
Dialogue: 1,0:06:53.90,0:06:57.93,Secondary,,0,0,0,,But we want to feed our neural network with our sequences
Dialogue: 1,0:06:58.17,0:07:01.50,Secondary,,0,0,0,,of the same length but not just one character.
Dialogue: 1,0:07:01.57,0:07:03.43,Secondary,,0,0,0,,We need to predict the next character. So
Dialogue: 1,0:07:05.43,0:07:06.76,Secondary,,0,0,0,,but luckily the
Dialogue: 1,0:07:06.77,0:07:11.27,Secondary,,0,0,0,,dataset API has this nice function batch that will do exactly that for us.
Dialogue: 1,0:07:11.27,0:07:14.40,Secondary,,0,0,0,,So if we pass, if we invoke the batch
Dialogue: 1,0:07:14.83,0:07:17.70,Secondary,,0,0,0,,method on our ID dataset,
Dialogue: 1,0:07:18.30,0:07:21.20,Secondary,,0,0,0,,to which we pass a given sequence length,
Dialogue: 1,0:07:21.60,0:07:26.40,Secondary,,0,0,0,,which we said to be 100 here, now the elements,
Dialogue: 1,0:07:27.0,0:07:30.70,Secondary,,0,0,0,,the data points that are stored in our dataset
Dialogue: 1,0:07:30.93,0:07:34.50,Secondary,,0,0,0,,are no longer characters, but the sequences of
Dialogue: 1,0:07:35.90,0:07:37.67,Secondary,,0,0,0,,100 characters.
Dialogue: 1,0:07:37.67,0:07:40.56,Secondary,,0,0,0,,So here you see an example.
Dialogue: 1,0:07:40.57,0:07:43.13,Secondary,,0,0,0,,If we take just one element, they are no longer characters,
Dialogue: 1,0:07:43.47,0:07:47.73,Secondary,,0,0,0,,but sequences of hundreds of their character IDs
Dialogue: 1,0:07:47.80,0:07:50.10,Secondary,,0,0,0,,0 you want not characters, but character IDs.
Dialogue: 1,0:07:52.37,0:07:54.63,Secondary,,0,0,0,,Okay, it's
Dialogue: 1,0:07:54.90,0:07:57.70,Secondary,,0,0,0,,not completely we are not completely done here.
Dialogue: 1,0:07:58.20,0:08:00.83,Secondary,,0,0,0,,We still need to create
Dialogue: 1,0:08:02.23,0:08:05.50,Secondary,,0,0,0,,the input sequences that we were going to pass to the decoder
Dialogue: 1,0:08:05.70,0:08:08.83,Secondary,,0,0,0,,and also the sequences that we want to predict.
Dialogue: 1,0:08:08.93,0:08:09.60,Secondary,,0,0,0,,Right?
Dialogue: 1,0:08:09.60,0:08:13.30,Secondary,,0,0,0,,And what are the sequences that are just the sequences of the next character
Dialogue: 1,0:08:13.57,0:08:15.57,Secondary,,0,0,0,,in the input sequence?
Dialogue: 1,0:08:15.57,0:08:19.7,Secondary,,0,0,0,,So for instance, here, if we have the sequence TensorFlow
Dialogue: 1,0:08:20.87,0:08:23.40,Secondary,,0,0,0,,and the sequence TensorFlow at the beginning,
Dialogue: 1,0:08:24.43,0:08:28.57,Secondary,,0,0,0,,then the input sequence we can do from
Dialogue: 1,0:08:28.57,0:08:32.36,Secondary,,0,0,0,,it is tens-or-flow, We know the W
Dialogue: 1,0:08:33.7,0:08:36.23,Secondary,,0,0,0,,and the target sequence that we want to predict
Dialogue: 1,0:08:36.60,0:08:39.7,Secondary,,0,0,0,,is the same sequence, but just shifted by one
Dialogue: 1,0:08:39.93,0:08:42.93,Secondary,,0,0,0,,on the right, so ensor-low and
Dialogue: 1,0:08:42.93,0:08:47.90,Secondary,,0,0,0,,you see that E is the next character for Ring T
Dialogue: 1,0:08:48.77,0:08:52.70,Secondary,,0,0,0,,and is the next there for E, etc.
Dialogue: 1,0:08:53.10,0:08:55.37,Secondary,,0,0,0,,So basically this little function does exactly that.
Dialogue: 1,0:08:55.70,0:08:58.67,Secondary,,0,0,0,,It takes an original sequence, creates
Dialogue: 1,0:08:59.33,0:09:03.60,Secondary,,0,0,0,,an input sequence from that by just truncating that sequence
Dialogue: 1,0:09:03.60,0:09:08.10,Secondary,,0,0,0,,0 removing the last character and that just the target sequence is created
Dialogue: 1,0:09:09.57,0:09:15.13,Secondary,,0,0,0,,by started at starting add the first character.
Dialogue: 1,0:09:15.90,0:09:18.43,Secondary,,0,0,0,,So how we do that, we just map
Dialogue: 1,0:09:19.80,0:09:23.33,Secondary,,0,0,0,,the split input target function to our sequence dataset.
Dialogue: 1,0:09:24.67,0:09:25.40,Secondary,,0,0,0,,Okay.
Dialogue: 1,0:09:26.47,0:09:27.60,Secondary,,0,0,0,,And it's already does it.
Dialogue: 1,0:09:27.60,0:09:29.17,Secondary,,0,0,0,,Now let's see how to build the model.
Dialogue: 1,0:09:30.60,0:09:34.40,Secondary,,0,0,0,,First off, we set a number of variables
Dialogue: 1,0:09:36.33,0:09:40.60,Secondary,,0,0,0,,the vocabulary size, the size of the vectors.
Dialogue: 1,0:09:40.60,0:09:46.7,Secondary,,0,0,0,,We want to represent the characters will I think That would be 256
Dialogue: 1,0:09:46.40,0:09:49.73,Secondary,,0,0,0,,and a number of neurons or recurrent layer we'd have.
Dialogue: 1,0:09:51.77,0:09:52.80,Secondary,,0,0,0,,For the model itself.
Dialogue: 1,0:09:52.80,0:09:54.83,Secondary,,0,0,0,,It's a relatively simple model.
Dialogue: 1,0:09:55.67,0:10:01.50,Secondary,,0,0,0,,We create it by using the Keras subclass API.
Dialogue: 1,0:10:01.50,0:10:03.93,Secondary,,0,0,0,,We create just a new class called MyModel
Dialogue: 1,0:10:04.57,0:10:07.10,Secondary,,0,0,0,,and we subclass here
Dialogue: 1,0:10:07.33,0:10:08.73,Secondary,,0,0,0,,from tf.keras.Model.
Dialogue: 1,0:10:08.73,0:10:11.70,Secondary,,0,0,0,,When you do that you only have to
Dialogue: 1,0:10:13.20,0:10:17.23,Secondary,,0,0,0,,override two functions, the constructor and the call function.
Dialogue: 1,0:10:17.53,0:10:20.87,Secondary,,0,0,0,,So let's see what each of these function does.
Dialogue: 1,0:10:20.87,0:10:26.13,Secondary,,0,0,0,,The first function takes essentially the hyper parameters of your model,
Dialogue: 1,0:10:26.13,0:10:29.13,Secondary,,0,0,0,,the vocabulary size, the embedding dimension, the number of neuron
Dialogue: 1,0:10:29.13,0:10:33.30,Secondary,,0,0,0,,that number of neurons for your recurrent layer,
Dialogue: 1,0:10:33.83,0:10:38.90,Secondary,,0,0,0,,and it just constricts the layers you will need and store them
Dialogue: 1,0:10:39.93,0:10:45.86,Secondary,,0,0,0,,as variables of the class.
Dialogue: 1,0:10:45.87,0:10:46.57,Secondary,,0,0,0,,Okay.
Dialogue: 1,0:10:47.40,0:10:49.33,Secondary,,0,0,0,,Now really
Dialogue: 1,0:10:49.40,0:10:51.93,Secondary,,0,0,0,,how these layers are connected,
Dialogue: 1,0:10:51.93,0:10:55.46,Secondary,,0,0,0,,all that is specified in the call function,
Dialogue: 1,0:10:55.47,0:10:57.70,Secondary,,0,0,0,,the architecture of your network, if you will.
Dialogue: 1,0:10:57.70,0:11:00.80,Secondary,,0,0,0,,If you want.
Dialogue: 1,0:11:00.80,0:11:02.50,Secondary,,0,0,0,,Let's see where to the body does. Here.
Dialogue: 1,0:11:02.50,0:11:07.77,Secondary,,0,0,0,,Take the input which are sequences of ids representing the characters.
Dialogue: 1,0:11:08.53,0:11:13.40,Secondary,,0,0,0,,We have a first layer that we'll create for each of the
Dialogue: 1,0:11:14.43,0:11:17.33,Secondary,,0,0,0,,inits a vector representing that.
Dialogue: 1,0:11:17.33,0:11:19.3,Secondary,,0,0,0,,So that's the training layer.
Dialogue: 1,0:11:19.3,0:11:24.73,Secondary,,0,0,0,,So as the training progresses, this vector is representing the characters.
Dialogue: 1,0:11:24.73,0:11:27.20,Secondary,,0,0,0,,We'll start to be more and more meaningful.
Dialogue: 1,0:11:27.67,0:11:29.70,Secondary,,0,0,0,,At least that's the idea.
Dialogue: 1,0:11:29.70,0:11:33.47,Secondary,,0,0,0,,Then these static representations of the characters
Dialogue: 1,0:11:33.87,0:11:36.60,Secondary,,0,0,0,,is passed to the recurrent layer that we'll somehow
Dialogue: 1,0:11:37.87,0:11:39.90,Secondary,,0,0,0,,modify these for representation
Dialogue: 1,0:11:39.90,0:11:45.7,Secondary,,0,0,0,,according to the context of what I've seen with what has been seen
Dialogue: 1,0:11:45.70,0:11:49.10,Secondary,,0,0,0,,previously and generate a state of
Dialogue: 1,0:11:49.10,0:11:52.3,Secondary,,0,0,0,,what is seen previously, that would be a reuse in the next step.
Dialogue: 1,0:11:52.70,0:11:55.73,Secondary,,0,0,0,,Finally, we pass the output of the
Dialogue: 1,0:11:57.7,0:11:59.93,Secondary,,0,0,0,,recurrent layer to a dense layer that will output
Dialogue: 1,0:12:00.50,0:12:03.60,Secondary,,0,0,0,,as many numbers that we as we have in our vocabulary,
Dialogue: 1,0:12:04.0,0:12:06.93,Secondary,,0,0,0,,which means one
Dialogue: 1,0:12:06.93,0:12:09.10,Secondary,,0,0,0,,score for each
Dialogue: 1,0:12:09.10,0:12:12.77,Secondary,,0,0,0,,of the possible 65 characters and the score
Dialogue: 1,0:12:13.80,0:12:17.10,Secondary,,0,0,0,,represent the probability of the character
Dialogue: 1,0:12:17.93,0:12:20.27,Secondary,,0,0,0,,being the next one.
Dialogue: 1,0:12:20.27,0:12:22.77,Secondary,,0,0,0,,So that's all that the model does.
Dialogue: 1,0:12:23.30,0:12:29.40,Secondary,,0,0,0,,Then we instantiated.
Dialogue: 1,0:12:29.40,0:12:33.13,Secondary,,0,0,0,,Once we have done that, we can look at the structure of the model
Dialogue: 1,0:12:33.13,0:12:37.33,Secondary,,0,0,0,,using model summary, and you see here you have the I'm building the year,
Dialogue: 1,0:12:37.33,0:12:41.53,Secondary,,0,0,0,,the recurrently year and the dust layer that we just encoded
Dialogue: 1,0:12:42.37,0:12:44.63,Secondary,,0,0,0,,implemented in our, in our model
Dialogue: 1,0:12:47.37,0:12:48.23,Secondary,,0,0,0,,does that.
Dialogue: 1,0:12:48.23,0:12:49.87,Secondary,,0,0,0,,So let's train the model.
Dialogue: 1,0:12:49.87,0:12:53.63,Secondary,,0,0,0,,Before we train the model, we need a loss and that's the loss function
Dialogue: 1,0:12:53.63,0:12:58.23,Secondary,,0,0,0,,that we compared the output of the model with the truth, right?
Dialogue: 1,0:12:58.83,0:13:03.33,Secondary,,0,0,0,,Since that's essentially a classification problem with many classes
Dialogue: 1,0:13:04.37,0:13:06.60,Secondary,,0,0,0,,and the classes being
Dialogue: 1,0:13:06.60,0:13:09.60,Secondary,,0,0,0,,each of the possible characters to be the next,
Dialogue: 1,0:13:10.83,0:13:14.7,Secondary,,0,0,0,,the loss would be the SparseCategoricalCrossentropy loss.
Dialogue: 1,0:13:14.83,0:13:18.57,Secondary,,0,0,0,,And also because the neural network
Dialogue: 1,0:13:19.0,0:13:24.7,Secondary,,0,0,0,,output, the logits are not directly the probability we configure this loss
Dialogue: 1,0:13:24.7,0:13:27.87,Secondary,,0,0,0,,to be computed not from the probability scores,
Dialogue: 1,0:13:27.87,0:13:30.30,Secondary,,0,0,0,,but from the logits scores.
Dialogue: 1,0:13:31.30,0:13:31.100,Secondary,,0,0,0,,Okay,
Dialogue: 1,0:13:32.33,0:13:36.30,Secondary,,0,0,0,,once we have the loss, we can combine our model,
Dialogue: 1,0:13:36.30,0:13:41.73,Secondary,,0,0,0,,which means that basically we tied to it a loss and also an optimizer.
Dialogue: 1,0:13:41.90,0:13:44.63,Secondary,,0,0,0,,And that will update the weights during training
Dialogue: 1,0:13:44.73,0:13:47.70,Secondary,,0,0,0,,to decrease the loss as much as possible.
Dialogue: 1,0:13:48.47,0:13:49.77,Secondary,,0,0,0,,Basically, it
Dialogue: 1,0:13:50.63,0:13:53.57,Secondary,,0,0,0,,then here we have a little bit of a callback that we will use
Dialogue: 1,0:13:54.50,0:13:57.83,Secondary,,0,0,0,,and that will save the weights during training,
Dialogue: 1,0:13:57.83,0:14:00.53,Secondary,,0,0,0,,which is a useful
Dialogue: 1,0:14:01.7,0:14:01.87,Secondary,,0,0,0,,item.
Dialogue: 1,0:14:01.87,0:14:04.57,Secondary,,0,0,0,,And we are all set up now to start the training.
Dialogue: 1,0:14:04.57,0:14:09.70,Secondary,,0,0,0,,So we do a model.fit on the data set.
Dialogue: 1,0:14:09.70,0:14:13.83,Secondary,,0,0,0,,We choose a number of epochs we want to be trained on.
Dialogue: 1,0:14:14.40,0:14:18.60,Secondary,,0,0,0,,An epoch is a full pass on the data set.
Dialogue: 1,0:14:18.60,0:14:22.80,Secondary,,0,0,0,,So here we we have a look at ten, ten times
Dialogue: 1,0:14:23.10,0:14:26.100,Secondary,,0,0,0,,the corpus of plays we have in our text vector
Dialogue: 1,0:14:27.83,0:14:30.60,Secondary,,0,0,0,,and we give the callback to make sure
Dialogue: 1,0:14:30.60,0:14:35.3,Secondary,,0,0,0,,that the weights are saved during the the training,
Dialogue: 1,0:14:36.47,0:14:37.7,Secondary,,0,0,0,,that's it.
Dialogue: 1,0:14:37.7,0:14:39.77,Secondary,,0,0,0,,So that's relatively simple. We train them my data.
Dialogue: 1,0:14:39.77,0:14:42.50,Secondary,,0,0,0,,We have a train model now what do we do with it?
Dialogue: 1,0:14:42.77,0:14:45.63,Secondary,,0,0,0,,And that's a bit of a complication in the
Dialogue: 1,0:14:45.63,0:14:46.23,Secondary,,0,0,0,,encoder.
Dialogue: 1,0:14:46.23,0:14:49.100,Secondary,,0,0,0,,Decoder architecture is that you cannot through the immediately use your model,
Dialogue: 1,0:14:50.30,0:14:55.33,Secondary,,0,0,0,,you need to write a sort of a decoding function that's here
Dialogue: 1,0:14:55.90,0:14:58.63,Secondary,,0,0,0,,that will decode the generated text
Dialogue: 1,0:14:59.43,0:15:02.10,Secondary,,0,0,0,,a step at a time using the trained model.
Dialogue: 1,0:15:03.53,0:15:03.87,Secondary,,0,0,0,,Okay.
Dialogue: 1,0:15:03.87,0:15:06.70,Secondary,,0,0,0,,So here in this case, we chose to
Dialogue: 1,0:15:07.80,0:15:10.96,Secondary,,0,0,0,,implement this decoding function
Dialogue: 1,0:15:10.97,0:15:12.50,Secondary,,0,0,0,,as a Keras model.
Dialogue: 1,0:15:12.50,0:15:15.97,Secondary,,0,0,0,,So we subclass from the tf.keras.Model.
Dialogue: 1,0:15:15.97,0:15:19.13,Secondary,,0,0,0,,The main method
Dialogue: 1,0:15:19.13,0:15:21.73,Secondary,,0,0,0,,in that model is to generate one step.
Dialogue: 1,0:15:22.23,0:15:25.23,Secondary,,0,0,0,,It's a quick look to what it does,
Dialogue: 1,0:15:25.63,0:15:28.97,Secondary,,0,0,0,,so it takes the inputs so the input can be
Dialogue: 1,0:15:29.67,0:15:33.73,Secondary,,0,0,0,,to prompt the initial prompt initial the sequence of character you want to
Dialogue: 1,0:15:34.90,0:15:37.20,Secondary,,0,0,0,,the encoder-decoder model to
Dialogue: 1,0:15:37.20,0:15:40.27,Secondary,,0,0,0,,complete, to predict, to generate new new characters.
Dialogue: 1,0:15:41.10,0:15:43.30,Secondary,,0,0,0,,So you bypass the input it
Dialogue: 1,0:15:44.20,0:15:47.27,Secondary,,0,0,0,,transform that text into a sequence of character,
Dialogue: 1,0:15:47.27,0:15:50.100,Secondary,,0,0,0,,and then the sequence of characters into a sequence of ids.
Dialogue: 1,0:15:51.10,0:15:53.10,Secondary,,0,0,0,,Using the ids_from_chars.
Dialogue: 1,0:15:53.10,0:15:57.47,Secondary,,0,0,0,,Here we have a setup previously, and then we call our model
Dialogue: 1,0:15:57.60,0:16:00.87,Secondary,,0,0,0,,or encoder-decoder model that has been previously trained.
Dialogue: 1,0:16:01.80,0:16:02.70,Secondary,,0,0,0,,And what does it do?
Dialogue: 1,0:16:02.70,0:16:05.87,Secondary,,0,0,0,,It takes this input of ids and
Dialogue: 1,0:16:07.17,0:16:09.97,Secondary,,0,0,0,,output the predicted logits.
Dialogue: 1,0:16:09.97,0:16:14.40,Secondary,,0,0,0,,So this calls for the most probable token the most probable character in this case,
Dialogue: 1,0:16:14.60,0:16:18.10,Secondary,,0,0,0,,0 along with the state that summarizes what has been seen previously.
Dialogue: 1,0:16:20.43,0:16:25.80,Secondary,,0,0,0,,From the predicted logits, we can compute, we can select
Dialogue: 1,0:16:26.60,0:16:30.50,Secondary,,0,0,0,,the most likely tokens or characters.
Dialogue: 1,0:16:30.67,0:16:34.20,Secondary,,0,0,0,,But before doing that there is a little bit of a trick,
Dialogue: 1,0:16:34.60,0:16:38.97,Secondary,,0,0,0,,which is that we divide the logits by a temperature, by a number.
Dialogue: 1,0:16:39.53,0:16:43.70,Secondary,,0,0,0,,So basically if the temperature is one, nothing happens.
Dialogue: 1,0:16:43.70,0:16:49.37,Secondary,,0,0,0,,But if the temperature is very high,
Dialogue: 1,0:16:49.37,0:16:52.70,Secondary,,0,0,0,,what it will do, it will makes the scores
Dialogue: 1,0:16:53.23,0:16:56.80,Secondary,,0,0,0,,associated to each of the token to be predictive.
Dialogue: 1,0:16:56.80,0:17:00.63,Secondary,,0,0,0,,Next will be relatively similar, close to zero.
Dialogue: 1,0:17:02.30,0:17:04.80,Secondary,,0,0,0,,This means that actually
Dialogue: 1,0:17:04.80,0:17:09.16,Secondary,,0,0,0,,this token would be more and more likely to be chosen, right?
Dialogue: 1,0:17:09.30,0:17:14.53,Secondary,,0,0,0,,So there would be more variety, more a more stuff can be predicted
Dialogue: 1,0:17:14.80,0:17:16.37,Secondary,,0,0,0,,if the temperature is higher.
Dialogue: 1,0:17:16.37,0:17:20.77,Secondary,,0,0,0,,So it's a bit more creative If you have a two high temperature,
Dialogue: 1,0:17:20.77,0:17:25.30,Secondary,,0,0,0,,of course, the neural network would just predict the gibberish.
Dialogue: 1,0:17:26.10,0:17:28.16,Secondary,,0,0,0,,Okay.
Dialogue: 1,0:17:28.17,0:17:31.20,Secondary,,0,0,0,,And if you have a true temperature, the highest probability
Dialogue: 1,0:17:31.20,0:17:34.20,Secondary,,0,0,0,,score will be just multiply by a very large number
Dialogue: 1,0:17:34.67,0:17:39.57,Secondary,,0,0,0,,because it's divided by a small number, it's a number between zero and one,
Dialogue: 1,0:17:40.10,0:17:42.90,Secondary,,0,0,0,,which means that the highest score will be
Dialogue: 1,0:17:44.27,0:17:46.10,Secondary,,0,0,0,,become much, much
Dialogue: 1,0:17:46.10,0:17:50.46,Secondary,,0,0,0,,bigger than the other scores, giving a much higher chance
Dialogue: 1,0:17:50.47,0:17:56.26,Secondary,,0,0,0,,to be selected, which gives you more of the deterministic behavior.
Dialogue: 1,0:17:56.27,0:17:57.40,Secondary,,0,0,0,,Okay, that's the temperature.
Dialogue: 1,0:17:57.40,0:18:00.10,Secondary,,0,0,0,,That's an important parameter, as in this type of architecture.
Dialogue: 1,0:18:01.53,0:18:01.83,Secondary,,0,0,0,,Okay.
Dialogue: 1,0:18:01.83,0:18:03.23,Secondary,,0,0,0,,And that's what it does.
Dialogue: 1,0:18:03.80,0:18:04.40,Secondary,,0,0,0,,Okay.
Dialogue: 1,0:18:04.40,0:18:09.10,Secondary,,0,0,0,,So now we have the predicted logits we use tf.random.categorical
Dialogue: 1,0:18:09.10,0:18:12.37,Secondary,,0,0,0,,to just sample from these probability scores
Dialogue: 1,0:18:13.47,0:18:18.13,Secondary,,0,0,0,,the most likely idea is to be next.
Dialogue: 1,0:18:18.87,0:18:22.76,Secondary,,0,0,0,,We transform that back to a character and that's what we return.
Dialogue: 1,0:18:23.70,0:18:26.70,Secondary,,0,0,0,,Okay, So that's essentially what the decoding function does
Dialogue: 1,0:18:27.0,0:18:30.13,Secondary,,0,0,0,,and most decoding function at the very same structure.
Dialogue: 1,0:18:30.13,0:18:34.7,Secondary,,0,0,0,,There is also this temperature trick that you can see as a
Dialogue: 1,0:18:34.90,0:18:37.20,Secondary,,0,0,0,,as a parameter in the case of large
Dialogue: 1,0:18:37.20,0:18:38.80,Secondary,,0,0,0,,language models.
Dialogue: 1,0:18:41.30,0:18:44.43,Secondary,,0,0,0,,Okay, so let's use our decoding function.
Dialogue: 1,0:18:44.43,0:18:46.60,Secondary,,0,0,0,,So typically you use that in the loop.
Dialogue: 1,0:18:46.60,0:18:51.93,Secondary,,0,0,0,,So here we are going to predict 1000 characters by repeatedly
Dialogue: 1,0:18:51.93,0:18:56.60,Secondary,,0,0,0,,making a call to the decoding function generated one step,
Dialogue: 1,0:18:57.7,0:18:59.7,Secondary,,0,0,0,,to which you feed
Dialogue: 1,0:19:00.43,0:19:02.53,Secondary,,0,0,0,,what has been predicted
Dialogue: 1,0:19:02.53,0:19:07.40,Secondary,,0,0,0,,before, along with the state summarizing what happened before,
Dialogue: 1,0:19:07.80,0:19:11.23,Secondary,,0,0,0,,and it predict the next character along with a new state.
Dialogue: 1,0:19:11.70,0:19:13.70,Secondary,,0,0,0,,And we start the process.
Dialogue: 1,0:19:13.70,0:19:15.60,Secondary,,0,0,0,,We do sort of a prompt here. That's
Dialogue: 1,0:19:16.70,0:19:17.50,Secondary,,0,0,0,,Romeo.
Dialogue: 1,0:19:17.50,0:19:19.87,Secondary,,0,0,0,,What are you going to say? And then the
Dialogue: 1,0:19:21.0,0:19:24.60,Secondary,,0,0,0,,there are let's let's see what the neuron that generates, right?
Dialogue: 1,0:19:24.60,0:19:28.57,Secondary,,0,0,0,,Says no good corona at least take your feetle
Dialogue: 1,0:19:28.83,0:19:31.67,Secondary,,0,0,0,,and if I seem to my love you...
Dialogue: 1,0:19:32.23,0:19:36.30,Secondary,,0,0,0,,so you see it's not it doesn't make a lot of sense here.
Dialogue: 1,0:19:36.57,0:19:39.10,Secondary,,0,0,0,,0 Remember I've trained it only a few minutes
Dialogue: 1,0:19:41.20,0:19:43.97,Secondary,,0,0,0,,on the work bench,
Dialogue: 1,0:19:44.10,0:19:50.63,Secondary,,0,0,0,,AI work bench in Vertex AI Workbench, which are great by the way, but here
Dialogue: 1,0:19:50.70,0:19:54.83,Secondary,,0,0,0,,that's a small instance which just one GPU So it was a very small training.
Dialogue: 1,0:19:55.13,0:19:57.90,Secondary,,0,0,0,,The model is written
Dialogue: 1,0:19:57.90,0:20:03.27,Secondary,,0,0,0,,in a few lines, but yet you still see that it can really pick up
Dialogue: 1,0:20:03.27,0:20:07.90,Secondary,,0,0,0,,a lot of things in the structure of the of the input data.
Dialogue: 1,0:20:07.90,0:20:11.7,Secondary,,0,0,0,,It detects patterns that you have characters.
Dialogue: 1,0:20:11.7,0:20:13.60,Secondary,,0,0,0,,So Romeo, that was our input,
Dialogue: 1,0:20:15.27,0:20:15.97,Secondary,,0,0,0,,but then
Dialogue: 1,0:20:15.97,0:20:19.26,Secondary,,0,0,0,,Leontes was generated by the network and then what
Dialogue: 1,0:20:19.60,0:20:21.83,Secondary,,0,0,0,,Leontes says. So
Dialogue: 1,0:20:23.53,0:20:24.30,Secondary,,0,0,0,,okay,
Dialogue: 1,0:20:25.20,0:20:26.23,Secondary,,0,0,0,,that's it.
Dialogue: 1,0:20:26.87,0:20:30.20,Secondary,,0,0,0,,If you like this presentation, you'll find more on our ASL
Dialogue: 1,0:20:30.20,0:20:34.10,Secondary,,0,0,0,,GitHub repository with 90 plus machine learning and notebooks.
Dialogue: 1,0:20:34.50,0:20:35.7,Secondary,,0,0,0,,Don't forget it.
Dialogue: 1,0:20:35.7,0:20:37.20,Secondary,,0,0,0,,If you find it useful, please star our repo.
Dialogue: 1,0:20:37.67,0:20:40.33,Secondary,,0,0,0,,Thanks for your time.
Dialogue: 1,0:00:00.60,0:00:05.43,Default,,0,0,0,,大家好！我是Google Advanced Solutions \NLab的机器学习工程师Benoit Dherin。
Dialogue: 1,0:00:05.77,0:00:12.27,Default,,0,0,0,,如果你想了解更多关于Advanced Solutions\N Lab的信息，请点击描述框下方的链接。
Dialogue: 1,0:00:13.23,0:00:23.27,Default,,0,0,0,,围绕着生成性AI和相关的新技术取得了令人兴奋的进展，例如像新\N的Vertex AI特性如GenAI Studio，Model Garden，GenAI API等。
Dialogue: 1,0:00:24.20,0:00:31.80,Default,,0,0,0,,在这个简短的课程中，我们的目标是帮\N助你了解一些生成式AI的基本概念。
Dialogue: 1,0:00:32.20,0:00:39.43,Default,,0,0,0,,今天，我将讲解与同一系列的“编码器-解\N码器架构概述”课程相辅助的代码。
Dialogue: 1,0:00:39.43,0:00:46.7,Default,,0,0,0,,我们将一起学习如何从头开始使用编码\N器-解码器架构建立一个诗歌生成器，
Dialogue: 1,0:00:46.80,0:00:49.96,Default,,0,0,0,,你可以在我们的GitHub Repo找到设置指南。
Dialogue: 1,0:00:50.57,0:00:52.77,Default,,0,0,0,,好的，现在让我们看一下代码。
Dialogue: 1,0:00:53.37,0:00:58.83,Default,,0,0,0,,要访问我们的Lab，请进入asl-ml-immersion文件夹。
Dialogue: 1,0:00:58.87,0:01:01.7,Default,,0,0,0,,然后是notebooks文件夹。
Dialogue: 1,0:01:02.33,0:01:04.43,Default,,0,0,0,,然后是text_models文件夹。
Dialogue: 1,0:01:05.0,0:01:10.47,Default,,0,0,0,,在solutions文件夹里你会找到text_generation的Notebook，
Dialogue: 1,0:01:10.97,0:01:16.33,Default,,0,0,0,,这就是我们今天要讲的实验。
Dialogue: 1,0:01:17.17,0:01:24.10,Default,,0,0,0,,在这个实验中，我们将基于编码器解码器\N架构实现一个基于字符的文本生成器。
Dialogue: 1,0:01:24.87,0:01:32.66,Default,,0,0,0,,基于字符意味着网络消耗和生成的Token是字符而不是单词。
Dialogue: 1,0:01:33.90,0:01:36.97,Default,,0,0,0,,我们将使用剧本作为数据集。
Dialogue: 1,0:01:38.33,0:01:45.13,Default,,0,0,0,,它们有特殊的结构，就像人们在彼此对话。
Dialogue: 1,0:01:45.30,0:01:53.7,Default,,0,0,0,,你在这里看到的是一个由训练过的神经网络生成的文本示例，
Dialogue: 1,0:01:53.77,0:01:56.50,Default,,0,0,0,,即使这些句子没有必然的意义，
Dialogue: 1,0:01:56.50,0:01:58.50,Default,,0,0,0,,也不一定符合语法规则，
Dialogue: 1,0:01:58.50,0:02:00.97,Default,,0,0,0,,但仍然有很多值得注意的地方。
Dialogue: 1,0:02:00.97,0:02:03.67,Default,,0,0,0,,首先，记住，它是基于字符的。
Dialogue: 1,0:02:03.67,0:02:07.80,Default,,0,0,0,,所以它只学习预测最可能的字符。
Dialogue: 1,0:02:08.33,0:02:11.7,Default,,0,0,0,,尽管如此，它还是学得很好
Dialogue: 1,0:02:11.7,0:02:14.13,Default,,0,0,0,,像单词被空格分开的概念，
Dialogue: 1,0:02:14.43,0:02:19.97,Default,,0,0,0,,以及剧本的基本结构，角色彼此对话。
Dialogue: 1,0:02:21.3,0:02:21.43,Default,,0,0,0,,所以继续
Dialogue: 1,0:02:21.43,0:02:32.73,Default,,0,0,0,,你将看到的这个非常小的网络是基于RNN 0架构的，并且只在\NVertex AIr工作台上训练了30个周期，这是一个非常快的训练时间。
Dialogue: 1,0:02:33.90,0:02:37.3,Default,,0,0,0,,那么现在我们来看看代码。
Dialogue: 1,0:02:42.10,0:02:47.37,Default,,0,0,0,,首先要导入我们需要的库。
Dialogue: 1,0:02:47.67,0:02:56.13,Default,,0,0,0,,特别是，我们可以使用TensorFlow Keras来导入编码器解码器架构。
Dialogue: 1,0:02:57.0,0:02:59.30,Default,,0,0,0,,然后我们下载数据集
Dialogue: 1,0:02:59.30,0:03:02.10,Default,,0,0,0,,使用tf.keras.utils.get_file。
Dialogue: 1,0:03:02.83,0:03:10.50,Default,,0,0,0,,现在数据集已经在磁盘上，我们只需要将其加载到变量text中。
Dialogue: 1,0:03:10.50,0:03:18.67,Default,,0,0,0,,这个text变量现在包含了代表整个莎\N士比亚剧本数据集的整个字符串。
Dialogue: 1,0:03:19.73,0:03:23.7,Default,,0,0,0,,我可以快速看看它是什么吗？
Dialogue: 1,0:03:23.7,0:03:27.40,Default,,0,0,0,,你会看到，如果我们打印出前250个字符，
Dialogue: 1,0:03:27.90,0:03:38.23,Default,,0,0,0,,有第一位公民对所有人说话，然后所有人对第一位公民说话。
Dialogue: 1,0:03:38.70,0:03:52.6,Default,,0,0,0,,这个单元计算我们在文本数据集中有多少唯一的\N字符，我们看到我们有65个唯一的字符，对吧？
Dialogue: 1,0:03:52.20,0:04:01.20,Default,,0,0,0,,这些字符将是神经网络在训练和生成过程中将消耗的Token。
Dialogue: 1,0:04:02.7,0:04:06.90,Default,,0,0,0,,现在的第一步是对文本进行向量化。
Dialogue: 1,0:04:07.23,0:04:08.93,Default,,0,0,0,,向量化是什么意思呢？
Dialogue: 1,0:04:08.93,0:04:17.37,Default,,0,0,0,,这意味着我们首先需要从实际的字符串序列中提取出字符序列，
Dialogue: 1,0:04:17.37,0:04:26.20,Default,,0,0,0,,我们可以用TensorFlow的tf.strings.unicode_split来做到这一点。
Dialogue: 1,0:04:26.90,0:04:37.53,Default,,0,0,0,,以现在，例如，文本在这里被转换成了字符序列的列表。
Dialogue: 1,0:04:39.37,0:04:44.90,Default,,0,0,0,,神经网络不能直接消耗字符，我们需要将它转换成数字。
Dialogue: 1,0:04:45.30,0:04:49.40,Default,,0,0,0,,所以我们需要简单地将每个字符映射到一个给定的id。
Dialogue: 1,0:04:50.37,0:05:04.50,Default,,0,0,0,,为此我们有tf.keras.layers.\NStringLookup，你只需要将你的词汇表列表\N传给它。
Dialogue: 1,0:05:04.50,0:05:17.53,Default,,0,0,0,,我们在语料库中有的65个唯一字符，然后我们生\N成一个层，当传入字符时，它会产生相应的id。
Dialogue: 1,0:05:18.33,0:05:25.93,Default,,0,0,0,,在那个层中，你有一个已经生成的字符和id之间的映射。
Dialogue: 1,0:05:27.90,0:05:40.30,Default,,0,0,0,,要获得反向映射，你使用相同的字符串查找\N层，使用从第一层获取的相同的词汇表。
Dialogue: 1,0:05:41.17,0:05:45.30,Default,,0,0,0,,但你将参数设置为真，invert等于True，
Dialogue: 1,0:05:45.60,0:05:54.70,Default,,0,0,0,,那将计算反向映射，即从id到字符的映射。
Dialogue: 1,0:05:54.70,0:06:07.47,Default,,0,0,0,,实际上，如果你传递一系列ID给这个映射，它会给你返回对应的字符，
Dialogue: 1,0:06:07.63,0:06:10.60,Default,,0,0,0,,使用在这个层的内存中存储的映射。
Dialogue: 1,0:06:11.73,0:06:14.23,Default,,0,0,0,,就是这样。
Dialogue: 1,0:06:15.17,0:06:15.90,Default,,0,0,0,,好的。
Dialogue: 1,0:06:15.90,0:06:20.63,Default,,0,0,0,,现在我们来准备我们将用来训练神经网络的数据集。
Dialogue: 1,0:06:20.93,0:06:34.80,Default,,0,0,0,,我们使用了tf.data.Dataset API，它有一个\N很好的方法叫做“from_tensor_slices”，
Dialogue: 1,0:06:34.80,0:06:43.97,Default,,0,0,0,,它将把一个实例数组，代表我们的整个剧本文\N本语料库的id，转化为一个TF data数据集。
Dialogue: 1,0:06:44.57,0:06:52.60,Default,,0,0,0,,此刻，这些数据集的元素仅仅是单个字符。
Dialogue: 1,0:06:52.60,0:06:53.90,Default,,0,0,0,,这对我们来说并不理想。
Dialogue: 1,0:06:53.90,0:07:01.50,Default,,0,0,0,,我们想要给我们的神经网络提供的是\N等长的序列，而不仅仅是一个字符。
Dialogue: 1,0:07:01.57,0:07:03.43,Default,,0,0,0,,我们需要预测下一个字符。
Dialogue: 1,0:07:05.43,0:07:06.76,Default,,0,0,0,,但幸运的是，
Dialogue: 1,0:07:06.77,0:07:11.27,Default,,0,0,0,,数据集API有一个很好的功能叫做\N“batch”，它能完美地为我们做到这一点。
Dialogue: 1,0:07:11.27,0:07:17.70,Default,,0,0,0,,所以如果我们在我们的ID数据集上调用“batch”方法，
Dialogue: 1,0:07:18.30,0:07:26.40,Default,,0,0,0,,并传递一个给定的序列长度（这里我们设定为100），
Dialogue: 1,0:07:27.0,0:07:37.67,Default,,0,0,0,,那么存储在我们的数据集中的数据点就\N不再是字符，而是100个字符的序列。
Dialogue: 1,0:07:37.67,0:07:40.56,Default,,0,0,0,,这里有一个例子。
Dialogue: 1,0:07:40.57,0:07:47.73,Default,,0,0,0,,如果我们只取一个元素，那么它们不再\N是字符，而是百位的字符ID序列，
Dialogue: 1,0:07:47.80,0:07:50.10,Default,,0,0,0,,你需要的不是字符，而是字符ID。
Dialogue: 1,0:07:52.37,0:07:57.70,Default,,0,0,0,,好的，我们还没完全完成。
Dialogue: 1,0:07:58.20,0:08:08.83,Default,,0,0,0,,我们仍然需要创建我们将传给解码器的\N输入序列，以及我们想要预测的序列。
Dialogue: 1,0:08:08.93,0:08:09.60,Default,,0,0,0,,对吧？
Dialogue: 1,0:08:09.60,0:08:15.57,Default,,0,0,0,,那么，这些序列是什么呢？它们只是输入序列中下一个字符的序列。
Dialogue: 1,0:08:15.57,0:08:19.7,Default,,0,0,0,,例如，如果我们有序列TensorFlow
Dialogue: 1,0:08:20.87,0:08:23.40,Default,,0,0,0,,并且在开始时有序列TensorFlow，
Dialogue: 1,0:08:24.43,0:08:32.36,Default,,0,0,0,,在序列的开头，我们能从中得到的输入序列\N是"Tens-or-flow"（省略了最后一个W），
Dialogue: 1,0:08:33.7,0:08:39.7,Default,,0,0,0,,我们想要预测的目标序列是相同的序列，但是向右移动一位，
Dialogue: 1,0:08:39.93,0:08:42.93,Default,,0,0,0,,所以是"ensor-flow"，
Dialogue: 1,0:08:42.93,0:08:52.70,Default,,0,0,0,,你可以看到E是T的下一个字符，N是E的下一个字符，等等。
Dialogue: 1,0:08:53.10,0:08:55.37,Default,,0,0,0,,基本上，这个小函数就是实现这一功能的，
Dialogue: 1,0:08:55.70,0:09:15.13,Default,,0,0,0,,它接受一个原始序列，通过截断这个序列（去掉最后一个字符）创\N建一个输入序列，目标序列则是从第一个字符开始创建的。
Dialogue: 1,0:09:15.90,0:09:23.33,Default,,0,0,0,,我们怎么做呢？我们只需将这个分割输入目\N标函数映射到我们的序列数据集即可。
Dialogue: 1,0:09:24.67,0:09:25.40,Default,,0,0,0,,好的。
Dialogue: 1,0:09:26.47,0:09:27.60,Default,,0,0,0,,这样就完成了。
Dialogue: 1,0:09:27.60,0:09:29.17,Default,,0,0,0,,现在让我们看看如何构建模型。
Dialogue: 1,0:09:30.60,0:09:34.40,Default,,0,0,0,,首先，我们设置一些变量：
Dialogue: 1,0:09:36.33,0:09:46.7,Default,,0,0,0,,词汇表大小，我们希望表示字符的向量的大小（我认为应该是256），
Dialogue: 1,0:09:46.40,0:09:49.73,Default,,0,0,0,,以及我们的循环层的神经元数量。
Dialogue: 1,0:09:51.77,0:09:52.80,Default,,0,0,0,,对于模型本身。
Dialogue: 1,0:09:52.80,0:09:54.83,Default,,0,0,0,,这是一个相对简单的模型。
Dialogue: 1,0:09:55.67,0:10:01.50,Default,,0,0,0,,我们通过使用Keras子类API来创建它。
Dialogue: 1,0:10:01.50,0:10:08.73,Default,,0,0,0,,我们创建了一个新的叫做"MyModel"的\N类，并从tf.keras.Model中子类化。
Dialogue: 1,0:10:08.73,0:10:17.23,Default,,0,0,0,,当你这样做的时候，你只需要覆盖两个函数，构造函数和调用函数。
Dialogue: 1,0:10:17.53,0:10:20.87,Default,,0,0,0,,那么让我们看看这两个函数分别做什么。
Dialogue: 1,0:10:20.87,0:10:26.13,Default,,0,0,0,,第一个函数主要接收模型的超参数，
Dialogue: 1,0:10:26.13,0:10:33.30,Default,,0,0,0,,即词汇表大小，嵌入维度，神经元数量，
Dialogue: 1,0:10:33.83,0:10:45.86,Default,,0,0,0,,然后它只是构建你需要的层并将它们存储为类的变量。
Dialogue: 1,0:10:45.87,0:10:46.57,Default,,0,0,0,,好的。
Dialogue: 1,0:10:47.40,0:10:51.93,Default,,0,0,0,,而这些层是如何连接的，
Dialogue: 1,0:10:51.93,0:10:57.70,Default,,0,0,0,,所有这些都在调用函数中指定，也就是你的网络架构。
Dialogue: 1,0:10:57.70,0:11:00.80,Default,,0,0,0,,如果你想的话。
Dialogue: 1,0:11:00.80,0:11:02.50,Default,,0,0,0,,让我们看看这个函数主要做什么。
Dialogue: 1,0:11:02.50,0:11:07.77,Default,,0,0,0,,它接收输入，也就是代表字符的id序列。
Dialogue: 1,0:11:08.53,0:11:19.3,Default,,0,0,0,,我们有一个第一层，它会为每一个输入创建一\N个代表那个输入的向量，也就是训练层。
Dialogue: 1,0:11:19.3,0:11:27.20,Default,,0,0,0,,随着训练的进行，这些代表字符的向量将开始变得越来越有意义，
Dialogue: 1,0:11:27.67,0:11:29.70,Default,,0,0,0,,至少这是我们的想法。
Dialogue: 1,0:11:29.70,0:11:39.90,Default,,0,0,0,,然后，这些静态的字符表示被传递给循环层，它会根\N据之前看到的内容的上下文来修改这些表示，
Dialogue: 1,0:11:39.90,0:11:52.3,Default,,0,0,0,,并生成一个代表之前看到的内容的状态，这将在下一步被重用。
Dialogue: 1,0:11:52.70,0:12:03.60,Default,,0,0,0,,最后，我们将循环层的输出传递给一个密集层，它\N将输出与我们的词汇表中的数量相同的数，
Dialogue: 1,0:12:04.0,0:12:20.27,Default,,0,0,0,,也就是每一个可能的65个字符的一个分数，\N这个分数代表字符是下一个字符的概率。
Dialogue: 1,0:12:20.27,0:12:22.77,Default,,0,0,0,,这就是模型做的所有事情。
Dialogue: 1,0:12:23.30,0:12:29.40,Default,,0,0,0,,首先，我们实例化了模型。
Dialogue: 1,0:12:29.40,0:12:33.13,Default,,0,0,0,,有了模型后，我们可以通过使用模型摘要来查看模型的结构。
Dialogue: 1,0:12:33.13,0:12:44.63,Default,,0,0,0,,你会看到我们刚刚在模型中实现的嵌入层、循环层和密集层。
Dialogue: 1,0:12:47.37,0:12:48.23,Default,,0,0,0,,就是这样。
Dialogue: 1,0:12:48.23,0:12:49.87,Default,,0,0,0,,接下来，我们要训练模型。
Dialogue: 1,0:12:49.87,0:12:53.63,Default,,0,0,0,,在训练模型之前，我们需要一个损失函数，
Dialogue: 1,0:12:53.63,0:12:58.23,Default,,0,0,0,,我们将用它来比较模型的输出和真实结果。
Dialogue: 1,0:12:58.83,0:13:03.33,Default,,0,0,0,,由于我们的问题本质上是一个多类分类问题，
Dialogue: 1,0:13:04.37,0:13:14.7,Default,,0,0,0,,类别就是可能的下一个字符，所以我们会使用损\N失函数SparseCategoricalCrossentropy。
Dialogue: 1,0:13:14.83,0:13:24.7,Default,,0,0,0,,由于神经网络的输出（对数）并不直接对应概率，
Dialogue: 1,0:13:24.7,0:13:30.30,Default,,0,0,0,,我们将损失函数配置为从对数分数而不是概率分数计算。
Dialogue: 1,0:13:31.30,0:13:31.100,Default,,0,0,0,,好的，
Dialogue: 1,0:13:32.33,0:13:36.30,Default,,0,0,0,,有了损失函数后，我们可以编译模型，
Dialogue: 1,0:13:36.30,0:13:41.73,Default,,0,0,0,,也就是为模型附加损失函数和优化器。
Dialogue: 1,0:13:41.90,0:13:47.70,Default,,0,0,0,,优化器将在训练过程中更新权重，尽可能地减少损失。
Dialogue: 1,0:13:48.47,0:14:01.87,Default,,0,0,0,,此外，我们还设置了一个在训练过程中保存权\N重的回调函数，这是一个很有用的工具。
Dialogue: 1,0:14:01.87,0:14:04.57,Default,,0,0,0,,至此，我们已经准备好开始训练了。
Dialogue: 1,0:14:04.57,0:14:13.83,Default,,0,0,0,,我们对数据集执行model.fit，选择想要训练的轮次。
Dialogue: 1,0:14:14.40,0:14:18.60,Default,,0,0,0,,一轮是对数据集的一次完整遍历。
Dialogue: 1,0:14:18.60,0:14:26.100,Default,,0,0,0,,在这里，我们将数据集遍历了十次，
Dialogue: 1,0:14:27.83,0:14:35.3,Default,,0,0,0,,并在训练过程中使用回调函数确保权重被保存。
Dialogue: 1,0:14:36.47,0:14:37.7,Default,,0,0,0,,就是这样。
Dialogue: 1,0:14:37.7,0:14:39.77,Default,,0,0,0,,这个过程相对简单。我们用数据训练模型，
Dialogue: 1,0:14:39.77,0:14:42.50,Default,,0,0,0,,现在我们有了训练好的模型，接下来我们该怎么做呢？
Dialogue: 1,0:14:42.77,0:15:02.10,Default,,0,0,0,,在编码器-解码器架构中，你不能直接使用你的模型，你\N需要编写一个解码函数来一步步解码生成的文本。
Dialogue: 1,0:15:03.53,0:15:03.87,Default,,0,0,0,,好的。
Dialogue: 1,0:15:03.87,0:15:12.50,Default,,0,0,0,,在这个例子中，我们选择将解码函数作为一个Keras模型实现。
Dialogue: 1,0:15:12.50,0:15:15.97,Default,,0,0,0,,我们从tf.keras.Model子类化。
Dialogue: 1,0:15:15.97,0:15:21.73,Default,,0,0,0,,模型中的主要方法是生成一步。
Dialogue: 1,0:15:22.23,0:15:40.27,Default,,0,0,0,,这个方法接收输入，即你希望编码器-解码器模\N型完成、预测、生成新字符的初始字符序列。
Dialogue: 1,0:15:41.10,0:15:50.100,Default,,0,0,0,,然后，它将这段文本转换为一串字符，然后再转换为一串id。
Dialogue: 1,0:15:51.10,0:15:53.10,Default,,0,0,0,,使用了ids_from_chars方法。
Dialogue: 1,0:15:53.10,0:15:57.47,Default,,0,0,0,,这里我们之前有一个设置，然后我们调用我们的模型
Dialogue: 1,0:15:57.60,0:16:00.87,Default,,0,0,0,,或者之前训练过的编码器-解码器模型。
Dialogue: 1,0:16:01.80,0:16:02.70,Default,,0,0,0,,它做什么呢？
Dialogue: 1,0:16:02.70,0:16:09.97,Default,,0,0,0,,它会接收id输入并输出预测的对数。
Dialogue: 1,0:16:09.97,0:16:14.40,Default,,0,0,0,,这个对数对应了最可能的Token，也就是最可能的字符，
Dialogue: 1,0:16:14.60,0:16:18.10,Default,,0,0,0,,以及一个总结前面看到的内容的状态。
Dialogue: 1,0:16:20.43,0:16:30.50,Default,,0,0,0,,从预测的对数中，我们可以选择最可能的Token或字符。
Dialogue: 1,0:16:30.67,0:16:34.20,Default,,0,0,0,,在做这之前，有一个小技巧，
Dialogue: 1,0:16:34.60,0:16:38.97,Default,,0,0,0,,就是我们会将对数除以一个温度(temperature)值。
Dialogue: 1,0:16:39.53,0:16:43.70,Default,,0,0,0,,基本上，如果温度是1，就没有任何变化。
Dialogue: 1,0:16:43.70,0:17:00.63,Default,,0,0,0,,如果温度很高，它会使得所有Token的得分变得接近于0，
Dialogue: 1,0:17:02.30,0:17:09.16,Default,,0,0,0,,这意味着这个Token更有可能被选中。
Dialogue: 1,0:17:09.30,0:17:16.37,Default,,0,0,0,,所以，如果温度更高，会有更多的多样性，
Dialogue: 1,0:17:16.37,0:17:20.77,Default,,0,0,0,,更多的内容可以被预测，这会使得模型变得更有创造性。
Dialogue: 1,0:17:20.77,0:17:25.30,Default,,0,0,0,,如果温度太高，神经网络只会预测出无意义的东西。
Dialogue: 1,0:17:26.10,0:17:28.16,Default,,0,0,0,,好的。
Dialogue: 1,0:17:28.17,0:17:34.20,Default,,0,0,0,,如果温度非常低，最高的概率得分会被乘以一个非常大的数，
Dialogue: 1,0:17:34.67,0:17:39.57,Default,,0,0,0,,因为它被一个小数除，这是一个介于0和1之间的数字，
Dialogue: 1,0:17:40.10,0:17:56.26,Default,,0,0,0,,这会使得最高得分的Token有更大的被选中的\N概率，也就是说，模型的行为会更加确定。
Dialogue: 1,0:17:56.27,0:17:57.40,Default,,0,0,0,,好的，这就是温度。
Dialogue: 1,0:17:57.40,0:18:00.10,Default,,0,0,0,,所以，温度是这种架构中一个重要的参数。
Dialogue: 1,0:18:01.53,0:18:01.83,Default,,0,0,0,,好的。
Dialogue: 1,0:18:01.83,0:18:03.23,Default,,0,0,0,,这就是它的功能。
Dialogue: 1,0:18:03.80,0:18:04.40,Default,,0,0,0,,好的。
Dialogue: 1,0:18:04.40,0:18:18.13,Default,,0,0,0,,有了预测的对数后，我们使用tf.random.categorical\N从这些概率分数中随机选择下一个最可能的id，
Dialogue: 1,0:18:18.87,0:18:22.76,Default,,0,0,0,,然后将它转换回一个字符，这就是我们返回的结果。
Dialogue: 1,0:18:23.70,0:18:30.13,Default,,0,0,0,,好的，这就是解码函数的主要工作，大多数解码函数都是这样的结构。
Dialogue: 1,0:18:30.13,0:18:38.80,Default,,0,0,0,,这个设置“温度”参数的技巧，你也可以应用在大语言模型中。
Dialogue: 1,0:18:41.30,0:18:44.43,Default,,0,0,0,,好的，接下来，我们要使用解码函数。
Dialogue: 1,0:18:44.43,0:18:46.60,Default,,0,0,0,,通常，你会在一个循环中使用它。
Dialogue: 1,0:18:46.60,0:18:56.60,Default,,0,0,0,,在这里，我们将通过反复调用解码函数生成1000个字符。
Dialogue: 1,0:18:57.7,0:19:07.40,Default,,0,0,0,,你将前一次预测的结果和总结前一次发\N生的状态一起输入到解码函数中，
Dialogue: 1,0:19:07.80,0:19:11.23,Default,,0,0,0,,它会预测下一个字符和一个新的状态，
Dialogue: 1,0:19:11.70,0:19:13.70,Default,,0,0,0,,然后我们就可以开始这个过程了。
Dialogue: 1,0:19:13.70,0:19:17.50,Default,,0,0,0,,我们以“罗密欧（Romeo）”作为Prompt。
Dialogue: 1,0:19:17.50,0:19:24.60,Default,,0,0,0,,你打算说什么？然后看看神经网络生成的结果。对吧？
Dialogue: 1,0:19:24.60,0:19:31.67,Default,,0,0,0,,生成的是：“no good corona at least take your \Nfeetle and if I seem to my love you...”
Dialogue: 1,0:19:32.23,0:19:36.30,Default,,0,0,0,,你会看到，结果并没有完全符合语法规则，
Dialogue: 1,0:19:36.57,0:19:50.63,Default,,0,0,0,,这是因为我只在Vertex AI工作台上用很短的时间训练了这个模型。
Dialogue: 1,0:19:50.70,0:19:54.83,Default,,0,0,0,,这是一个小实例，只用了一个GPU，所以训练规模很小。
Dialogue: 1,0:19:55.13,0:19:57.90,Default,,0,0,0,,模型的代码只有几行，
Dialogue: 1,0:19:57.90,0:20:07.90,Default,,0,0,0,,但你仍然能看到，它真的能从输入数据的结构中捕获到很多东西。
Dialogue: 1,0:20:07.90,0:20:11.7,Default,,0,0,0,,它能够检测到你输入的字符模式。
Dialogue: 1,0:20:11.7,0:20:13.60,Default,,0,0,0,,比如，“罗密欧”就是我们的输入，
Dialogue: 1,0:20:15.27,0:20:15.97,Default,,0,0,0,,然后神经网络生成了“莱昂特斯（Leontes）”，
Dialogue: 1,0:20:15.97,0:20:21.83,Default,,0,0,0,,以及“莱昂特斯”说的话。
Dialogue: 1,0:20:23.53,0:20:26.23,Default,,0,0,0,,好的，就这样。
Dialogue: 1,0:20:26.87,0:20:34.10,Default,,0,0,0,,如果你喜欢这个介绍，你可以在我们的ASL \NGitHub Repo中找到更多的机器学习Notebooks。
Dialogue: 1,0:20:34.50,0:20:37.20,Default,,0,0,0,,如果你觉得这个Repo很有用，请给我们的Repo加星星⭐️。
Dialogue: 1,0:20:37.67,0:20:40.33,Default,,0,0,0,,感谢你花时间来听我讲解。