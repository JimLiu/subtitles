1
00:00:00,600 --> 00:00:01,399
Hello, everybody!

2
00:00:01,399 --> 00:00:01,999
My name is Benoit

3
00:00:02,000 --> 00:00:05,432
Dherin, a machine learning engineer at Google's Advanced Solutions Lab.

4
00:00:05,766 --> 00:00:09,299
If you want to know more about what the Advanced Solutions Lab is, please

5
00:00:09,300 --> 00:00:12,266
follow the link below in the description box.

6
00:00:13,233 --> 00:00:17,532
There is lots of excitement currently around generative AI and new advancements,

7
00:00:17,533 --> 00:00:20,666
including new Vertex AI features such as GenAI

8
00:00:20,666 --> 00:00:23,266
Studio, Model Garden, GenAI API.

9
00:00:24,199 --> 00:00:27,399
Our objective in this short session is to give you a solid footing

10
00:00:27,699 --> 00:00:31,798
on some of the underlying concepts that make all the Gen AI magic possible.

11
00:00:32,200 --> 00:00:34,799
Today, I’ll go over the code that’s complementary

12
00:00:34,799 --> 00:00:39,432
to the “Encoder-Decoder Architecture Overview” course in the same series.

13
00:00:39,432 --> 00:00:42,532
We will see together how to build a poetry generator

14
00:00:42,533 --> 00:00:43,599
from scratch using the encoder-decoder architecture.

15
00:00:43,600 --> 00:00:46,066
Using the angular decoder architecture

16
00:00:46,799 --> 00:00:49,964
you’d find to set up instructions in our GitHub repository.

17
00:00:50,566 --> 00:00:52,766
Okay, let's now have a look at the code.

18
00:00:53,365 --> 00:00:58,831
To access our lab, go in the asl-ml-immersion folder.

19
00:00:58,865 --> 00:01:01,065
Then the notebooks folder.

20
00:01:02,332 --> 00:01:04,432
Then the text_models folder.

21
00:01:05,000 --> 00:01:10,466
And in the solutions from there you'll find the text generation notebook.

22
00:01:10,965 --> 00:01:16,332
That's the lab that we'll cover today.

23
00:01:17,165 --> 00:01:21,032
In this lab we will implement a character based text generator

24
00:01:21,400 --> 00:01:24,100
based on the encoder decoder architecture.

25
00:01:24,865 --> 00:01:28,465
Character based means that the tokens consumed

26
00:01:28,865 --> 00:01:32,664
and generated by the network are characters and not words.

27
00:01:33,900 --> 00:01:36,965
We will use plays as a data set.

28
00:01:38,332 --> 00:01:40,032
They have a special structure

29
00:01:40,033 --> 00:01:45,132
which are that of people talking with each other.

30
00:01:45,299 --> 00:01:49,432
And here you see an example of a piece of text

31
00:01:49,433 --> 00:01:53,066
that has been generated by the trained neural network.

32
00:01:53,766 --> 00:01:56,500
When the sentences are not necessarily making sense

33
00:01:56,500 --> 00:01:58,500
nor are grammatically correct.

34
00:01:58,500 --> 00:02:00,965
This is remarkable in many ways.

35
00:02:00,965 --> 00:02:03,665
First of all, remember, it's character based.

36
00:02:03,665 --> 00:02:07,799
So it means that it learns to predict only the most probable characters.

37
00:02:08,332 --> 00:02:11,065
Despite that, it was able to learn pretty well

38
00:02:11,066 --> 00:02:14,133
The notion of words separated by blank spaces.

39
00:02:14,432 --> 00:02:19,965
And also the basic structure of a play with the characters talking to each other.

40
00:02:21,032 --> 00:02:21,432
So going

41
00:02:21,432 --> 00:02:26,065
of what is a very small network, as you will see, it's based on the rnn

42
00:02:26,066 --> 00:02:29,100
0 and architecture and only trained for 30 epochs in the vertex

43
00:02:30,000 --> 00:02:32,733
air workbench, which is a pretty fast training time.

44
00:02:33,900 --> 00:02:37,033
So let's look at the code now.

45
00:02:42,099 --> 00:02:47,365
So the first thing is to import the libraries that we need.

46
00:02:47,665 --> 00:02:52,198
In particular, we could or encoder decoder architecture

47
00:02:52,665 --> 00:02:56,132
using TensorFlow Keras to impart that.

48
00:02:57,000 --> 00:02:59,299
Then we download our data set

49
00:02:59,300 --> 00:03:02,099
using tf.keras.utils.get_file.

50
00:03:02,832 --> 00:03:10,498
So now the dataset is on disk and we just need to load it into a variable called text.

51
00:03:10,500 --> 00:03:14,832
So the text variable now contains the whole string representing

52
00:03:14,832 --> 00:03:18,665
all the all the plays in that Shakespeare dataset.

53
00:03:19,733 --> 00:03:23,066
Can I have a quick look at what it is?

54
00:03:23,066 --> 00:03:27,399
And you see if we printed the first 250 characters.

55
00:03:27,900 --> 00:03:30,133
You have the first citizens speaking to

56
00:03:31,432 --> 00:03:33,731
everybody and everybody else is speaking

57
00:03:34,900 --> 00:03:38,233
to the first citizen.

58
00:03:38,699 --> 00:03:43,032
The cell computes the number of unique characters that we have in that

59
00:03:44,400 --> 00:03:47,699
in the text dataset, and we see that we have

60
00:03:48,265 --> 00:03:52,064
65 unique characters, right?

61
00:03:52,199 --> 00:03:56,631
These characters would be the tokens that the neural network will consume

62
00:03:57,300 --> 00:04:01,199
during training and will generating during this service.

63
00:04:02,066 --> 00:04:04,232
So the first step here

64
00:04:04,233 --> 00:04:06,899
now is to vectorize the text.

65
00:04:07,233 --> 00:04:08,932
What do we mean by that?

66
00:04:08,932 --> 00:04:13,132
It means that first of all,

67
00:04:13,133 --> 00:04:17,365
we will need to extract

68
00:04:17,365 --> 00:04:21,898
from the actual string sequence of characters, which we can do

69
00:04:22,165 --> 00:04:26,198
with TensorFlow by using tf.strings.unicode_split.

70
00:04:26,899 --> 00:04:29,399
So now, for example, texts

71
00:04:30,333 --> 00:04:34,400
here are transformed into a list

72
00:04:34,865 --> 00:04:37,531
of sequences of characters.

73
00:04:39,365 --> 00:04:41,932
A neural network cannot consume immediately.

74
00:04:41,932 --> 00:04:42,865
The characters.

75
00:04:42,865 --> 00:04:44,898
We need to transform that into numbers.

76
00:04:45,300 --> 00:04:49,399
So we need to simply map each of the characters to a given

77
00:04:50,365 --> 00:04:52,799
id. For that we have the

78
00:04:53,033 --> 00:04:56,233
tf.keras.layers.StringLookup

79
00:04:57,665 --> 00:05:00,798
to which you just need to pass to

80
00:05:02,100 --> 00:05:04,499
the list of your vocabulary.

81
00:05:04,500 --> 00:05:08,233
The 65 unique character that we have in our corpus

82
00:05:09,033 --> 00:05:14,400
and that we produce a layer that when passed the characters

83
00:05:14,766 --> 00:05:17,532
will produce corresponding ids.

84
00:05:18,333 --> 00:05:23,100
So within that, that layer you have a mapping that has been generated

85
00:05:23,565 --> 00:05:25,932
between the characters and the

86
00:05:27,899 --> 00:05:30,899
id. To get the inverse mapping,

87
00:05:30,932 --> 00:05:33,632
you use the same layer of string lookup

88
00:05:34,766 --> 00:05:37,733
with the exact same vocabulary that you retrieve

89
00:05:37,733 --> 00:05:40,300
from the first of the year by using get vocabulary.

90
00:05:41,165 --> 00:05:45,299
But you set that parameter to be true, invert, equal equal true,

91
00:05:45,600 --> 00:05:50,600
and that will compute the invert mapping, which is the mapping from

92
00:05:51,000 --> 00:05:54,200
id to chars

93
00:05:54,466 --> 00:05:54,699
Right.

94
00:05:54,699 --> 00:05:56,998
And indeed, if you pass to this mapping

95
00:05:58,266 --> 00:06:01,233
sequence of ID’s, the ID’s,

96
00:06:02,333 --> 00:06:04,865
it gives you back

97
00:06:04,865 --> 00:06:07,465
the corresponding characters.

98
00:06:07,632 --> 00:06:10,599
Using the mapping that start in the memory of this layer.

99
00:06:11,733 --> 00:06:14,233
So that's that.

100
00:06:15,165 --> 00:06:15,899
Okay.

101
00:06:15,899 --> 00:06:20,631
Now let's that's been the dataset that we will train our neural network with.

102
00:06:20,932 --> 00:06:23,464
For that we are using the

103
00:06:24,199 --> 00:06:27,032
tf.data.Dataset API,

104
00:06:28,033 --> 00:06:30,133
which has this nice method

105
00:06:30,266 --> 00:06:33,133
from tons of slices

106
00:06:33,600 --> 00:06:34,799
which will convert.

107
00:06:34,800 --> 00:06:39,865
That answer of instance represents or whole corpus of text of plays

108
00:06:40,365 --> 00:06:43,965
as id it will store that into it to have data data sets.

109
00:06:44,565 --> 00:06:49,164
So at this point, the elements of these datasets

110
00:06:49,766 --> 00:06:52,600
are just the individual characters.

111
00:06:52,600 --> 00:06:53,899
So that's not great for us.

112
00:06:53,899 --> 00:06:57,932
But we want to feed our neural network with our sequences

113
00:06:58,165 --> 00:07:01,499
of the same length but not just one character.

114
00:07:01,565 --> 00:07:03,432
We need to predict the next character. So

115
00:07:05,432 --> 00:07:06,764
but luckily the

116
00:07:06,766 --> 00:07:11,266
dataset API has this nice function batch that will do exactly that for us.

117
00:07:11,266 --> 00:07:14,399
So if we pass, if we invoke the batch

118
00:07:14,833 --> 00:07:17,700
method on our ID dataset,

119
00:07:18,300 --> 00:07:21,199
to which we pass a given sequence length,

120
00:07:21,600 --> 00:07:26,399
which we said to be 100 here, now the elements,

121
00:07:27,000 --> 00:07:30,700
the data points that are stored in our dataset

122
00:07:30,932 --> 00:07:34,499
are no longer characters, but the sequences of

123
00:07:35,899 --> 00:07:37,665
100 characters.

124
00:07:37,665 --> 00:07:40,564
So here you see an example.

125
00:07:40,565 --> 00:07:43,132
If we take just one element, they are no longer characters,

126
00:07:43,466 --> 00:07:47,733
but sequences of hundreds of their character IDs

127
00:07:47,800 --> 00:07:50,100
0 you want not characters, but character IDs.

128
00:07:52,365 --> 00:07:54,631
Okay, it's

129
00:07:54,899 --> 00:07:57,698
not completely we are not completely done here.

130
00:07:58,199 --> 00:08:00,832
We still need to create

131
00:08:02,233 --> 00:08:05,499
the input sequences that we were going to pass to the decoder

132
00:08:05,699 --> 00:08:08,832
and also the sequences that we want to predict.

133
00:08:08,932 --> 00:08:09,599
Right?

134
00:08:09,600 --> 00:08:13,300
And what are the sequences that are just the sequences of the next character

135
00:08:13,565 --> 00:08:15,565
in the input sequence?

136
00:08:15,565 --> 00:08:19,065
So for instance, here, if we have the sequence TensorFlow

137
00:08:20,865 --> 00:08:23,398
and the sequence TensorFlow at the beginning,

138
00:08:24,432 --> 00:08:28,565
then the input sequence we can do from

139
00:08:28,565 --> 00:08:32,364
it is tens-or-flow, We know the W

140
00:08:33,066 --> 00:08:36,232
and the target sequence that we want to predict

141
00:08:36,600 --> 00:08:39,066
is the same sequence, but just shifted by one

142
00:08:39,932 --> 00:08:42,932
on the right, so ensor-low and

143
00:08:42,932 --> 00:08:47,898
you see that E is the next character for Ring T

144
00:08:48,765 --> 00:08:52,699
and is the next there for E, etc.

145
00:08:53,100 --> 00:08:55,366
So basically this little function does exactly that.

146
00:08:55,700 --> 00:08:58,666
It takes an original sequence, creates

147
00:08:59,332 --> 00:09:03,599
an input sequence from that by just truncating that sequence

148
00:09:03,600 --> 00:09:08,100
0 removing the last character and that just the target sequence is created

149
00:09:09,566 --> 00:09:15,133
by started at starting add the first character.

150
00:09:15,899 --> 00:09:18,431
So how we do that, we just map

151
00:09:19,799 --> 00:09:23,331
the split input target function to our sequence dataset.

152
00:09:24,666 --> 00:09:25,400
Okay.

153
00:09:26,466 --> 00:09:27,599
And it's already does it.

154
00:09:27,600 --> 00:09:29,166
Now let's see how to build the model.

155
00:09:30,600 --> 00:09:34,399
First off, we set a number of variables

156
00:09:36,332 --> 00:09:40,599
the vocabulary size, the size of the vectors.

157
00:09:40,600 --> 00:09:46,066
We want to represent the characters will I think That would be 256

158
00:09:46,399 --> 00:09:49,732
and a number of neurons or recurrent layer we'd have.

159
00:09:51,765 --> 00:09:52,799
For the model itself.

160
00:09:52,799 --> 00:09:54,831
It's a relatively simple model.

161
00:09:55,666 --> 00:10:01,499
We create it by using the Keras subclass API.

162
00:10:01,500 --> 00:10:03,932
We create just a new class called MyModel

163
00:10:04,566 --> 00:10:07,099
and we subclass here

164
00:10:07,332 --> 00:10:08,731
from tf.keras.Model.

165
00:10:08,732 --> 00:10:11,699
When you do that you only have to

166
00:10:13,200 --> 00:10:17,233
override two functions, the constructor and the call function.

167
00:10:17,533 --> 00:10:20,865
So let's see what each of these function does.

168
00:10:20,865 --> 00:10:26,132
The first function takes essentially the hyper parameters of your model,

169
00:10:26,133 --> 00:10:29,133
the vocabulary size, the embedding dimension, the number of neuron

170
00:10:29,133 --> 00:10:33,299
that number of neurons for your recurrent layer,

171
00:10:33,832 --> 00:10:38,899
and it just constricts the layers you will need and store them

172
00:10:39,932 --> 00:10:45,864
as variables of the class.

173
00:10:45,865 --> 00:10:46,565
Okay.

174
00:10:47,399 --> 00:10:49,332
Now really

175
00:10:49,399 --> 00:10:51,931
how these layers are connected,

176
00:10:51,932 --> 00:10:55,464
all that is specified in the call function,

177
00:10:55,466 --> 00:10:57,700
the architecture of your network, if you will.

178
00:10:57,700 --> 00:11:00,799
If you want.

179
00:11:00,799 --> 00:11:02,499
Let's see where to the body does. Here.

180
00:11:02,500 --> 00:11:07,766
Take the input which are sequences of ids representing the characters.

181
00:11:08,533 --> 00:11:13,400
We have a first layer that we'll create for each of the

182
00:11:14,432 --> 00:11:17,331
inits a vector representing that.

183
00:11:17,332 --> 00:11:19,032
So that's the training layer.

184
00:11:19,033 --> 00:11:24,732
So as the training progresses, this vector is representing the characters.

185
00:11:24,732 --> 00:11:27,199
We'll start to be more and more meaningful.

186
00:11:27,666 --> 00:11:29,699
At least that's the idea.

187
00:11:29,700 --> 00:11:33,466
Then these static representations of the characters

188
00:11:33,865 --> 00:11:36,599
is passed to the recurrent layer that we'll somehow

189
00:11:37,865 --> 00:11:39,898
modify these for representation

190
00:11:39,899 --> 00:11:45,065
according to the context of what I've seen with what has been seen

191
00:11:45,700 --> 00:11:49,099
previously and generate a state of

192
00:11:49,100 --> 00:11:52,032
what is seen previously, that would be a reuse in the next step.

193
00:11:52,700 --> 00:11:55,732
Finally, we pass the output of the

194
00:11:57,066 --> 00:11:59,933
recurrent layer to a dense layer that will output

195
00:12:00,500 --> 00:12:03,600
as many numbers that we as we have in our vocabulary,

196
00:12:04,000 --> 00:12:06,932
which means one

197
00:12:06,932 --> 00:12:09,098
score for each

198
00:12:09,100 --> 00:12:12,765
of the possible 65 characters and the score

199
00:12:13,799 --> 00:12:17,098
represent the probability of the character

200
00:12:17,932 --> 00:12:20,265
being the next one.

201
00:12:20,265 --> 00:12:22,765
So that's all that the model does.

202
00:12:23,299 --> 00:12:29,398
Then we instantiated.

203
00:12:29,399 --> 00:12:33,132
Once we have done that, we can look at the structure of the model

204
00:12:33,133 --> 00:12:37,332
using model summary, and you see here you have the I'm building the year,

205
00:12:37,332 --> 00:12:41,532
the recurrently year and the dust layer that we just encoded

206
00:12:42,365 --> 00:12:44,631
implemented in our, in our model

207
00:12:47,365 --> 00:12:48,232
does that.

208
00:12:48,232 --> 00:12:49,865
So let's train the model.

209
00:12:49,865 --> 00:12:53,631
Before we train the model, we need a loss and that's the loss function

210
00:12:53,633 --> 00:12:58,232
that we compared the output of the model with the truth, right?

211
00:12:58,832 --> 00:13:03,332
Since that's essentially a classification problem with many classes

212
00:13:04,365 --> 00:13:06,599
and the classes being

213
00:13:06,600 --> 00:13:09,600
each of the possible characters to be the next,

214
00:13:10,832 --> 00:13:14,065
the loss would be the SparseCategoricalCrossentropy loss.

215
00:13:14,832 --> 00:13:18,565
And also because the neural network

216
00:13:19,000 --> 00:13:24,065
output, the logits are not directly the probability we configure this loss

217
00:13:24,066 --> 00:13:27,865
to be computed not from the probability scores,

218
00:13:27,865 --> 00:13:30,299
but from the logits scores.

219
00:13:31,299 --> 00:13:31,999
Okay,

220
00:13:32,332 --> 00:13:36,299
once we have the loss, we can combine our model,

221
00:13:36,299 --> 00:13:41,731
which means that basically we tied to it a loss and also an optimizer.

222
00:13:41,899 --> 00:13:44,632
And that will update the weights during training

223
00:13:44,732 --> 00:13:47,699
to decrease the loss as much as possible.

224
00:13:48,466 --> 00:13:49,766
Basically, it

225
00:13:50,633 --> 00:13:53,565
then here we have a little bit of a callback that we will use

226
00:13:54,500 --> 00:13:57,832
and that will save the weights during training,

227
00:13:57,832 --> 00:14:00,532
which is a useful

228
00:14:01,066 --> 00:14:01,865
item.

229
00:14:01,865 --> 00:14:04,565
And we are all set up now to start the training.

230
00:14:04,566 --> 00:14:09,700
So we do a model.fit on the data set.

231
00:14:09,700 --> 00:14:13,833
We choose a number of epochs we want to be trained on.

232
00:14:14,399 --> 00:14:18,599
An epoch is a full pass on the data set.

233
00:14:18,600 --> 00:14:22,800
So here we we have a look at ten, ten times

234
00:14:23,100 --> 00:14:26,999
the corpus of plays we have in our text vector

235
00:14:27,832 --> 00:14:30,598
and we give the callback to make sure

236
00:14:30,600 --> 00:14:35,032
that the weights are saved during the the training,

237
00:14:36,466 --> 00:14:37,066
that's it.

238
00:14:37,066 --> 00:14:39,765
So that's relatively simple. We train them my data.

239
00:14:39,765 --> 00:14:42,499
We have a train model now what do we do with it?

240
00:14:42,765 --> 00:14:45,632
And that's a bit of a complication in the

241
00:14:45,633 --> 00:14:46,232
encoder.

242
00:14:46,232 --> 00:14:49,998
Decoder architecture is that you cannot through the immediately use your model,

243
00:14:50,299 --> 00:14:55,332
you need to write a sort of a decoding function that's here

244
00:14:55,899 --> 00:14:58,632
that will decode the generated text

245
00:14:59,432 --> 00:15:02,098
a step at a time using the trained model.

246
00:15:03,533 --> 00:15:03,865
Okay.

247
00:15:03,865 --> 00:15:06,699
So here in this case, we chose to

248
00:15:07,799 --> 00:15:10,964
implement this decoding function

249
00:15:10,966 --> 00:15:12,500
as a Keras model.

250
00:15:12,500 --> 00:15:15,966
So we subclass from the tf.keras.Model.

251
00:15:15,966 --> 00:15:19,132
The main method

252
00:15:19,133 --> 00:15:21,733
in that model is to generate one step.

253
00:15:22,232 --> 00:15:25,232
It's a quick look to what it does,

254
00:15:25,633 --> 00:15:28,966
so it takes the inputs so the input can be

255
00:15:29,666 --> 00:15:33,733
to prompt the initial prompt initial the sequence of character you want to

256
00:15:34,899 --> 00:15:37,198
the encoder-decoder model to

257
00:15:37,200 --> 00:15:40,265
complete, to predict, to generate new new characters.

258
00:15:41,100 --> 00:15:43,300
So you bypass the input it

259
00:15:44,200 --> 00:15:47,265
transform that text into a sequence of character,

260
00:15:47,265 --> 00:15:50,999
and then the sequence of characters into a sequence of ids.

261
00:15:51,100 --> 00:15:53,100
Using the ids_from_chars.

262
00:15:53,100 --> 00:15:57,465
Here we have a setup previously, and then we call our model

263
00:15:57,600 --> 00:16:00,866
or encoder-decoder model that has been previously trained.

264
00:16:01,799 --> 00:16:02,699
And what does it do?

265
00:16:02,700 --> 00:16:05,865
It takes this input of ids and

266
00:16:07,166 --> 00:16:09,965
output the predicted logits.

267
00:16:09,966 --> 00:16:14,400
So this calls for the most probable token the most probable character in this case,

268
00:16:14,600 --> 00:16:18,100
0 along with the state that summarizes what has been seen previously.

269
00:16:20,432 --> 00:16:25,799
From the predicted logits, we can compute, we can select

270
00:16:26,600 --> 00:16:30,499
the most likely tokens or characters.

271
00:16:30,666 --> 00:16:34,199
But before doing that there is a little bit of a trick,

272
00:16:34,600 --> 00:16:38,965
which is that we divide the logits by a temperature, by a number.

273
00:16:39,533 --> 00:16:43,699
So basically if the temperature is one, nothing happens.

274
00:16:43,700 --> 00:16:49,365
But if the temperature is very high,

275
00:16:49,365 --> 00:16:52,699
what it will do, it will makes the scores

276
00:16:53,232 --> 00:16:56,799
associated to each of the token to be predictive.

277
00:16:56,799 --> 00:17:00,632
Next will be relatively similar, close to zero.

278
00:17:02,299 --> 00:17:04,799
This means that actually

279
00:17:04,799 --> 00:17:09,164
this token would be more and more likely to be chosen, right?

280
00:17:09,299 --> 00:17:14,531
So there would be more variety, more a more stuff can be predicted

281
00:17:14,799 --> 00:17:16,365
if the temperature is higher.

282
00:17:16,365 --> 00:17:20,765
So it's a bit more creative If you have a two high temperature,

283
00:17:20,766 --> 00:17:25,299
of course, the neural network would just predict the gibberish.

284
00:17:26,099 --> 00:17:28,164
Okay.

285
00:17:28,165 --> 00:17:31,198
And if you have a true temperature, the highest probability

286
00:17:31,200 --> 00:17:34,200
score will be just multiply by a very large number

287
00:17:34,665 --> 00:17:39,565
because it's divided by a small number, it's a number between zero and one,

288
00:17:40,099 --> 00:17:42,898
which means that the highest score will be

289
00:17:44,266 --> 00:17:46,099
become much, much

290
00:17:46,099 --> 00:17:50,464
bigger than the other scores, giving a much higher chance

291
00:17:50,465 --> 00:17:56,264
to be selected, which gives you more of the deterministic behavior.

292
00:17:56,266 --> 00:17:57,399
Okay, that's the temperature.

293
00:17:57,400 --> 00:18:00,100
That's an important parameter, as in this type of architecture.

294
00:18:01,532 --> 00:18:01,832
Okay.

295
00:18:01,833 --> 00:18:03,232
And that's what it does.

296
00:18:03,799 --> 00:18:04,399
Okay.

297
00:18:04,400 --> 00:18:09,099
So now we have the predicted logits we use tf.random.categorical

298
00:18:09,099 --> 00:18:12,365
to just sample from these probability scores

299
00:18:13,465 --> 00:18:18,131
the most likely idea is to be next.

300
00:18:18,865 --> 00:18:22,764
We transform that back to a character and that's what we return.

301
00:18:23,700 --> 00:18:26,700
Okay, So that's essentially what the decoding function does

302
00:18:27,000 --> 00:18:30,133
and most decoding function at the very same structure.

303
00:18:30,133 --> 00:18:34,065
There is also this temperature trick that you can see as a

304
00:18:34,900 --> 00:18:37,199
as a parameter in the case of large

305
00:18:37,200 --> 00:18:38,800
language models.

306
00:18:41,299 --> 00:18:44,432
Okay, so let's use our decoding function.

307
00:18:44,432 --> 00:18:46,598
So typically you use that in the loop.

308
00:18:46,599 --> 00:18:51,932
So here we are going to predict 1000 characters by repeatedly

309
00:18:51,932 --> 00:18:56,598
making a call to the decoding function generated one step,

310
00:18:57,066 --> 00:18:59,066
to which you feed

311
00:19:00,432 --> 00:19:02,532
what has been predicted

312
00:19:02,532 --> 00:19:07,399
before, along with the state summarizing what happened before,

313
00:19:07,799 --> 00:19:11,231
and it predict the next character along with a new state.

314
00:19:11,700 --> 00:19:13,700
And we start the process.

315
00:19:13,700 --> 00:19:15,599
We do sort of a prompt here. That's

316
00:19:16,700 --> 00:19:17,500
Romeo.

317
00:19:17,500 --> 00:19:19,866
What are you going to say? And then the

318
00:19:21,000 --> 00:19:24,599
there are let's let's see what the neuron that generates, right?

319
00:19:24,599 --> 00:19:28,565
Says no good corona at least take your feetle

320
00:19:28,833 --> 00:19:31,666
and if I seem to my love you...

321
00:19:32,232 --> 00:19:36,299
so you see it's not it doesn't make a lot of sense here.

322
00:19:36,566 --> 00:19:39,100
0 Remember I've trained it only a few minutes

323
00:19:41,200 --> 00:19:43,966
on the work bench,

324
00:19:44,099 --> 00:19:50,632
AI work bench in Vertex AI Workbench, which are great by the way, but here

325
00:19:50,700 --> 00:19:54,833
that's a small instance which just one GPU So it was a very small training.

326
00:19:55,133 --> 00:19:57,899
The model is written

327
00:19:57,900 --> 00:20:03,265
in a few lines, but yet you still see that it can really pick up

328
00:20:03,266 --> 00:20:07,900
a lot of things in the structure of the of the input data.

329
00:20:07,900 --> 00:20:11,065
It detects patterns that you have characters.

330
00:20:11,066 --> 00:20:13,599
So Romeo, that was our input,

331
00:20:15,266 --> 00:20:15,965
but then

332
00:20:15,965 --> 00:20:19,264
Leontes was generated by the network and then what

333
00:20:19,599 --> 00:20:21,832
Leontes says. So

334
00:20:23,532 --> 00:20:24,299
okay,

335
00:20:25,200 --> 00:20:26,232
that's it.

336
00:20:26,865 --> 00:20:30,199
If you like this presentation, you'll find more on our ASL

337
00:20:30,200 --> 00:20:34,099
GitHub repository with 90 plus machine learning and notebooks.

338
00:20:34,500 --> 00:20:35,066
Don't forget it.

339
00:20:35,066 --> 00:20:37,199
If you find it useful, please star our repo.

340
00:20:37,665 --> 00:20:40,331
Thanks for your time.
