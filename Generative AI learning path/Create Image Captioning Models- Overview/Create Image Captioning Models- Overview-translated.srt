1
00:00:01,199 --> 00:00:02,165
大家好。

2
00:00:02,165 --> 00:00:09,099
我是谷歌高级解决方案实验室的机器学习工程师Takumi。

4
00:00:09,099 --> 00:00:14,999
目前很多人都在谈论生成式AI及其新进展，

6
00:00:15,000 --> 00:00:26,666
正如你们中的一些人可能知道的那样，谷歌和谷歌云也发布了很多与生成式AI相关的新产品和功能。

9
00:00:26,666 --> 00:00:37,200
但在这个视频系列中，我们的目标不是创建我们的生成式AI的状态，也不是介绍谷歌云的新产品。

12
00:00:37,200 --> 00:00:42,299
相反，我们将解释这背后有哪些技术。

13
00:00:43,665 --> 00:00:50,765
特别是在这个视频中，我将讨论如何实际创建一个非常简单的生成模型，

15
00:00:51,299 --> 00:01:00,198
使用诸如编码器解码器注意力机制和一点Transformer的技术来创建图像说明文字模型。

17
00:01:01,200 --> 00:01:10,664
如果你对这些概念不是很熟悉，我建议在此之前查看其他关于它们的视频。

20
00:01:10,665 --> 00:01:10,965
好的，

21
00:01:10,965 --> 00:01:15,498
那么如果你准备好了，让我们来谈谈图像说明文字任务和数据集，

22
00:01:15,500 --> 00:01:20,533
首先，我们将使用这种类型的数据集。

23
00:01:21,099 --> 00:01:33,499
如你所见，这里有很多图像和文本数据配对，我们的目标是构建和训练一个模型，可以根据图像生成这种类型的说明文字。

26
00:01:34,132 --> 00:01:38,665
我们将通过构建这种模型来实现这一目标。

27
00:01:38,665 --> 00:01:50,964
如你所见，这是一种编码器解码器模型，但在这种情况下，编码器和解码器处理不同类型的数据，即图像和文本。

29
00:01:50,965 --> 00:02:00,731
因此，我们首先将图像传递给编码器，它从图像中提取信息并创建一些特征向量。

31
00:02:00,733 --> 00:02:08,632
然后，将向量传递给解码器，逐个生成字来构建说明文字。

33
00:02:09,599 --> 00:02:11,965
这个编码器部分很简单。

34
00:02:11,966 --> 00:02:20,099
你可以使用任何类型的图像骨干网络，如ResNet、EfficientNet或Vision Transformer。

36
00:02:20,099 --> 00:02:24,599
我们想要做的是使用这些骨干网络来提取特征。

37
00:02:25,566 --> 00:02:32,998
因此，代码也非常简单，在代码方面，我们将在下一个视频中查看整个笔记本示例。

39
00:02:33,466 --> 00:02:39,299
所以在这里，让我们只关注一些重要的行。

41
00:02:39,300 --> 00:02:47,231
如你所见，我们在这里使用了经典的InceptionResNetV2，来自Carousel应用程序。

43
00:02:47,233 --> 00:02:52,500
但是，这可以是任何图像骨干网络。

44
00:02:52,500 --> 00:03:00,599
接下来的部分有点复杂，让我们仔细看看。

46
00:03:00,599 --> 00:03:03,299
这是解码器的整个架构。

47
00:03:04,400 --> 00:03:17,899
它逐个获取，并将编码器输出中的词和图像信息进行组合，尝试预测下一个词。

51
00:03:17,900 --> 00:03:21,465
解码器本身是一种迭代操作。

52
00:03:21,900 --> 00:03:28,732
通过反复调用或回归，最终可以生成六条说明文字。

54
00:03:29,900 --> 00:03:39,098
有很多这种解码器设计的变体，但在这里我们构建了类似于Transformer的架构，尽管我们仍然使用RNN GRU。

57
00:03:40,765 --> 00:03:45,298
让我们放大每个组件。

58
00:03:45,300 --> 00:03:56,099
第一个嵌入层创建了单词嵌入，这在其他视频中已经讨论过了，我们将其传递给GUR层。

61
00:03:56,099 --> 00:03:57,965
如果你忘记了你猜的是什么。

62
00:03:57,966 --> 00:04:08,731
这是一种循环神经网络的变体，或者你可以称之为RNN，RNN接收输入并更新其内部状态并生成输出。

66
00:04:09,599 --> 00:04:14,665
所以通过传递序列数据，如文本数据，它可以保持序列依赖性。

68
00:04:14,665 --> 00:04:24,898
这些来自以前的输入，比如之前的词，那个GRU的输出进入了注意力层，它混合了文本和图像的信息。

71
00:04:26,100 --> 00:04:31,033
在TensorFlow Keras中，我们可以像使用其他层一样使用预定义的层。

73
00:04:32,165 --> 00:04:38,100
有多种实现，但是我们在这里简单地使用了tf.keras.layers.Attention。

75
00:04:39,300 --> 00:04:50,532
但是，如果你想使用更像Transformer的架构，你可以了解tf.keras.layers.MultiHeadAttention，它使用多个注意力头。

78
00:04:51,600 --> 00:04:54,933
你可以简单地切换并以几乎相同的方式使用它。

79
00:04:56,266 --> 00:05:03,765
在我们的注意力层中，你可能已经在关于注意力机制的另一个视频中看到了这个。

81
00:05:04,800 --> 00:05:16,833
但这里独特的是，它通过处理文本数据关注图像特征，这样，它可以通过混合两种信息来计算注意力分数。

85
00:05:18,600 --> 00:05:26,833
回到代码，你可以看到这个注意力层接受两个输入，gru_ouput和encoder_output。

88
00:05:28,565 --> 00:05:36,599
在内部，GRU的输出用作注意力查询和键，编码器输出用作值。

90
00:05:38,165 --> 00:05:43,364
最后的组件是Add层和LayerNormalization层。

92
00:05:44,932 --> 00:05:48,500
Add层只是添加两个相同的移位向量。

94
00:05:50,100 --> 00:06:04,765
如你所见，GRU的输出被传递给我们讨论过的注意力层以及直接传递给这个Add层，这两个流最终在这个Add层中合并。

98
00:06:05,000 --> 00:06:14,200
这种架构被称为跳跃连接（Skip Connection），自Resonant以来一直非常受欢迎的深度神经网络设计模式。

101
00:06:15,333 --> 00:06:20,600
因此，它也被称为残差连接（Residual Connection）。

102
00:06:20,600 --> 00:06:32,166
这种跳跃连接非常有用，尤其是当你想设计一个非常深的神经网络时，它还用于Transformer。

105
00:06:32,500 --> 00:06:42,499
有了这个，我们现在可以构建一个完整的解码器，因此我们准备使用说明文字数据集训练编码器解码器图像说明文字模型。

108
00:06:43,399 --> 00:06:47,932
我们将在下一个视频中看到它是如何工作的。

109
00:06:47,932 --> 00:07:02,766
但在继续下一个之前，我想多解释一下关于推理阶段的内容，在这个阶段我们实际上可以为图像生成说明文字，因为这个过程可能看起来有点复杂。

114
00:07:02,766 --> 00:07:08,499
在这里，我们可以看到三个步骤，我们将在一个自定义推理函数中实现这些步骤，

116
00:07:10,000 --> 00:07:17,865
第一，生成初始状态并创建一个起始Token，在训练阶段。

119
00:07:17,865 --> 00:07:23,698
TensorFlow Chorus可以自动处理每个序列的状态，

121
00:07:23,699 --> 00:07:31,500
但在这个推理阶段，由于我们设计了自己的自定义函数，我们需要明确地编写处理它的逻辑。

123
00:07:33,266 --> 00:07:40,500
因此，在每个说明文字的开始，我们明确地用某个值初始化GRU状态，

126
00:07:41,800 --> 00:07:46,565
同时记住我们的解码器是一个自回归函数。

128
00:07:47,466 --> 00:07:56,632
但是由于我们在推理开始时还没有得到任何预测，所以我们从一个特殊的开始Token开始。

130
00:07:56,632 --> 00:08:00,232
这意味着一个句子的开始，

131
00:08:00,733 --> 00:08:06,665
第二，将图像输入到编码器，并按照我们讨论的进行特征向量的提取，

133
00:08:07,233 --> 00:08:27,064
第三，将向量传递给这个时间解码器，在for循环中生成一个说明文字的单词，直到它返回一个结束Token，或者达到最大说明文字长度，这只是一个指定一些数字的超参数，比如264。

138
00:08:27,699 --> 00:08:41,499
在这个完整的循环中，我们定义了所有说明文字通过反复调用解码器生成的过程，结束Token也是一个特殊的Token，意味着序列的结束。

142
00:08:42,265 --> 00:08:56,133
因此，当我们的解码器生成这个Token时，我们可以完成这个完整的循环，或者当说明文字的长度达到某个最大长度时，可以跳出循环。

146
00:08:56,133 --> 00:08:58,200
让我们逐一看一下代码。

147
00:08:59,832 --> 00:09:06,065
在第一步中，我们初始化了两个东西：GRU状态和起始Token。

149
00:09:06,066 --> 00:09:16,299
在这种情况下，GRU的状态只是用零向量初始化，而起始Token作为解码器的第一个输入词。

152
00:09:18,066 --> 00:09:30,600
关于这里使用的word_to_index函数，我将在下一个视频中解释，但它基本上只是将词汇分词为我们的Token，这是标准的文本预处理技术。

155
00:09:31,865 --> 00:09:37,866
在下一步中，我们预处理输入的图像，并将其传递给我们训练的编码器。

158
00:09:39,399 --> 00:09:53,166
在图像预处理方面，它首先读取并解码JPEG，然后将其从任意分辨率调整为特定分辨率，

162
00:09:53,166 --> 00:10:00,432
接着在第三行将比例从0到255更改为0到1，

164
00:10:01,932 --> 00:10:05,331
最后是解码器循环。

165
00:10:05,332 --> 00:10:06,364
这有点复杂。

166
00:10:06,365 --> 00:10:10,765
因此，我将在下一个视频中更详细地解释实际代码。

167
00:10:11,533 --> 00:10:17,698
但这里的主要内容是通过传递三个事物来调用解码器。

169
00:10:17,700 --> 00:10:24,932
解码输入意味着解码器输入，它应该有一个在上一次迭代中预测的词Token。

171
00:10:26,133 --> 00:10:37,266
正如我们所说，如果这是第一次迭代，那么这将是开始Token，GRU状态是我们讨论过的当前状态。

174
00:10:37,932 --> 00:10:44,599
请注意记录器的这个输出，更新了GRU的状态。

176
00:10:45,365 --> 00:10:47,732
最后但同样重要的是特征，

177
00:10:48,500 --> 00:10:52,565
这是我们用编码器提取的图像特征。

178
00:10:52,566 --> 00:10:58,799
通过传递它们，我们可以得到实际的下一个变量预测。

179
00:10:58,799 --> 00:11:03,031
这是一个非常简单的从图像生成文本的模型，

180
00:11:03,666 --> 00:11:10,132
但这种迭代在很大的语言生成模型中也非常相似，

182
00:11:10,432 --> 00:11:17,433
就像Google Board， 他们基本上也是以这种方式预测下一个词，以自回归的方式，

184
00:11:17,865 --> 00:11:25,664
一个接一个地基于一些信息和学习到的知识，这些知识都嵌入在大量的参数中。

186
00:11:27,066 --> 00:11:30,900
在下一个视频中，我将带你浏览整个Notebook，

187
00:11:31,566 --> 00:11:37,865
然后我们将检查这个模型可以生成哪些说明文字。

189
00:11:37,865 --> 00:11:42,732
非常感谢你的观看，我们下一个视频见。
