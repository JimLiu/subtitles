{
  "chunks": [
    {
      "items": [
        {
          "id": "1",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 1,
            "milliseconds": 199
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 2,
            "milliseconds": 165
          },
          "text": "Hi everyone."
        },
        {
          "id": "2",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 2,
            "milliseconds": 165
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 6,
            "milliseconds": 698
          },
          "text": "I am Takumi, machine learning engineer at Google Advanced Solutions."
        },
        {
          "id": "3",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 6,
            "milliseconds": 700
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 9,
            "milliseconds": 99
          },
          "text": "Lab."
        },
        {
          "id": "4",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 9,
            "milliseconds": 99
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 12,
            "milliseconds": 299
          },
          "text": "Currently a lot of people are talking about generative AI"
        },
        {
          "id": "5",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 12,
            "milliseconds": 333
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 14,
            "milliseconds": 999
          },
          "text": "and its new advancement,"
        },
        {
          "id": "6",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 15,
            "milliseconds": 0
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 18,
            "milliseconds": 932
          },
          "text": "and as some of you may know, Google and Google Cloud"
        },
        {
          "id": "7",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 19,
            "milliseconds": 65
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 23,
            "milliseconds": 199
          },
          "text": "also released so many generative AI related new products"
        },
        {
          "id": "8",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 23,
            "milliseconds": 300
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 26,
            "milliseconds": 666
          },
          "text": "and features."
        },
        {
          "id": "9",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 26,
            "milliseconds": 666
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 30,
            "milliseconds": 733
          },
          "text": "But in this video series, our goal is not to create a state"
        },
        {
          "id": "10",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 30,
            "milliseconds": 733
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 34,
            "milliseconds": 200
          },
          "text": "of our generative AIS, nor to introduce"
        },
        {
          "id": "11",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 34,
            "milliseconds": 200
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 37,
            "milliseconds": 200
          },
          "text": "Google Cloud new products."
        }
      ],
      "source": [
        "Hi everyone.",
        "I am Takumi, machine learning engineer at Google Advanced Solutions Lab.",
        "",
        "Currently a lot of people are talking about generative AI",
        "and its new advancement,",
        "and as some of you may know, Google and Google Cloud",
        "also released so many generative AI related new products",
        "and features.",
        "But in this video series, our goal is not to create a state",
        "of our generative AIS, nor to introduce",
        "Google Cloud new products."
      ],
      "result": [
        "大家好。",
        "我是谷歌高级解决方案实验室的机器学习工程师Takumi。",
        "",
        "目前很多人都在谈论生成式AI及其新进展，",
        "",
        "正如你们中的一些人可能知道的那样，谷歌和谷歌云也发布了很多与生成式AI相关的新产品和功能。",
        "",
        "",
        "但在这个视频系列中，我们的目标不是创建我们的生成式AI的状态，也不是介绍谷歌云的新产品。",
        "",
        ""
      ],
      "status": "success",
      "errors": [],
      "mismatched": false,
      "output": {
        "paragraph": "大家好。我是谷歌高级解决方案的机器学习工程师Takumi。爱。目前很多人都在谈论生成式AI及其新进展，正如你们中的一些人可能知道的那样，谷歌和谷歌云也发布了很多与生成式AI相关的新产品和功能。但在这个视频系列中，我们的目标不是创建我们的生成式AI的状态，也不是介绍谷歌云的新产品。",
        "sentences": [
          {
            "translated": "大家好。",
            "indexes": [
              0
            ]
          },
          {
            "translated": "我是谷歌高级解决方案的机器学习工程师Takumi。",
            "indexes": [
              1
            ]
          },
          {
            "translated": "爱。",
            "indexes": [
              2
            ]
          },
          {
            "translated": "目前很多人都在谈论生成式AI及其新进展，",
            "indexes": [
              3,
              4
            ]
          },
          {
            "translated": "正如你们中的一些人可能知道的那样，谷歌和谷歌云也发布了很多与生成式AI相关的新产品和功能。",
            "indexes": [
              5,
              6,
              7
            ]
          },
          {
            "translated": "但在这个视频系列中，我们的目标不是创建我们的生成式AI的状态，也不是介绍谷歌云的新产品。",
            "indexes": [
              8,
              9,
              10
            ]
          }
        ]
      }
    },
    {
      "items": [
        {
          "id": "12",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 37,
            "milliseconds": 200
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 42,
            "milliseconds": 299
          },
          "text": "Instead, we will explain what kind of technology has walking behind them."
        },
        {
          "id": "13",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 43,
            "milliseconds": 665
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 46,
            "milliseconds": 964
          },
          "text": "And especially in this video, I'm going to talk about"
        },
        {
          "id": "14",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 47,
            "milliseconds": 432
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 50,
            "milliseconds": 765
          },
          "text": "how to actually create a very simple generative model,"
        },
        {
          "id": "15",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 51,
            "milliseconds": 299
          },
          "endTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 55,
            "milliseconds": 364
          },
          "text": "image captioning model by using a technologies"
        },
        {
          "id": "16",
          "startTime": {
            "hours": 0,
            "minutes": 0,
            "seconds": 55,
            "milliseconds": 399
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 0,
            "milliseconds": 198
          },
          "text": "like encoder decoder attention mechanism and a bit transformer."
        },
        {
          "id": "17",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 1,
            "milliseconds": 200
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 3,
            "milliseconds": 666
          },
          "text": "If you're not very familiar with these concepts,"
        },
        {
          "id": "18",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 4,
            "milliseconds": 233
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 7,
            "milliseconds": 99
          },
          "text": "I recommend checking other videos, talking about them"
        },
        {
          "id": "19",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 7,
            "milliseconds": 99
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 10,
            "milliseconds": 664
          },
          "text": "before this."
        },
        {
          "id": "20",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 10,
            "milliseconds": 665
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 10,
            "milliseconds": 965
          },
          "text": "Okay."
        },
        {
          "id": "21",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 10,
            "milliseconds": 965
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 15,
            "milliseconds": 498
          },
          "text": "So if you're ready, let's talk about image captioning tasks and data"
        },
        {
          "id": "22",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 15,
            "milliseconds": 500
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 20,
            "milliseconds": 533
          },
          "text": "set out first, we're going to use this kind of dataset."
        }
      ],
      "source": [
        "Instead, we will explain what kind of technology has walking behind them.",
        "And especially in this video, I'm going to talk about",
        "how to actually create a very simple generative model,",
        "image captioning model by using a technologies",
        "like encoder decoder attention mechanism and a bit transformer.",
        "If you're not very familiar with these concepts,",
        "I recommend checking other videos, talking about them",
        "before this.",
        "Okay.",
        "So if you're ready, let's talk about image captioning tasks and data",
        "set out first, we're going to use this kind of dataset."
      ],
      "result": [
        "相反，我们将解释这背后有哪些技术。",
        "特别是在这个视频中，我将讨论如何实际创建一个非常简单的生成模型，",
        "",
        "使用诸如编码器解码器注意力机制和一点Transformer的技术来创建图像说明文字模型。",
        "",
        "如果你对这些概念不是很熟悉，我建议在此之前查看其他关于它们的视频。",
        "",
        "",
        "好的，",
        "那么如果你准备好了，让我们来谈谈图像说明文字任务和数据集，",
        "首先，我们将使用这种类型的数据集。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false,
      "output": {
        "paragraph": "相反，我们将解释这背后有哪些技术。特别是在这个视频中，我将讨论如何实际创建一个非常简单的生成模型，使用诸如编码器解码器注意力机制和一点Transformer的技术来创建图像说明文字模型。如果你对这些概念不是很熟悉，我建议在此之前查看其他关于它们的视频。好的，那么如果你准备好了，让我们来谈谈图像说明文字任务和数据集，首先，我们将使用这种类型的数据集。",
        "sentences": [
          {
            "translated": "相反，我们将解释这背后有哪些技术。",
            "indexes": [
              0
            ]
          },
          {
            "translated": "特别是在这个视频中，我将讨论如何实际创建一个非常简单的生成模型，",
            "indexes": [
              1
            ]
          },
          {
            "translated": "使用诸如编码器解码器注意力机制和一点Transformer的技术来创建图像说明文字模型。",
            "indexes": [
              2,
              3
            ]
          },
          {
            "translated": "如果你对这些概念不是很熟悉，我建议在此之前查看其他关于它们的视频。",
            "indexes": [
              4,
              5,
              6
            ]
          },
          {
            "translated": "好的，",
            "indexes": [
              7
            ]
          },
          {
            "translated": "那么如果你准备好了，让我们来谈谈图像说明文字任务和数据集，",
            "indexes": [
              8,
              9
            ]
          },
          {
            "translated": "首先，我们将使用这种类型的数据集。",
            "indexes": [
              10
            ]
          }
        ]
      }
    },
    {
      "items": [
        {
          "id": "23",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 21,
            "milliseconds": 99
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 24,
            "milliseconds": 898
          },
          "text": "As you can see, there are a lot of pairs of images and text data"
        },
        {
          "id": "24",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 26,
            "milliseconds": 200
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 29,
            "milliseconds": 99
          },
          "text": "and our goal is to build and train a model"
        },
        {
          "id": "25",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 29,
            "milliseconds": 400
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 33,
            "milliseconds": 499
          },
          "text": "that can generate these kind of takes captions based on images,"
        },
        {
          "id": "26",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 34,
            "milliseconds": 132
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 38,
            "milliseconds": 665
          },
          "text": "and we'll make it happen by building this kind of model."
        },
        {
          "id": "27",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 38,
            "milliseconds": 665
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 44,
            "milliseconds": 599
          },
          "text": "As you can see, it is kind of encoder decoder model, but in this case, encode"
        },
        {
          "id": "28",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 44,
            "milliseconds": 599
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 50,
            "milliseconds": 964
          },
          "text": "and decoder handle different modality of data, which is image and text."
        },
        {
          "id": "29",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 50,
            "milliseconds": 965
          },
          "endTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 56,
            "milliseconds": 298
          },
          "text": "So we pass the images to encoder at first and it extract information"
        },
        {
          "id": "30",
          "startTime": {
            "hours": 0,
            "minutes": 1,
            "seconds": 56,
            "milliseconds": 299
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 0,
            "milliseconds": 731
          },
          "text": "from the images and create some feature vectors."
        },
        {
          "id": "31",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 0,
            "milliseconds": 733
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 3,
            "milliseconds": 966
          },
          "text": "And then the vectors are passed to the decoder"
        },
        {
          "id": "32",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 4,
            "milliseconds": 533
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 8,
            "milliseconds": 632
          },
          "text": "which actually build the captions by generating was one by one."
        }
      ],
      "source": [
        "As you can see, there are a lot of pairs of images and text data",
        "and our goal is to build and train a model",
        "that can generate these kind of takes captions based on images,",
        "and we'll make it happen by building this kind of model.",
        "As you can see, it is kind of encoder decoder model, but in this case, encode",
        "and decoder handle different modality of data, which is image and text.",
        "So we pass the images to encoder at first and it extract information",
        "from the images and create some feature vectors.",
        "And then the vectors are passed to the decoder",
        "which actually build the captions by generating was one by one."
      ],
      "result": [
        "如你所见，这里有很多图像和文本数据配对，我们的目标是构建和训练一个模型，可以根据图像生成这种类型的说明文字。",
        "",
        "",
        "我们将通过构建这种模型来实现这一目标。",
        "如你所见，这是一种编码器解码器模型，但在这种情况下，编码器和解码器处理不同类型的数据，即图像和文本。",
        "",
        "因此，我们首先将图像传递给编码器，它从图像中提取信息并创建一些特征向量。",
        "",
        "然后，将向量传递给解码器，逐个生成字来构建说明文字。",
        ""
      ],
      "status": "success",
      "errors": [],
      "mismatched": false,
      "output": {
        "paragraph": "如你所见，这里有很多图像和文本数据配对，我们的目标是构建和训练一个模型，可以根据图像生成这种类型的说明文字。我们将通过构建这种模型来实现这一目标。如你所见，这是一种编码器解码器模型，但在这种情况下，编码器和解码器处理不同类型的数据，即图像和文本。因此，我们首先将图像传递给编码器，它从图像中提取信息并创建一些特征向量。然后，将向量传递给解码器，逐个生成字来构建说明文字。",
        "sentences": [
          {
            "translated": "如你所见，这里有很多图像和文本数据配对，我们的目标是构建和训练一个模型，可以根据图像生成这种类型的说明文字。",
            "indexes": [
              0,
              1,
              2
            ]
          },
          {
            "translated": "我们将通过构建这种模型来实现这一目标。",
            "indexes": [
              3
            ]
          },
          {
            "translated": "如你所见，这是一种编码器解码器模型，但在这种情况下，编码器和解码器处理不同类型的数据，即图像和文本。",
            "indexes": [
              4,
              5
            ]
          },
          {
            "translated": "因此，我们首先将图像传递给编码器，它从图像中提取信息并创建一些特征向量。",
            "indexes": [
              6,
              7
            ]
          },
          {
            "translated": "然后，将向量传递给解码器，逐个生成字来构建说明文字。",
            "indexes": [
              8,
              9
            ]
          }
        ]
      }
    },
    {
      "items": [
        {
          "id": "33",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 9,
            "milliseconds": 599
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 11,
            "milliseconds": 965
          },
          "text": "So this encoder part is easy."
        },
        {
          "id": "34",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 11,
            "milliseconds": 966
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 16,
            "milliseconds": 65
          },
          "text": "You can use any kinds of image backbone like resonant"
        },
        {
          "id": "35",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 16,
            "milliseconds": 233
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 20,
            "milliseconds": 99
          },
          "text": "efficient net or vision transformer."
        },
        {
          "id": "36",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 20,
            "milliseconds": 99
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 24,
            "milliseconds": 599
          },
          "text": "What we want to do here is to extract features by using that kind of backbones."
        },
        {
          "id": "37",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 25,
            "milliseconds": 566
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 29,
            "milliseconds": 699
          },
          "text": "So code is very simple too, in terms of the code,"
        },
        {
          "id": "38",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 29,
            "milliseconds": 699
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 32,
            "milliseconds": 998
          },
          "text": "we're going to see the entire notebook example in the next video."
        },
        {
          "id": "39",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 33,
            "milliseconds": 466
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 36,
            "milliseconds": 199
          },
          "text": "So here, let's just focus on some important"
        },
        {
          "id": "40",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 36,
            "milliseconds": 199
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 39,
            "milliseconds": 299
          },
          "text": "lines."
        },
        {
          "id": "41",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 39,
            "milliseconds": 300
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 43,
            "milliseconds": 399
          },
          "text": "As you can see, we are using classical inception version of V2 here"
        },
        {
          "id": "42",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 44,
            "milliseconds": 332
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 47,
            "milliseconds": 231
          },
          "text": "from Carousel Applications."
        }
      ],
      "source": [
        "So this encoder part is easy.",
        "You can use any kinds of image backbone like resonant",
        "efficient net or vision transformer.",
        "What we want to do here is to extract features by using that kind of backbones.",
        "So code is very simple too, in terms of the code,",
        "we're going to see the entire notebook example in the next video.",
        "So here, let's just focus on some important",
        "lines.",
        "As you can see, we are using classical inception version of V2 here",
        "from Carousel Applications."
      ],
      "result": [
        "这个编码器部分很简单。",
        "你可以使用任何类型的图像骨干网络，如ResNet、EfficientNet或Vision Transformer。",
        "",
        "我们想要做的是使用这些骨干网络来提取特征。",
        "因此，代码也非常简单，在代码方面，我们将在下一个视频中查看整个笔记本示例。",
        "",
        "所以在这里，让我们只关注一些重要的行。",
        "",
        "如你所见，我们在这里使用了经典的InceptionResNetV2，来自Carousel应用程序。",
        ""
      ],
      "status": "success",
      "errors": [],
      "mismatched": false,
      "output": {
        "paragraph": "这个编码器部分很简单。你可以使用任何类型的图像骨干，如resonant、efficient net或vision transformer。我们想要做的是使用这种骨干来提取特征。因此，代码也非常简单，在代码方面，我们将在下一个视频中查看整个笔记本示例。所以在这里，让我们只关注一些重要的行。如你所见，我们在这里使用了经典的inception V2版本，来自Carousel应用程序。",
        "sentences": [
          {
            "translated": "这个编码器部分很简单。",
            "indexes": [
              0
            ]
          },
          {
            "translated": "你可以使用任何类型的图像骨干，如resonant、efficient net或vision transformer。",
            "indexes": [
              1
            ]
          },
          {
            "translated": "我们想要做的是使用这种骨干来提取特征。",
            "indexes": [
              2,
              3
            ]
          },
          {
            "translated": "因此，代码也非常简单，在代码方面，我们将在下一个视频中查看整个笔记本示例。",
            "indexes": [
              4,
              5
            ]
          },
          {
            "translated": "所以在这里，让我们只关注一些重要的行。",
            "indexes": [
              6,
              7
            ]
          },
          {
            "translated": "如你所见，我们在这里使用了经典的inception V2版本，来自Carousel应用程序。",
            "indexes": [
              8,
              9
            ]
          }
        ]
      }
    },
    {
      "items": [
        {
          "id": "43",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 47,
            "milliseconds": 233
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 52,
            "milliseconds": 500
          },
          "text": "But again, this can be any image backbones."
        },
        {
          "id": "44",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 52,
            "milliseconds": 500
          },
          "endTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 55,
            "milliseconds": 266
          },
          "text": "So the next part that the quarter is a bit complex."
        },
        {
          "id": "45",
          "startTime": {
            "hours": 0,
            "minutes": 2,
            "seconds": 56,
            "milliseconds": 165
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 0,
            "milliseconds": 599
          },
          "text": "So let's take a look very carefully."
        },
        {
          "id": "46",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 0,
            "milliseconds": 599
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 3,
            "milliseconds": 299
          },
          "text": "So this is the entire architecture of the decoder."
        },
        {
          "id": "47",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 4,
            "milliseconds": 400
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 6,
            "milliseconds": 699
          },
          "text": "It it gets was one by one"
        },
        {
          "id": "48",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 6,
            "milliseconds": 699
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 10,
            "milliseconds": 165
          },
          "text": "and makes the information of wires and images"
        },
        {
          "id": "49",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 10,
            "milliseconds": 199
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 12,
            "milliseconds": 764
          },
          "text": "which is coming from the encoder output"
        },
        {
          "id": "50",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 14,
            "milliseconds": 133
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 17,
            "milliseconds": 899
          },
          "text": "and tried to predict the next wires."
        },
        {
          "id": "51",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 17,
            "milliseconds": 900
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 21,
            "milliseconds": 465
          },
          "text": "So this decoder itself is in kind of iterative operation."
        },
        {
          "id": "52",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 21,
            "milliseconds": 900
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 25,
            "milliseconds": 565
          },
          "text": "So by calling it again and again or to regress fully,"
        },
        {
          "id": "53",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 26,
            "milliseconds": 165
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 28,
            "milliseconds": 732
          },
          "text": "we can eventually generate six captions."
        }
      ],
      "source": [
        "But again, this can be any image backbones.",
        "So the next part that the quarter is a bit complex.",
        "So let's take a look very carefully.",
        "So this is the entire architecture of the decoder.",
        "It it gets was one by one",
        "and makes the information of wires and images",
        "which is coming from the encoder output",
        "and tried to predict the next wires.",
        "So this decoder itself is in kind of iterative operation.",
        "So by calling it again and again or to regress fully,",
        "we can eventually generate six captions."
      ],
      "result": [
        "但是，这可以是任何图像骨干网络。",
        "接下来的部分有点复杂，让我们仔细看看。",
        "",
        "这是解码器的整个架构。",
        "它逐个获取，并将编码器输出中的词和图像信息进行组合，尝试预测下一个词。",
        "",
        "",
        "",
        "解码器本身是一种迭代操作。",
        "通过反复调用或回归，最终可以生成六条说明文字。",
        ""
      ],
      "status": "success",
      "errors": [],
      "mismatched": false,
      "output": {
        "paragraph": "但是，这可以是任何图像骨干。接下来的部分有点复杂，让我们仔细看看。这是解码器的整个架构。它逐个获取，并将编码器输出中的线和图像信息进行组合，尝试预测下一个线。解码器本身是一种迭代操作。通过反复调用或回归，最终可以生成六个说明文字。",
        "sentences": [
          {
            "translated": "但是，这可以是任何图像骨干。",
            "indexes": [
              0
            ]
          },
          {
            "translated": "接下来的部分有点复杂，让我们仔细看看。",
            "indexes": [
              1,
              2
            ]
          },
          {
            "translated": "这是解码器的整个架构。",
            "indexes": [
              3
            ]
          },
          {
            "translated": "它逐个获取，并将编码器输出中的线和图像信息进行组合，尝试预测下一个线。",
            "indexes": [
              4,
              5,
              6,
              7
            ]
          },
          {
            "translated": "解码器本身是一种迭代操作。",
            "indexes": [
              8
            ]
          },
          {
            "translated": "通过反复调用或回归，最终可以生成六个说明文字。",
            "indexes": [
              9,
              10
            ]
          }
        ]
      }
    },
    {
      "items": [
        {
          "id": "54",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 29,
            "milliseconds": 900
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 30,
            "milliseconds": 566
          },
          "text": "There are so many"
        },
        {
          "id": "55",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 30,
            "milliseconds": 566
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 34,
            "milliseconds": 932
          },
          "text": "variations for this decoder design, but here we build transformer"
        },
        {
          "id": "56",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 34,
            "milliseconds": 932
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 39,
            "milliseconds": 98
          },
          "text": "like architecture, although we still use are in RNN GUR."
        },
        {
          "id": "57",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 40,
            "milliseconds": 765
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 45,
            "milliseconds": 298
          },
          "text": "So let's zoom into each component."
        },
        {
          "id": "58",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 45,
            "milliseconds": 300
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 48,
            "milliseconds": 332
          },
          "text": "The first embedding layer creates word embeddings,"
        },
        {
          "id": "59",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 48,
            "milliseconds": 866
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 51,
            "milliseconds": 800
          },
          "text": "which was discussed in other videos"
        },
        {
          "id": "60",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 51,
            "milliseconds": 800
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 56,
            "milliseconds": 99
          },
          "text": "and we are passing it to GUR layer."
        },
        {
          "id": "61",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 56,
            "milliseconds": 99
          },
          "endTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 57,
            "milliseconds": 965
          },
          "text": "If you forgot what you are your guess."
        },
        {
          "id": "62",
          "startTime": {
            "hours": 0,
            "minutes": 3,
            "seconds": 57,
            "milliseconds": 966
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 0,
            "milliseconds": 533
          },
          "text": "It's a variation of recurrent neural network"
        },
        {
          "id": "63",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 0,
            "milliseconds": 800
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 3,
            "milliseconds": 599
          },
          "text": "or you can call r in n"
        },
        {
          "id": "64",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 3,
            "milliseconds": 599
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 5,
            "milliseconds": 932
          },
          "text": "r n n takes inputs and updates"
        },
        {
          "id": "65",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 5,
            "milliseconds": 932
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 8,
            "milliseconds": 731
          },
          "text": "its internal states and generate output."
        }
      ],
      "source": [
        "There are so many",
        "variations for this decoder design, but here we build transformer",
        "like architecture, although we still use are in RNN GUR.",
        "So let's zoom into each component.",
        "The first embedding layer creates word embeddings,",
        "which was discussed in other videos",
        "and we are passing it to GUR layer.",
        "If you forgot what you are your guess.",
        "It's a variation of recurrent neural network",
        "or you can call r in n",
        "r n n takes inputs and updates",
        "its internal states and generate output."
      ],
      "result": [
        "有很多这种解码器设计的变体，但在这里我们构建了类似于Transformer的架构，尽管我们仍然使用RNN GRU。",
        "",
        "",
        "让我们放大每个组件。",
        "第一个嵌入层创建了单词嵌入，这在其他视频中已经讨论过了，我们将其传递给GUR层。",
        "",
        "",
        "如果你忘记了你猜的是什么。",
        "这是一种循环神经网络的变体，或者你可以称之为RNN，RNN接收输入并更新其内部状态并生成输出。",
        "",
        "",
        ""
      ],
      "status": "success",
      "errors": [],
      "mismatched": false,
      "output": {
        "paragraph": "有很多这种解码器设计的变体，但在这里我们构建了类似于Transformer的架构，尽管我们仍然使用能量R2。让我们放大每个组件。第一个嵌入层创建了单词嵌入，这在其他视频中已经讨论过了，我们将其传递给烤架层。如果你忘了你是你的猜测。这是一种循环神经网络的变体，或者你可以称之为RNN，RNN接收输入并更新其内部状态并生成输出。",
        "sentences": [
          {
            "translated": "有很多这种解码器设计的变体，但在这里我们构建了类似于Transformer的架构，尽管我们仍然使用能量R2。",
            "indexes": [
              0,
              1,
              2
            ]
          },
          {
            "translated": "让我们放大每个组件。",
            "indexes": [
              3
            ]
          },
          {
            "translated": "第一个嵌入层创建了单词嵌入，这在其他视频中已经讨论过了，我们将其传递给烤架层。",
            "indexes": [
              4,
              5,
              6
            ]
          },
          {
            "translated": "如果你忘了你是你的猜测。",
            "indexes": [
              7
            ]
          },
          {
            "translated": "这是一种循环神经网络的变体，或者你可以称之为RNN，RNN接收输入并更新其内部状态并生成输出。",
            "indexes": [
              8,
              9,
              10,
              11
            ]
          }
        ]
      }
    },
    {
      "items": [
        {
          "id": "66",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 9,
            "milliseconds": 599
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 14,
            "milliseconds": 65
          },
          "text": "So by passing sequential data like text data, it keeps two sequential"
        },
        {
          "id": "67",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 14,
            "milliseconds": 66
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 14,
            "milliseconds": 665
          },
          "text": "dependencies."
        },
        {
          "id": "68",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 14,
            "milliseconds": 665
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 17,
            "milliseconds": 564
          },
          "text": "These from previous inputs like previous was"
        },
        {
          "id": "69",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 18,
            "milliseconds": 932
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 21,
            "milliseconds": 731
          },
          "text": "that grow output goes to attention layer"
        },
        {
          "id": "70",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 22,
            "milliseconds": 132
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 24,
            "milliseconds": 898
          },
          "text": "which mixes the information of texts and image"
        },
        {
          "id": "71",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 26,
            "milliseconds": 100
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 27,
            "milliseconds": 966
          },
          "text": "in TensorFlow Keras, we can use"
        },
        {
          "id": "72",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 27,
            "milliseconds": 966
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 31,
            "milliseconds": 33
          },
          "text": "predefined layers in the same way as other layers."
        },
        {
          "id": "73",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 32,
            "milliseconds": 165
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 34,
            "milliseconds": 32
          },
          "text": "There are multiple implementations,"
        },
        {
          "id": "74",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 34,
            "milliseconds": 33
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 38,
            "milliseconds": 100
          },
          "text": "but here we simply use F cross layers attention."
        },
        {
          "id": "75",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 39,
            "milliseconds": 300
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 44,
            "milliseconds": 165
          },
          "text": "But if you want to use more transformer like architecture, you can know. So"
        },
        {
          "id": "76",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 44,
            "milliseconds": 165
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 47,
            "milliseconds": 765
          },
          "text": "the pick up tf cross layers most attention"
        },
        {
          "id": "77",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 48,
            "milliseconds": 165
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 50,
            "milliseconds": 532
          },
          "text": "which uses multiple attention heads."
        }
      ],
      "source": [
        "So by passing sequential data like text data, it keeps two sequential",
        "dependencies.",
        "These from previous inputs like previous was",
        "that grow output goes to attention layer",
        "which mixes the information of texts and image",
        "in TensorFlow Keras, we can use",
        "predefined layers in the same way as other layers.",
        "There are multiple implementations,",
        "but here we simply use F cross layers attention.",
        "But if you want to use more transformer like architecture, you can know. So",
        "the pick up tf cross layers most attention",
        "which uses multiple attention heads."
      ],
      "result": [
        "所以通过传递序列数据，如文本数据，它可以保持序列依赖性。",
        "",
        "这些来自以前的输入，比如之前的词，那个GRU的输出进入了注意力层，它混合了文本和图像的信息。",
        "",
        "",
        "在TensorFlow Keras中，我们可以像使用其他层一样使用预定义的层。",
        "",
        "有多种实现，但是我们在这里简单地使用了tf.keras.layers.Attention。",
        "",
        "但是，如果你想使用更像Transformer的架构，你可以了解tf.keras.layers.MultiHeadAttention，它使用多个注意力头。",
        "",
        ""
      ],
      "status": "success",
      "errors": [],
      "mismatched": false,
      "output": {
        "paragraph": "通过传递像文本数据这样的顺序数据，它保持两个顺序依赖关系。这些来自之前的输入，例如之前的增长输出进入注意力层，这将文本和图像的信息混合在一起。在TensorFlow Chorus中，我们可以像使用其他层一样使用预定义的层。有多种实现，但在这里我们只是简单地使用F交叉层注意力。但是，如果你想使用更像Transformer的架构，你可以了解。所以，选择tf交叉层最关注，它使用多个关注头。",
        "sentences": [
          {
            "translated": "通过传递像文本数据这样的顺序数据，它保持两个顺序依赖关系。",
            "indexes": [
              0,
              1
            ]
          },
          {
            "translated": "这些来自之前的输入，例如之前的增长输出进入注意力层，这将文本和图像的信息混合在一起。",
            "indexes": [
              2,
              3,
              4
            ]
          },
          {
            "translated": "在TensorFlow Chorus中，我们可以像使用其他层一样使用预定义的层。",
            "indexes": [
              5,
              6
            ]
          },
          {
            "translated": "有多种实现，但在这里我们只是简单地使用F交叉层注意力。",
            "indexes": [
              7,
              8
            ]
          },
          {
            "translated": "但是，如果你想使用更像Transformer的架构，你可以了解。",
            "indexes": [
              9
            ]
          },
          {
            "translated": "所以，选择tf交叉层最关注，它使用多个关注头。",
            "indexes": [
              10,
              11
            ]
          }
        ]
      }
    },
    {
      "items": [
        {
          "id": "78",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 51,
            "milliseconds": 600
          },
          "endTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 54,
            "milliseconds": 933
          },
          "text": "You can simply switch and use it in almost the same way."
        },
        {
          "id": "79",
          "startTime": {
            "hours": 0,
            "minutes": 4,
            "seconds": 56,
            "milliseconds": 266
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 0,
            "milliseconds": 165
          },
          "text": "Inside our attention layer, it looks like this as you may"
        },
        {
          "id": "80",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 0,
            "milliseconds": 165
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 3,
            "milliseconds": 765
          },
          "text": "have already seen in another video about attention mechanism."
        },
        {
          "id": "81",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 4,
            "milliseconds": 800
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 6,
            "milliseconds": 100
          },
          "text": "0 But the unique thing here is it pays"
        },
        {
          "id": "82",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 7,
            "milliseconds": 0
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 10,
            "milliseconds": 233
          },
          "text": "attention to image feature from text data"
        },
        {
          "id": "83",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 11,
            "milliseconds": 699
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 14,
            "milliseconds": 465
          },
          "text": "by doing so, it can calculate attention"
        },
        {
          "id": "84",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 14,
            "milliseconds": 466
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 16,
            "milliseconds": 833
          },
          "text": "score by mixing both information."
        },
        {
          "id": "85",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 18,
            "milliseconds": 600
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 19,
            "milliseconds": 899
          },
          "text": "Going back to code,"
        },
        {
          "id": "86",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 19,
            "milliseconds": 899
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 23,
            "milliseconds": 99
          },
          "text": "you can see this attention layer takes two inputs"
        },
        {
          "id": "87",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 24,
            "milliseconds": 0
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 26,
            "milliseconds": 833
          },
          "text": "gru_ouput and encoder output."
        }
      ],
      "source": [
        "You can simply switch and use it in almost the same way.",
        "Inside our attention layer, it looks like this as you may",
        "have already seen in another video about attention mechanism.",
        "0 But the unique thing here is it pays",
        "attention to image feature from text data",
        "by doing so, it can calculate attention",
        "score by mixing both information.",
        "Going back to code,",
        "you can see this attention layer takes two inputs",
        "gru_ouput and encoder output."
      ],
      "result": [
        "你可以简单地切换并以几乎相同的方式使用它。",
        "在我们的注意力层中，你可能已经在关于注意力机制的另一个视频中看到了这个。",
        "",
        "但这里独特的是，它通过处理文本数据关注图像特征，这样，它可以通过混合两种信息来计算注意力分数。",
        "",
        "",
        "",
        "回到代码，你可以看到这个注意力层接受两个输入，gru_ouput和encoder_output。",
        "",
        ""
      ],
      "status": "success",
      "errors": [],
      "mismatched": false,
      "output": {
        "paragraph": "你可以简单地切换并以几乎相同的方式使用它。在我们的注意力层中，你可能已经在关于注意力机制的另一个视频中看到了这个。但这里独特的是，它通过处理文本数据关注图像特征，这样，它可以通过混合两种信息来计算注意力分数。回到代码，你可以看到这个注意力层接受两个输入，Geri输出和编码器输出。",
        "sentences": [
          {
            "translated": "你可以简单地切换并以几乎相同的方式使用它。",
            "indexes": [
              0
            ]
          },
          {
            "translated": "在我们的注意力层中，你可能已经在关于注意力机制的另一个视频中看到了这个。",
            "indexes": [
              1,
              2
            ]
          },
          {
            "translated": "但这里独特的是，它通过处理文本数据关注图像特征，这样，它可以通过混合两种信息来计算注意力分数。",
            "indexes": [
              3,
              4,
              5,
              6
            ]
          },
          {
            "translated": "回到代码，你可以看到这个注意力层接受两个输入，Geri输出和编码器输出。",
            "indexes": [
              7,
              8,
              9
            ]
          }
        ]
      }
    },
    {
      "items": [
        {
          "id": "88",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 28,
            "milliseconds": 565
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 32,
            "milliseconds": 399
          },
          "text": "Internally, GRU ouput is used as attention"
        },
        {
          "id": "89",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 32,
            "milliseconds": 399
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 36,
            "milliseconds": 599
          },
          "text": "query and key and encoder output as value."
        },
        {
          "id": "90",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 38,
            "milliseconds": 165
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 40,
            "milliseconds": 865
          },
          "text": "The last components are add layer"
        },
        {
          "id": "91",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 41,
            "milliseconds": 199
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 43,
            "milliseconds": 364
          },
          "text": "and layer normalization layer"
        },
        {
          "id": "92",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 44,
            "milliseconds": 932
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 45,
            "milliseconds": 799
          },
          "text": "Add layer"
        },
        {
          "id": "93",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 45,
            "milliseconds": 800
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 48,
            "milliseconds": 500
          },
          "text": "Just add two same shift vectors."
        },
        {
          "id": "94",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 50,
            "milliseconds": 100
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 53,
            "milliseconds": 865
          },
          "text": "As you can see here, the GRU ouput as passed to attention"
        },
        {
          "id": "95",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 53,
            "milliseconds": 865
          },
          "endTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 58,
            "milliseconds": 299
          },
          "text": "layer as we discussed and to this Add layer directly,"
        },
        {
          "id": "96",
          "startTime": {
            "hours": 0,
            "minutes": 5,
            "seconds": 59,
            "milliseconds": 800
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 1,
            "milliseconds": 932
          },
          "text": "these two flows are eventually"
        },
        {
          "id": "97",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 1,
            "milliseconds": 932
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 4,
            "milliseconds": 765
          },
          "text": "marched in this Add layer."
        }
      ],
      "source": [
        "Internally, GRU ouput is used as attention",
        "query and key and encoder output as value.",
        "The last components are add layer",
        "and layer normalization layer",
        "at layer.",
        "Just add two same shift vectors.",
        "As you can see here, the GRU ouput as passed to attention",
        "layer as we discussed and to this ad layer directly,",
        "these two flows are eventually",
        "marched in this other layer."
      ],
      "result": [
        "在内部，GRU的输出用作注意力查询和键，编码器输出用作值。",
        "",
        "最后的组件是Add层和LayerNormalization层。",
        "",
        "Add层只是添加两个相同的移位向量。",
        "",
        "如你所见，GRU的输出被传递给我们讨论过的注意力层以及直接传递给这个Add层，这两个流最终在这个Add层中合并。",
        "",
        "",
        ""
      ],
      "status": "success",
      "errors": [],
      "mismatched": false,
      "output": {
        "paragraph": "在内部，组输出用作注意力查询和键，编码器输出用作值。最后的组件是Add层和层归一化层。只需添加两个相同的移位向量。如你所见，组输出被传递给我们讨论过的注意力层以及直接传递给这个Add层，这两个流最终在这个其他层中合并。",
        "sentences": [
          {
            "translated": "在内部，组输出用作注意力查询和键，编码器输出用作值。",
            "indexes": [
              0,
              1
            ]
          },
          {
            "translated": "最后的组件是Add层（Add Layer）和LayerNormalization层。",
            "indexes": [
              2,
              3
            ]
          },
          {
            "translated": "只需添加两个相同的移位向量。",
            "indexes": [
              4
            ]
          },
          {
            "translated": "如你所见，组输出被传递给我们讨论过的注意力层以及直接传递给这个Add层，这两个流最终在这个其他层中合并。",
            "indexes": [
              5,
              6,
              7,
              8,
              9
            ]
          }
        ]
      }
    },
    {
      "items": [
        {
          "id": "98",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 5,
            "milliseconds": 0
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 6,
            "milliseconds": 500
          },
          "text": "This kind of architecture"
        },
        {
          "id": "99",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 6,
            "milliseconds": 500
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 10,
            "milliseconds": 732
          },
          "text": "is called skip connection, which has been a very popular"
        },
        {
          "id": "100",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 10,
            "milliseconds": 766
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 14,
            "milliseconds": 200
          },
          "text": "deep neural network design pattern since Resonant."
        },
        {
          "id": "101",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 15,
            "milliseconds": 333
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 20,
            "milliseconds": 600
          },
          "text": "So it is also called residual connection."
        },
        {
          "id": "102",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 20,
            "milliseconds": 600
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 25,
            "milliseconds": 733
          },
          "text": "This skip connection is very useful, especially when you want to design"
        },
        {
          "id": "103",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 25,
            "milliseconds": 733
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 29,
            "milliseconds": 300
          },
          "text": "a very deep neural network and it is also"
        },
        {
          "id": "104",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 29,
            "milliseconds": 300
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 32,
            "milliseconds": 166
          },
          "text": "used in the transformer."
        },
        {
          "id": "105",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 32,
            "milliseconds": 500
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 35,
            "milliseconds": 600
          },
          "text": "With this now we could build an entire decoder,"
        },
        {
          "id": "106",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 36,
            "milliseconds": 333
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 39,
            "milliseconds": 599
          },
          "text": "so we are ready to train the encoder decoder image"
        },
        {
          "id": "107",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 39,
            "milliseconds": 600
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 42,
            "milliseconds": 499
          },
          "text": "captioning model using the captioning dataset."
        }
      ],
      "source": [
        "This kind of architecture",
        "is called skip connection, which has been a very popular",
        "deep neural network design pattern since Resonant.",
        "So it is also called residual connection.",
        "This skip connection is very useful, especially when you want to design",
        "a very deep neural network and it is also",
        "used in the transformer.",
        "With this now we could build an entire decoder,",
        "so we are ready to train the encoder decoder image",
        "captioning model using the captioning dataset."
      ],
      "result": [
        "这种架构被称为跳跃连接（Skip Connection），自Resonant以来一直非常受欢迎的深度神经网络设计模式。",
        "",
        "",
        "因此，它也被称为残差连接（Residual Connection）。",
        "这种跳跃连接非常有用，尤其是当你想设计一个非常深的神经网络时，它还用于Transformer。",
        "",
        "",
        "有了这个，我们现在可以构建一个完整的解码器，因此我们准备使用说明文字数据集训练编码器解码器图像说明文字模型。",
        "",
        ""
      ],
      "status": "success",
      "errors": [],
      "mismatched": false,
      "output": {
        "paragraph": "这种架构被称为跳跃连接，自Resonant以来一直非常受欢迎的深度神经网络设计模式。因此，它也被称为残差连接。这种跳跃连接非常有用，尤其是当你想设计一个非常深的神经网络时，它还用于Transformer。有了这个，我们现在可以构建一个完整的解码器，因此我们准备使用说明文字数据集训练编码器解码器图像说明文字模型。",
        "sentences": [
          {
            "translated": "这种架构被称为跳跃连接，自Resonant以来一直非常受欢迎的深度神经网络设计模式。",
            "indexes": [
              0,
              1,
              2
            ]
          },
          {
            "translated": "因此，它也被称为残差连接。",
            "indexes": [
              3
            ]
          },
          {
            "translated": "这种跳跃连接非常有用，尤其是当你想设计一个非常深的神经网络时，它还用于Transformer。",
            "indexes": [
              4,
              5,
              6
            ]
          },
          {
            "translated": "有了这个，我们现在可以构建一个完整的解码器，因此我们准备使用说明文字数据集训练编码器解码器图像说明文字模型。",
            "indexes": [
              7,
              8,
              9
            ]
          }
        ]
      }
    },
    {
      "items": [
        {
          "id": "108",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 43,
            "milliseconds": 399
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 47,
            "milliseconds": 932
          },
          "text": "We will see how it works in the next video."
        },
        {
          "id": "109",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 47,
            "milliseconds": 932
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 50,
            "milliseconds": 331
          },
          "text": "But before moving on to the next one,"
        },
        {
          "id": "110",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 50,
            "milliseconds": 699
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 53,
            "milliseconds": 731
          },
          "text": "I want to explain a bit more about inference phase"
        },
        {
          "id": "111",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 54,
            "milliseconds": 100
          },
          "endTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 57,
            "milliseconds": 566
          },
          "text": "where we can actually generate captions for obviously images"
        },
        {
          "id": "112",
          "startTime": {
            "hours": 0,
            "minutes": 6,
            "seconds": 58,
            "milliseconds": 300
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 0,
            "milliseconds": 699
          },
          "text": "because this process may look a bit complex"
        },
        {
          "id": "113",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 2,
            "milliseconds": 533
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 2,
            "milliseconds": 766
          },
          "text": "here."
        },
        {
          "id": "114",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 2,
            "milliseconds": 766
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 5,
            "milliseconds": 899
          },
          "text": "We can see three steps and we're going to implement"
        },
        {
          "id": "115",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 5,
            "milliseconds": 899
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 8,
            "milliseconds": 499
          },
          "text": "these steps in a custom inference function,"
        },
        {
          "id": "116",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 10,
            "milliseconds": 0
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 13,
            "milliseconds": 200
          },
          "text": "the number one, and generate the initial state"
        },
        {
          "id": "117",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 13,
            "milliseconds": 500
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 16,
            "milliseconds": 432
          },
          "text": "and create a star token"
        },
        {
          "id": "118",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 16,
            "milliseconds": 432
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 17,
            "milliseconds": 865
          },
          "text": "in training phase."
        }
      ],
      "source": [
        "We will see how it works in the next video.",
        "But before moving on to the next one,",
        "I want to explain a bit more about inference phase",
        "where we can actually generate captions for obviously images",
        "because this process may look a bit complex",
        "here.",
        "We can see three steps and we're going to implement",
        "these steps in a custom inference function,",
        "the number one, and generate the initial state",
        "and create a star token",
        "in training phase."
      ],
      "result": [
        "我们将在下一个视频中看到它是如何工作的。",
        "但在继续下一个之前，我想多解释一下关于推理阶段的内容，在这个阶段我们实际上可以为图像生成说明文字，因为这个过程可能看起来有点复杂。",
        "",
        "",
        "",
        "",
        "在这里，我们可以看到三个步骤，我们将在一个自定义推理函数中实现这些步骤，",
        "",
        "第一，生成初始状态并创建一个起始Token，在训练阶段。",
        "",
        ""
      ],
      "status": "success",
      "errors": [],
      "mismatched": false,
      "output": {
        "paragraph": "我们将在下一个视频中看到它是如何工作的。但在继续下一个之前，我想多解释一下关于推理阶段的内容，我们可以为显而易见的图像生成说明文字，因为这个过程可能看起来有点复杂。在这里，我们可以看到三个步骤，我们将在一个自定义推理函数中实现这些步骤，第一，生成初始状态并创建一个起始Token，在训练阶段。",
        "sentences": [
          {
            "translated": "我们将在下一个视频中看到它是如何工作的。",
            "indexes": [
              0
            ]
          },
          {
            "translated": "但在继续下一个之前，我想多解释一下关于推理阶段的内容，",
            "indexes": [
              1
            ]
          },
          {
            "translated": "我们可以为显而易见的图像生成说明文字，因为这个过程可能看起来有点复杂。",
            "indexes": [
              2,
              3,
              4
            ]
          },
          {
            "translated": "在这里，我们可以看到三个步骤，我们将在一个自定义推理函数中实现这些步骤，",
            "indexes": [
              5,
              6,
              7
            ]
          },
          {
            "translated": "第一，生成初始状态并创建一个起始Token，在训练阶段。",
            "indexes": [
              8,
              9,
              10
            ]
          }
        ]
      }
    },
    {
      "items": [
        {
          "id": "119",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 17,
            "milliseconds": 865
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 20,
            "milliseconds": 599
          },
          "text": "TensorFlow Keras can automatically handle"
        },
        {
          "id": "120",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 21,
            "milliseconds": 65
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 23,
            "milliseconds": 698
          },
          "text": "a state for each sequence,"
        },
        {
          "id": "121",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 23,
            "milliseconds": 699
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 27,
            "milliseconds": 865
          },
          "text": "but in this inference phase, since we design our own custom function,"
        },
        {
          "id": "122",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 28,
            "milliseconds": 266
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 31,
            "milliseconds": 500
          },
          "text": "we need to write a logic to deal with it explicitly."
        },
        {
          "id": "123",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 33,
            "milliseconds": 266
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 35,
            "milliseconds": 699
          },
          "text": "So at the beginning of each captioning"
        },
        {
          "id": "124",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 35,
            "milliseconds": 699
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 38,
            "milliseconds": 498
          },
          "text": "we explicitly initialize the GRU state"
        },
        {
          "id": "125",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 38,
            "milliseconds": 500
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 40,
            "milliseconds": 500
          },
          "text": "with some value"
        },
        {
          "id": "126",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 41,
            "milliseconds": 800
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 43,
            "milliseconds": 432
          },
          "text": "and at the same time"
        },
        {
          "id": "127",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 43,
            "milliseconds": 432
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 46,
            "milliseconds": 565
          },
          "text": "remember our decoder is an auto regressive function."
        },
        {
          "id": "128",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 47,
            "milliseconds": 466
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 51,
            "milliseconds": 232
          },
          "text": "But since we haven't get any wider prediction yet at the beginning"
        },
        {
          "id": "129",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 51,
            "milliseconds": 233
          },
          "endTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 56,
            "milliseconds": 632
          },
          "text": "of the inference we start talking, which is a special token."
        }
      ],
      "source": [
        "TensorFlow Keras can automatically handle",
        "a state for each sequence,",
        "but in this inference phase, since we design our own custom function,",
        "we need to write a logic to deal with it explicitly.",
        "So at the beginning of each captioning",
        "we explicitly initialize the GRU state",
        "with some value",
        "and at the same time",
        "remember our decoder is an auto regressive function.",
        "But since we haven't get any wider prediction yet at the beginning",
        "of the inference we start talking, which is a special token."
      ],
      "result": [
        "TensorFlow Chorus可以自动处理每个序列的状态，",
        "",
        "但在这个推理阶段，由于我们设计了自己的自定义函数，我们需要明确地编写处理它的逻辑。",
        "",
        "因此，在每个说明文字的开始，我们明确地用某个值初始化GRU状态，",
        "",
        "",
        "同时记住我们的解码器是一个自回归函数。",
        "",
        "但是由于我们在推理开始时还没有得到任何预测，所以我们从一个特殊的开始Token开始。",
        ""
      ],
      "status": "success",
      "errors": [
        "mismatched: 11 vs 12, Mon Jun 26 2023 15:41:36 GMT-0500 (Central Daylight Time)"
      ],
      "mismatched": false,
      "output": {
        "paragraph": "TensorFlow Chorus可以自动处理每个序列的状态，但在这个推理阶段，由于我们设计了自己的自定义函数，我们需要明确地编写处理它的逻辑。因此，在每个说明文字的开始，我们明确地用某个值初始化状态，同时记住我们的解码器是一个自回归函数。但是，由于我们在推理开始时还没有得到任何更广泛的预测，我们开始说话，这是一个特殊的Token。",
        "sentences": [
          {
            "translated": "TensorFlow Chorus可以自动处理每个序列的状态，",
            "indexes": [
              0
            ]
          },
          {
            "translated": "但在这个推理阶段，由于我们设计了自己的自定义函数，我们需要明确地编写处理它的逻辑。",
            "indexes": [
              1,
              2
            ]
          },
          {
            "translated": "因此，在每个说明文字的开始，我们明确地用某个值初始化状态，",
            "indexes": [
              4,
              5
            ]
          },
          {
            "translated": "同时记住我们的解码器是一个自回归函数。",
            "indexes": [
              7,
              8
            ]
          },
          {
            "translated": "但是，由于我们在推理开始时还没有得到任何更广泛的预测，我们开始说话，这是一个特殊的Token。",
            "indexes": [
              9,
              10,
              11
            ]
          }
        ]
      }
    },
    {
      "items": [
        {
          "id": "130",
          "startTime": {
            "hours": 0,
            "minutes": 7,
            "seconds": 56,
            "milliseconds": 632
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 0,
            "milliseconds": 232
          },
          "text": "That means the beginning of a sentence"
        },
        {
          "id": "131",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 0,
            "milliseconds": 733
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 2,
            "milliseconds": 165
          },
          "text": "number 2%"
        },
        {
          "id": "132",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 2,
            "milliseconds": 165
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 6,
            "milliseconds": 665
          },
          "text": "input image to the encoder and instruct the feature vector as we discussed"
        },
        {
          "id": "133",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 7,
            "milliseconds": 233
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 10,
            "milliseconds": 233
          },
          "text": "and number three passed a vector"
        },
        {
          "id": "134",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 10,
            "milliseconds": 233
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 15,
            "milliseconds": 633
          },
          "text": "to this time decoder and generate a caption word in the for loop"
        },
        {
          "id": "135",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 16,
            "milliseconds": 65
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 18,
            "milliseconds": 598
          },
          "text": "until it returns and token"
        },
        {
          "id": "136",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 19,
            "milliseconds": 65
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 23,
            "milliseconds": 465
          },
          "text": "or it reads to max caption lengths, which is just a hyper parameter"
        },
        {
          "id": "137",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 23,
            "milliseconds": 899
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 27,
            "milliseconds": 64
          },
          "text": "specifying some number like 264."
        },
        {
          "id": "138",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 27,
            "milliseconds": 699
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 31,
            "milliseconds": 665
          },
          "text": "And in this full loop we define all the procedures of caption"
        },
        {
          "id": "139",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 31,
            "milliseconds": 665
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 35,
            "milliseconds": 532
          },
          "text": "generation by calling the decoder also aggressively"
        },
        {
          "id": "140",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 37,
            "milliseconds": 166
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 38,
            "milliseconds": 299
          },
          "text": "and token is a"
        },
        {
          "id": "141",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 38,
            "milliseconds": 299
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 41,
            "milliseconds": 499
          },
          "text": "special token two, which means to end of the sequence."
        }
      ],
      "source": [
        "That means the beginning of a sentence",
        "number 2%",
        "input image to the encoder and instruct the feature vector as we discussed",
        "and number three passed a vector",
        "to this time decoder and generate a caption word in the for loop",
        "until it returns and token",
        "or it reads to max caption lengths, which is just a hyper parameter",
        "specifying some number like 264.",
        "And in this full loop we define all the procedures of caption",
        "generation by calling the decoder also aggressively",
        "and token is a",
        "special token two, which means to end of the sequence."
      ],
      "result": [
        "这意味着一个句子的开始，",
        "第二，将图像输入到编码器，并按照我们讨论的进行特征向量的提取，",
        "",
        "第三，将向量传递给这个时间解码器，在for循环中生成一个说明文字的单词，直到它返回一个结束Token，或者达到最大说明文字长度，这只是一个指定一些数字的超参数，比如264。",
        "",
        "",
        "",
        "",
        "在这个完整的循环中，我们定义了所有说明文字通过反复调用解码器生成的过程，结束Token也是一个特殊的Token，意味着序列的结束。",
        "",
        "",
        ""
      ],
      "status": "success",
      "errors": [],
      "mismatched": false,
      "output": {
        "paragraph": "这意味着一个句子的开始，数字2%。将图像输入到编码器中，并按照我们讨论的指示特征向量，然后将向量传递给这个时间解码器，并在for循环中生成一个说明文字词，直到它返回一个Token，或者它读到最大说明文字长度，这只是一个超参数，指定像264这样的数字。在这个完整的循环中，我们通过积极调用解码器来定义所有说明文字生成的过程，Token是一个特殊的Token2，表示序列的结束。",
        "sentences": [
          {
            "translated": "这意味着一个句子的开始，数字2%。",
            "indexes": [
              0,
              1
            ]
          },
          {
            "translated": "将图像输入到编码器中，并按照我们讨论的指示特征向量，然后将向量传递给这个时间解码器，并在for循环中生成一个说明文字词，直到它返回一个Token，或者它读到最大说明文字长度，这只是一个超参数，指定像264这样的数字。",
            "indexes": [
              2,
              3,
              4,
              5,
              6,
              7
            ]
          },
          {
            "translated": "在这个完整的循环中，我们通过积极调用解码器来定义所有说明文字生成的过程，Token是一个特殊的Token2，表示序列的结束。",
            "indexes": [
              8,
              9,
              10,
              11
            ]
          }
        ]
      }
    },
    {
      "items": [
        {
          "id": "142",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 42,
            "milliseconds": 265
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 44,
            "milliseconds": 999
          },
          "text": "So when our decoder generate this token,"
        },
        {
          "id": "143",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 45,
            "milliseconds": 265
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 48,
            "milliseconds": 164
          },
          "text": "we can finish this full loop"
        },
        {
          "id": "144",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 48,
            "milliseconds": 166
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 51,
            "milliseconds": 633
          },
          "text": "or you can go out of the loop when the lengths of the caption"
        },
        {
          "id": "145",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 51,
            "milliseconds": 633
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 56,
            "milliseconds": 133
          },
          "text": "rigid, some number max capsule lengths."
        },
        {
          "id": "146",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 56,
            "milliseconds": 133
          },
          "endTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 58,
            "milliseconds": 200
          },
          "text": "Let's take a look at the code one by one."
        },
        {
          "id": "147",
          "startTime": {
            "hours": 0,
            "minutes": 8,
            "seconds": 59,
            "milliseconds": 832
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 1,
            "milliseconds": 965
          },
          "text": "In the first step we initialize"
        },
        {
          "id": "148",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 2,
            "milliseconds": 0
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 6,
            "milliseconds": 65
          },
          "text": "two things GRU state and start token."
        },
        {
          "id": "149",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 6,
            "milliseconds": 66
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 10,
            "milliseconds": 665
          },
          "text": "In this case, GRU state is simply initialized with zero vectors"
        },
        {
          "id": "150",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 11,
            "milliseconds": 765
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 13,
            "milliseconds": 931
          },
          "text": "n which says start tokens"
        },
        {
          "id": "151",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 13,
            "milliseconds": 932
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 16,
            "milliseconds": 299
          },
          "text": "as the first input word for the decoder"
        },
        {
          "id": "152",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 18,
            "milliseconds": 66
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 22,
            "milliseconds": 365
          },
          "text": "in terms of the word_to_index function used here, I'm going to explain"
        },
        {
          "id": "153",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 22,
            "milliseconds": 365
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 27,
            "milliseconds": 599
          },
          "text": "in the next video, but it basically is just tokenized awards to our token,"
        },
        {
          "id": "154",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 28,
            "milliseconds": 100
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 30,
            "milliseconds": 600
          },
          "text": "which is the standard text pre-processing technique."
        }
      ],
      "source": [
        "So when our decoder generate this token,",
        "we can finish this full loop",
        "or you can go out of the loop when the lengths of the caption",
        "rigid, some number max capsule lengths.",
        "Let's take a look at the code one by one.",
        "In the first step we initialize",
        "two things GRU state and start token.",
        "In this case, GRU state is simply initialized with zero vectors",
        "n which says start tokens",
        "as the first input word for the decoder",
        "in terms of the word_to_index function used here, I'm going to explain",
        "in the next video, but it basically is just tokenized awards to our token,",
        "which is the standard text pre-processing technique."
      ],
      "result": [
        "因此，当我们的解码器生成这个Token时，我们可以完成这个完整的循环，或者当说明文字的长度达到某个最大长度时，可以跳出循环。",
        "",
        "",
        "",
        "让我们逐一看一下代码。",
        "在第一步中，我们初始化了两个东西：GRU状态和起始Token。",
        "",
        "在这种情况下，GRU的状态只是用零向量初始化，而起始Token作为解码器的第一个输入词。",
        "",
        "",
        "关于这里使用的word_to_index函数，我将在下一个视频中解释，但它基本上只是将词汇分词为我们的Token，这是标准的文本预处理技术。",
        "",
        ""
      ],
      "status": "success",
      "errors": [],
      "mismatched": false,
      "output": {
        "paragraph": "因此，当我们的解码器生成这个Token时，我们可以完成这个完整的循环，或者当说明文字的长度达到某个最大长度时，可以跳出循环。让我们逐一看一下代码。在第一步中，我们初始化了两个东西：Jerry状态和起始Token。在这种情况下，Jerry的状态只是用零向量初始化，而起始Token作为解码器的第一个输入词。关于这里使用的bar到index函数，我将在下一个视频中解释，但它基本上只是将词汇分词为我们的Token，这是标准的文本预处理技术。",
        "sentences": [
          {
            "translated": "因此，当我们的解码器生成这个Token时，我们可以完成这个完整的循环，或者当说明文字的长度达到某个最大长度时，可以跳出循环。",
            "indexes": [
              0,
              1,
              2,
              3
            ]
          },
          {
            "translated": "让我们逐一看一下代码。",
            "indexes": [
              4
            ]
          },
          {
            "translated": "在第一步中，我们初始化了两个东西：Jerry状态和起始Token。",
            "indexes": [
              5,
              6
            ]
          },
          {
            "translated": "在这种情况下，Jerry的状态只是用零向量初始化，而起始Token作为解码器的第一个输入词。",
            "indexes": [
              7,
              8,
              9
            ]
          },
          {
            "translated": "关于这里使用的bar到index函数，我将在下一个视频中解释，但它基本上只是将词汇分词为我们的Token，这是标准的文本预处理技术。",
            "indexes": [
              10,
              11,
              12
            ]
          }
        ]
      }
    },
    {
      "items": [
        {
          "id": "155",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 31,
            "milliseconds": 865
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 32,
            "milliseconds": 399
          },
          "text": "In the next"
        },
        {
          "id": "156",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 32,
            "milliseconds": 399
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 35,
            "milliseconds": 265
          },
          "text": "step, we pre process to input image habit"
        },
        {
          "id": "157",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 35,
            "milliseconds": 633
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 37,
            "milliseconds": 866
          },
          "text": "and pass it to the encoder we train."
        },
        {
          "id": "158",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 39,
            "milliseconds": 399
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 41,
            "milliseconds": 464
          },
          "text": "In terms of the image pre-processing,"
        },
        {
          "id": "159",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 42,
            "milliseconds": 133
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 46,
            "milliseconds": 732
          },
          "text": "it reads in decode JPEG in the first line and resize it"
        },
        {
          "id": "160",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 47,
            "milliseconds": 365
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 49,
            "milliseconds": 965
          },
          "text": "from any arbitrarily resolutions"
        },
        {
          "id": "161",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 49,
            "milliseconds": 966
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 53,
            "milliseconds": 166
          },
          "text": "to specific resolution"
        },
        {
          "id": "162",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 53,
            "milliseconds": 166
          },
          "endTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 57,
            "milliseconds": 33
          },
          "text": "and it changes to scale from 0"
        },
        {
          "id": "163",
          "startTime": {
            "hours": 0,
            "minutes": 9,
            "seconds": 57,
            "milliseconds": 33
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 0,
            "milliseconds": 432
          },
          "text": "to 255 two 0 to 1 in the third line"
        },
        {
          "id": "164",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 1,
            "milliseconds": 932
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 5,
            "milliseconds": 331
          },
          "text": "and the last phase decoder for loop."
        }
      ],
      "source": [
        "In the next",
        "step, we pre process to input image habit",
        "and pass it to the encoder we train.",
        "In terms of the image pre-processing,",
        "it reads in decode JPEG in the first line and resize it",
        "from any arbitrarily resolutions",
        "to specific resolution",
        "and it changes to scale from 0",
        "to 255 two 0 to 1 in the third line",
        "and the last phase decoder for loop."
      ],
      "result": [
        "在下一步中，我们预处理输入的图像，并将其传递给我们训练的编码器。",
        "",
        "",
        "在图像预处理方面，它首先读取并解码JPEG，然后将其从任意分辨率调整为特定分辨率，",
        "",
        "",
        "",
        "接着在第三行将比例从0到255更改为0到1，",
        "",
        "最后是解码器循环。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false,
      "output": {
        "paragraph": "在下一步中，我们预处理输入图像习惯，并将其传递给我们训练的编码器。在图像预处理方面，它首先读取并解码JPEG，然后将其从任意分辨率调整为特定分辨率，接着在第三行将比例从0到255更改为0到1，最后是解码器循环。",
        "sentences": [
          {
            "translated": "在下一步中，我们预处理输入图像习惯，并将其传递给我们训练的编码器。",
            "indexes": [
              0,
              1,
              2
            ]
          },
          {
            "translated": "在图像预处理方面，它首先读取并解码JPEG，然后将其从任意分辨率调整为特定分辨率，",
            "indexes": [
              3,
              4,
              5,
              6
            ]
          },
          {
            "translated": "接着在第三行将比例从0到255更改为0到1，",
            "indexes": [
              7,
              8
            ]
          },
          {
            "translated": "最后是解码器循环。",
            "indexes": [
              9
            ]
          }
        ]
      }
    },
    {
      "items": [
        {
          "id": "165",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 5,
            "milliseconds": 332
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 6,
            "milliseconds": 364
          },
          "text": "It is a bit complex."
        },
        {
          "id": "166",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 6,
            "milliseconds": 365
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 10,
            "milliseconds": 765
          },
          "text": "So I will explain in the next video more in detail with the actual code."
        },
        {
          "id": "167",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 11,
            "milliseconds": 533
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 14,
            "milliseconds": 432
          },
          "text": "But the main thing here is to call the decoder"
        },
        {
          "id": "168",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 14,
            "milliseconds": 432
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 17,
            "milliseconds": 698
          },
          "text": "by passing the three things."
        },
        {
          "id": "169",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 17,
            "milliseconds": 700
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 20,
            "milliseconds": 333
          },
          "text": "Decode inputs means decoder inputs,"
        },
        {
          "id": "170",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 21,
            "milliseconds": 0
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 24,
            "milliseconds": 932
          },
          "text": "which should have a wire token predicted in the previous iteration."
        },
        {
          "id": "171",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 26,
            "milliseconds": 133
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 31,
            "milliseconds": 299
          },
          "text": "And as we talked, if it is the first iteration, this would be the start token"
        },
        {
          "id": "172",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 34,
            "milliseconds": 399
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 35,
            "milliseconds": 165
          },
          "text": "of state"
        },
        {
          "id": "173",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 35,
            "milliseconds": 166
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 37,
            "milliseconds": 266
          },
          "text": "is the current of state we discussed."
        },
        {
          "id": "174",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 37,
            "milliseconds": 932
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 41,
            "milliseconds": 331
          },
          "text": "And please note that the recorder of this output,"
        },
        {
          "id": "175",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 41,
            "milliseconds": 399
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 44,
            "milliseconds": 599
          },
          "text": "the updated GRU state"
        },
        {
          "id": "176",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 45,
            "milliseconds": 365
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 47,
            "milliseconds": 732
          },
          "text": "and last but not least features."
        }
      ],
      "source": [
        "It is a bit complex.",
        "So I will explain in the next video more in detail with the actual code.",
        "But the main thing here is to call the decoder",
        "by passing the three things.",
        "Decode inputs means decoder inputs,",
        "which should have a wire token predicted in the previous iteration.",
        "And as we talked, if it is the first iteration, this would be the start token",
        "of state",
        "is the current of state we discussed.",
        "And please note that the recorder of this output,",
        "the updated GRU state",
        "and last but not least features."
      ],
      "result": [
        "这有点复杂。",
        "因此，我将在下一个视频中更详细地解释实际代码。",
        "但这里的主要内容是通过传递三个事物来调用解码器。",
        "",
        "解码输入意味着解码器输入，它应该有一个在上一次迭代中预测的词Token。",
        "",
        "正如我们所说，如果这是第一次迭代，那么这将是开始Token，GRU状态是我们讨论过的当前状态。",
        "",
        "",
        "请注意记录器的这个输出，更新了GRU的状态。",
        "",
        "最后但同样重要的是特征，"
      ],
      "status": "success",
      "errors": [
        "mismatched: 12 vs 11, Mon Jun 26 2023 15:44:29 GMT-0500 (Central Daylight Time)"
      ],
      "mismatched": false,
      "output": {
        "paragraph": "这有点复杂。因此，我将在下一个视频中更详细地解释实际代码。但这里的主要内容是通过传递三个事物来调用解码器。解码输入意味着解码器输入，它应该在上一次迭代中预测到一个线性Token。正如我们所说，如果这是第一次迭代，那么这将是开始Token，状态是我们讨论过的当前状态。请注意，这个输出的记录器，更新后的杰瑞状态，以及最后但并非最不重要的特征。",
        "sentences": [
          {
            "translated": "这有点复杂。",
            "indexes": [
              0
            ]
          },
          {
            "translated": "因此，我将在下一个视频中更详细地解释实际代码。",
            "indexes": [
              1
            ]
          },
          {
            "translated": "但这里的主要内容是通过传递三个事物来调用解码器。",
            "indexes": [
              2
            ]
          },
          {
            "translated": "解码输入意味着解码器输入，它应该在上一次迭代中预测到一个线性Token。",
            "indexes": [
              3,
              4
            ]
          },
          {
            "translated": "正如我们所说，如果这是第一次迭代，那么这将是开始Token，状态是我们讨论过的当前状态。",
            "indexes": [
              5,
              6,
              7
            ]
          },
          {
            "translated": "请注意，这个输出的记录器，更新后的杰瑞状态，以及最后但并非最不重要的特征。",
            "indexes": [
              8,
              9,
              10
            ]
          }
        ]
      }
    },
    {
      "items": [
        {
          "id": "177",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 48,
            "milliseconds": 500
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 52,
            "milliseconds": 565
          },
          "text": "This is the image feature we extracted with the encoder."
        },
        {
          "id": "178",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 52,
            "milliseconds": 566
          },
          "endTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 58,
            "milliseconds": 799
          },
          "text": "By passing them we can get the actual next var the prediction."
        },
        {
          "id": "179",
          "startTime": {
            "hours": 0,
            "minutes": 10,
            "seconds": 58,
            "milliseconds": 799
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 3,
            "milliseconds": 31
          },
          "text": "This is a very simple text generation model from images,"
        },
        {
          "id": "180",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 3,
            "milliseconds": 666
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 6,
            "milliseconds": 565
          },
          "text": "but this kind of iteration is very similar"
        },
        {
          "id": "181",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 6,
            "milliseconds": 932
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 10,
            "milliseconds": 132
          },
          "text": "even in very large language generation models"
        },
        {
          "id": "182",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 10,
            "milliseconds": 432
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 13,
            "milliseconds": 464
          },
          "text": "like Google Board,"
        },
        {
          "id": "183",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 13,
            "milliseconds": 466
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 17,
            "milliseconds": 433
          },
          "text": "they basically predict the next Var, also rigorously in this way,"
        },
        {
          "id": "184",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 17,
            "milliseconds": 865
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 22,
            "milliseconds": 365
          },
          "text": "one by one based on some information and learned knowledge,"
        },
        {
          "id": "185",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 22,
            "milliseconds": 732
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 25,
            "milliseconds": 664
          },
          "text": "which is embedded in the huge number of parameters."
        },
        {
          "id": "186",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 27,
            "milliseconds": 66
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 30,
            "milliseconds": 900
          },
          "text": "In the next video, I will walk you through the entire notebook"
        },
        {
          "id": "187",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 31,
            "milliseconds": 566
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 34,
            "milliseconds": 265
          },
          "text": "and then we will check what kind of captions"
        },
        {
          "id": "188",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 34,
            "milliseconds": 265
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 37,
            "milliseconds": 865
          },
          "text": "this model can generate."
        },
        {
          "id": "189",
          "startTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 37,
            "milliseconds": 865
          },
          "endTime": {
            "hours": 0,
            "minutes": 11,
            "seconds": 42,
            "milliseconds": 732
          },
          "text": "Thank you so much for watching and see you in the next video."
        }
      ],
      "source": [
        "This is the image feature we extracted with the encoder.",
        "By passing them we can get the actual next var the prediction.",
        "This is a very simple text generation model from images,",
        "but this kind of iteration is very similar",
        "even in very large language generation models",
        "like Google Board,",
        "they basically predict the next Var, also rigorously in this way,",
        "one by one based on some information and learned knowledge,",
        "which is embedded in the huge number of parameters.",
        "In the next video, I will walk you through the entire notebook",
        "and then we will check what kind of captions",
        "this model can generate.",
        "Thank you so much for watching and see you in the next video."
      ],
      "result": [
        "这是我们用编码器提取的图像特征。",
        "通过传递它们，我们可以得到实际的下一个变量预测。",
        "这是一个非常简单的从图像生成文本的模型，",
        "但这种迭代在很大的语言生成模型中也非常相似，",
        "",
        "就像Google Board， 他们基本上也是以这种方式预测下一个词，以自回归的方式，",
        "",
        "一个接一个地基于一些信息和学习到的知识，这些知识都嵌入在大量的参数中。",
        "",
        "在下一个视频中，我将带你浏览整个Notebook，",
        "然后我们将检查这个模型可以生成哪些说明文字。",
        "",
        "非常感谢你的观看，我们下一个视频见。"
      ],
      "status": "success",
      "errors": [],
      "mismatched": false,
      "output": {
        "paragraph": "这是我们用编码器提取的图像特征。通过传递它们，我们可以得到实际的下一个变量预测。这是一个非常简单的从图像生成文本的模型，但这种迭代在很大的语言生成模型中也非常相似，比如谷歌Board，它们基本上也是以这种方式严格预测下一个变量，一个接一个地基于一些信息和学到的知识，这些知识嵌入在大量的参数中。在下一个视频中，我将带你浏览整个笔记本，然后我们将检查这个模型可以生成哪些说明文字。非常感谢你的观看，我们下一个视频见。",
        "sentences": [
          {
            "translated": "这是我们用编码器提取的图像特征。",
            "indexes": [
              0
            ]
          },
          {
            "translated": "通过传递它们，我们可以得到实际的下一个变量预测。",
            "indexes": [
              1
            ]
          },
          {
            "translated": "这是一个非常简单的从图像生成文本的模型，",
            "indexes": [
              2
            ]
          },
          {
            "translated": "但这种迭代在很大的语言生成模型中也非常相似，",
            "indexes": [
              3
            ]
          },
          {
            "translated": "比如谷歌Board，它们基本上也是以这种方式严格预测下一个变量，",
            "indexes": [
              4,
              5
            ]
          },
          {
            "translated": "一个接一个地基于一些信息和学到的知识，这些知识嵌入在大量的参数中。",
            "indexes": [
              6,
              7
            ]
          },
          {
            "translated": "在下一个视频中，我将带你浏览整个笔记本，",
            "indexes": [
              8
            ]
          },
          {
            "translated": "然后我们将检查这个模型可以生成哪些说明文字。",
            "indexes": [
              9,
              10
            ]
          },
          {
            "translated": "非常感谢你的观看，我们下一个视频见。",
            "indexes": [
              11,
              12
            ]
          }
        ]
      }
    }
  ],
  "sourcePath": "input/Generative AI learning path/Create Image Captioning Models- Overview.srt",
  "ouputBasePath": "input/Generative AI learning path/Create Image Captioning Models- Overview",
  "totalCost": 0.5670000000000002,
  "translationPath": "input/Generative AI learning path/Create Image Captioning Models- Overview/translation.json"
}
