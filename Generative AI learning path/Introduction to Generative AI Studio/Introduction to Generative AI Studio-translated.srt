1
00:00:00,420 --> 00:00:03,960
欢迎来到生成式AI工作室课程的介绍。

2
00:00:03,960 --> 00:00:09,299
在这个视频中，你将了解什么是生成式AI工作室，并描述其使用选项。

3
00:00:09,300 --> 00:00:13,319
你还可以亲自尝试生成式AI工作室的语言工具。

4
00:00:13,320 --> 00:00:15,420
什么是生成式AI？

5
00:00:15,960 --> 00:00:19,739
它是一种为你生成内容的人工智能类型。

6
00:00:20,399 --> 00:00:22,078
生成的内容有哪些？

7
00:00:22,079 --> 00:00:27,838
嗯，生成的内容可以是多模态的，包括文本、图像、音频和视频。

8
00:00:27,839 --> 00:00:33,478
当给定一个Prompt或请求时，生成式AI可以帮助你完成各种任务，例如

9
00:00:33,479 --> 00:00:42,659
文档摘要、信息提取、代码生成、营销活动创建、虚拟助手和呼叫中心机器人。

11
00:00:42,659 --> 00:00:44,519
这只是其中的一些例子！

12
00:00:44,520 --> 00:00:46,919
AI如何生成新内容？

13
00:00:46,920 --> 00:00:49,979
它从大量现有内容中学习。

14
00:00:49,979 --> 00:00:52,258
这包括文本、音频和视频。

15
00:00:52,259 --> 00:00:58,140
从现有内容中学习的过程称为训练，其结果是创建一个“基础模型”。

17
00:00:58,140 --> 00:01:05,939
LLM（大语言模型），如Bard这样的聊天机器人，是一个典型的基础模型。

19
00:01:05,939 --> 00:01:13,859
基础模型可以用来生成内容并解决一般问题，例如内容提取和文档摘要。

22
00:01:13,859 --> 00:01:22,259
它还可以通过你领域的新数据集进一步训练，以解决特定问题，如财务模型生成和医疗咨询。

24
00:01:22,260 --> 00:01:26,340
这将导致创建一个定制的新模型，以满足你的特定需求。

25
00:01:27,359 --> 00:01:35,939
如何使用基础模型为你的应用程序提供动力，以及如何进一步训练或调整基础模型以解决你特定领域的问题？

27
00:01:35,939 --> 00:01:44,400
Google Cloud 提供了几个易于使用的工具，帮助你在具有或不具有 AI 和机器学习背景的项目中使用生成式 AI。

29
00:01:45,060 --> 00:01:47,280
其中一个工具就是 Vertex AI。

30
00:01:47,819 --> 00:01:55,859
Vertex AI 是一个端到端的 ML 开发平台，位于 Google Cloud 上，帮助你构建部署和管理机器学习模型。

32
00:01:55,859 --> 00:02:01,439
使用 Vertex AI，如果你是一个应用开发者或数据科学家，想要构建一个应用，

34
00:02:01,439 --> 00:02:08,639
你可以使用生成式AI工作室快速原型和定制生成式AI模型，无需编码或低代码。

36
00:02:08,639 --> 00:02:16,199
如果你是一个数据科学家或ML开发者，想要构建和自动化一个生成式AI模型，你可以从Model Garden开始。

38
00:02:16,860 --> 00:02:26,818
Model Garden让你发现和交互Google的基础和第三方开源模型，并内置了MLOps工具，以自动化ML管道。

40
00:02:28,080 --> 00:02:31,320
在本课程中，你将关注生成式 AI 工作室。

41
00:02:32,039 --> 00:02:35,519
生成式 AI 工作室支持语言、视觉和语音。

42
00:02:35,520 --> 00:02:37,799
在学习这门课程时，列表会不断增长。

43
00:02:38,400 --> 00:02:43,199
对于语言，你可以设计一个Prompt来执行任务和调整语言模型。

44
00:02:43,199 --> 00:02:48,239
对于视觉，你可以根据Prompt生成图像并进一步编辑图像。

45
00:02:48,240 --> 00:02:52,440
对于语音，你可以从语音生成文本，反之亦然。

46
00:02:53,099 --> 00:02:56,639
让我们关注在生成式AI工作室中可以用语言做什么。

47
00:02:56,639 --> 00:03:03,778
具体来说，你可以：为与你的业务用例相关的任务设计Prompt，包括代码生成。

49
00:03:04,439 --> 00:03:09,358
通过指定上下文来创建对话，指导模型如何回应。

50
00:03:09,360 --> 00:03:17,699
调整模型以更好地适应你的用例，然后将其部署到端点以获得预测或在Prompt设计中进行测试。

52
00:03:17,699 --> 00:03:20,699
让我们详细介绍这三个功能。

53
00:03:20,699 --> 00:03:22,138
首先是Prompt设计。

54
00:03:22,139 --> 00:03:27,539
要开始尝试大语言模型（LLM），请点击“NEW PROMPT”。

55
00:03:31,680 --> 00:03:37,439
在生成式AI的世界里，Prompt只是输入文本的一个比较酷的名字。

57
00:03:37,439 --> 00:03:41,639
你可以将所需的输入文本（如问题和指令）提供给模型。

58
00:03:41,639 --> 00:03:48,719
模型将根据你设置的Prompt提供响应，因此，你得到的答案取决于你提出的问题。

60
00:03:48,719 --> 00:03:57,959
找出并设计最佳输入文本以获得所需响应的过程称为Prompt设计，这通常需要大量实验。

62
00:03:57,960 --> 00:04:00,180
让我们从一个自由形式的Prompt开始。

63
00:04:00,180 --> 00:04:03,780
设计Prompt的一种方法是简单地告诉模型你想要什么。

64
00:04:03,780 --> 00:04:05,100
换句话说，提供一个指令。

65
00:04:06,000 --> 00:04:11,458
例如，生成我去Joshua Tree国家公园露营旅行所需的物品清单。

67
00:04:11,460 --> 00:04:17,340
我们将这段文字发送给模型，然后...你可以看到模型输出了一个我们不想在露营时缺少的有用物品清单。

70
00:04:18,360 --> 00:04:25,199
这种通过编写单一命令使LLM采用某种行为的方法称为零示例Prompt（Zero-shot prompting）。

72
00:04:25,199 --> 00:04:29,999
通常，有3种方法可以用来按照你期望的方式塑造模型的响应。

74
00:04:30,000 --> 00:04:40,379
零示例Prompt - 是一种方法，即在这种方法中，LLM没有获得关于要执行的特定任务的额外数据，而只是给出了描述任务的Prompt。

77
00:04:40,379 --> 00:04:46,439
例如，如果你想让LLM回答一个问题，你只需在Prompt中写：“什么是Prompt设计？”。

79
00:04:46,439 --> 00:04:52,980
单示例Prompt（One-shot prompting） - 是一种方法，即在这种方法中，给LLM一个要执行任务相关的示例。

81
00:04:52,980 --> 00:04:58,199
例如，如果你想让LLM写一首诗，你可以提供一个诗的示例。

82
00:04:58,199 --> 00:05:06,418
少量示例Prompt（Few-shot prompting） - 是一种方法，即在这种方法中，给LLM几个要执行任务相关的示例。

84
00:05:06,420 --> 00:05:12,839
例如，如果你想让LLM写一篇新闻文章，你可以给它提供一些新闻文章来阅读。

86
00:05:12,839 --> 00:05:20,519
你可以使用结构化模式来设计少量示例Prompt，通过提供上下文和额外的例子让模型学习。

88
00:05:21,360 --> 00:05:24,119
结构化Prompt包含几个不同的组成部分：

89
00:05:24,120 --> 00:05:27,600
首先我们有上下文，它指导模型应该如何回应。

90
00:05:27,600 --> 00:05:34,740
你可以指定模型可以或不能使用的词汇，关注或避免的主题，或者特定的结果格式。

92
00:05:34,740 --> 00:05:37,860
每次向模型发送请求时，上下文都会应用。

93
00:05:37,860 --> 00:05:42,480
假设我们想使用LLM根据一些背景文本回答问题。

94
00:05:42,480 --> 00:05:47,220
在这种情况下，一段描述亚马逊雨林植被变化的文字。

95
00:05:47,220 --> 00:05:50,039
我们可以将背景文本粘贴为上下文。

96
00:05:50,759 --> 00:05:56,759
然后，我们添加一些可以从这段文字中回答的问题示例，比如LGM是什么意思？

98
00:05:56,759 --> 00:06:00,538
或者沉积物分析表明了什么？

99
00:06:00,540 --> 00:06:06,300
我们需要添加这些问题的相应答案，以展示我们希望模型如何回应。

101
00:06:06,300 --> 00:06:10,319
然后，我们可以通过发送一个新问题作为输入来测试我们设计的Prompt。

102
00:06:10,319 --> 00:06:16,019
就这样，你仅用几分钟就搭建了一个基于背景文本问答的问答系统原型！

104
00:06:16,019 --> 00:06:18,719
请注意关于Prompt设计的一些最佳实践。

105
00:06:18,720 --> 00:06:28,319
简洁明了，具体且明确地提问，一次只询问一个任务，将生成任务转化为分类任务。

107
00:06:28,319 --> 00:06:36,419
例如，不要问应该学习哪种编程语言，而是问 Python、Java 或 C 对编程初学者更合适。

109
00:06:36,420 --> 00:06:46,438
通过包含示例来提高返回结果的质量，添加说明和一些示例往往会产生良好的效果，但目前还没有最佳的Prompt编写方法。

112
00:06:46,439 --> 00:06:52,139
你可能需要尝试不同的结构、格式和示例，以找出最适合你使用场景的方法。

114
00:06:52,139 --> 00:06:57,480
有关Prompt设计的更多信息，请查阅阅读列表中的文本Prompt设计。

116
00:06:58,439 --> 00:07:03,179
因此，如果你设计了一个你认为效果很好的Prompt，可以保存它并稍后返回。

118
00:07:03,180 --> 00:07:12,660
你保存的Prompt将显示在Prompt库中，Prompt库是一个精选的示例Prompt集合，展示了生成式 AI 模型如何适用于各种用例。

120
00:07:12,660 --> 00:07:21,898
最后，除了测试不同的Prompt和Prompt结构外，还有一些模型参数可以尝试，以提高返回结果的质量。

122
00:07:22,439 --> 00:07:24,959
首先，你可以选择不同的模型。

123
00:07:24,959 --> 00:07:28,619
每个模型都经过调整，以便在特定任务上表现良好。

124
00:07:28,620 --> 00:07:38,699
你还可以指定temperature、Top P和Top K。这些参数都通过控制输出Token的选择来调整响应的随机性。

126
00:07:38,699 --> 00:07:44,039
当你向模型发送Prompt时，它会生成一个关于接下来可能出现的单词的概率数组。

128
00:07:44,040 --> 00:07:47,879
从这个数组中，我们需要一些策略来决定返回什么。

129
00:07:48,600 --> 00:07:52,560
一个简单的策略可能是在每个时间步选择最可能的单词。

130
00:07:52,560 --> 00:07:56,699
但这种方法可能导致无趣且有时重复的答案。

131
00:07:56,699 --> 00:08:03,540
相反，如果你在模型返回的分布上随机抽样，你可能会得到一些不太可能的返回结果。

133
00:08:03,540 --> 00:08:09,839
通过控制随机性的程度，你可以获得更多意想不到的，有些人可能会说是创造性的返回结果。

135
00:08:09,839 --> 00:08:15,058
回到模型参数，Temperature是用来调整随机性程度的一个数字。

136
00:08:15,060 --> 00:08:19,500
低Temperature：意味着选择那些可能性较高且更可预测的单词。

138
00:08:19,500 --> 00:08:24,179
在这种情况下，这些单词是位于列表开头的花朵和其他单词。

140
00:08:24,180 --> 00:08:31,979
这个设置通常更适合像问答和摘要这样的任务，在这些任务中，你期望得到一个变化较小的“可预测”答案。

142
00:08:31,980 --> 00:08:37,439
…高Temperature：意味着选择那些可能性较低且更不寻常的单词。

145
00:08:37,440 --> 00:08:41,100
在这种情况下，这些都是位于列表末尾的错误和其他词语。

147
00:08:42,000 --> 00:08:46,740
如果你想生成更具创意或出人意料的内容，这个设置很好。

148
00:08:46,740 --> 00:08:54,960
除了调整Temperature，Top K 还可以让模型从可能性最高的前 K 个词中随机返回一个词。

150
00:08:55,740 --> 00:09:02,219
例如，Top 2 意味着你可以从包括花和树在内的前 2 个可能的词中随机选择一个。

152
00:09:02,220 --> 00:09:06,539
这种方法允许其他高分词有被选中的机会。

153
00:09:06,539 --> 00:09:18,059
然而，如果词的概率分布非常偏斜，你有一个非常可能的词，其他所有词都非常不可能，这种方法可能会导致一些奇怪的返回结果。

156
00:09:19,200 --> 00:09:26,699
选择最佳 Top K 值的难度，导致了另一种流行的方法，即动态设置单词简表的大小。

158
00:09:27,360 --> 00:09:32,400
Top P 允许模型从前 P 个概率的词中随机返回一个词。

159
00:09:33,240 --> 00:09:47,158
使用 Top P，你可以从一组总和不超过 P 的可能性中选择。例如，P为0.75意味着你从一组累积概率大于0.75的词中取样。

162
00:09:47,159 --> 00:09:52,199
在这种情况下，它包括三个词：花、树和草药。

163
00:09:52,919 --> 00:10:01,020
这样，单词集的大小可以根据列表中下一个词的概率分布动态增加和减少。

165
00:10:01,740 --> 00:10:06,539
总之，生成式AI工作室为你提供了一些模型参数供你尝试，

166
00:10:06,539 --> 00:10:14,639
如模型、Temperature、Top K和Top P。请注意，你不需要经常调整它们，尤其是Top K和Top P。

168
00:10:16,019 --> 00:10:18,959
现在让我们看看第二个功能，创建对话。

169
00:10:19,740 --> 00:10:22,619
首先，你需要指定对话上下文。

170
00:10:22,620 --> 00:10:25,499
上下文指示模型应如何回应。

171
00:10:26,039 --> 00:10:33,179
例如，指定模型可以或不能使用的词汇，关注或避免的主题，或响应格式。

173
00:10:33,179 --> 00:10:36,538
每次向模型发送请求时，都会应用上下文。

174
00:10:37,139 --> 00:10:42,839
举个简单的例子，你可以定义一个场景，告诉AI如何回应技术支持的查询。

176
00:10:42,840 --> 00:10:44,279
你的名字叫Roy。

177
00:10:44,279 --> 00:10:47,219
你是IT部门的一名支持技术员。

178
00:10:47,220 --> 00:10:52,200
对于任何查询你只回应“你尝试过关机重启了吗？”

180
00:10:52,200 --> 00:10:56,159
你可以调整右侧的参数，就像设计Prompt时一样。

181
00:10:56,159 --> 00:11:01,859
要查看它的工作原理，你可以在聊天框中输入我的电脑很慢，然后按回车。

182
00:11:01,860 --> 00:11:06,299
AI回应：你尝试过关机重启了吗？

183
00:11:06,299 --> 00:11:08,339
正如你告诉AI的那样。

184
00:11:09,419 --> 00:11:15,058
酷的是，谷歌提供了API和SDK，帮助你构建自己的应用程序。

186
00:11:15,059 --> 00:11:17,338
你可以简单地点击查看代码。

187
00:11:17,340 --> 00:11:23,579
首先，你需要下载适合你编程语言的 Vertex AI SDK，比如Python 和 Curl。

189
00:11:23,580 --> 00:11:26,639
SDK 是软件设计工具包（Software Design Kits）的缩写。

190
00:11:26,639 --> 00:11:29,398
它们实现了功能并为你完成工作。

191
00:11:29,399 --> 00:11:32,218
你可以像调用代码中的库一样使用它们。

192
00:11:32,220 --> 00:11:37,080
然后按照示例代码和 API，将代码插入到你的应用程序中。

193
00:11:38,279 --> 00:11:41,399
现在让我们看看第三个功能，调整（Tune）语言模型。

194
00:11:41,399 --> 00:11:45,538
如果你一直在使用大语言模型进行原型设计，你可能会想知道是否有

195
00:11:45,539 --> 00:11:48,719
一种方法可以在Prompt设计之外提高响应质量。

196
00:11:48,720 --> 00:11:54,538
那么让我们学习如何调整大语言模型以及如何从生成 AI 工作室启动调优任务。

198
00:11:54,539 --> 00:11:58,859
简要回顾一下，Prompt是你传递给模型的文本输入。

199
00:11:58,860 --> 00:12:00,659
你的Prompt可能看起来像一条指令...

200
00:12:00,659 --> 00:12:02,519
也许你还添加了一些例子...

201
00:12:02,519 --> 00:12:06,959
然后你将这段文本发送给模型，以便它采用你想要的行为。

202
00:12:09,059 --> 00:12:12,539
Prompt设计允许快速实验和定制。

203
00:12:12,539 --> 00:12:17,940
而且因为你不需要编写复杂的代码，所以你不需要成为机器学习专家就可以开始。

205
00:12:18,539 --> 00:12:20,339
但是生成Prompt可能会很棘手。

206
00:12:20,340 --> 00:12:25,860
用词或词序的微小变化可能会以无法完全预测的方式影响模型结果。

208
00:12:25,860 --> 00:12:28,980
而且你不能在一个Prompt中放入太多的例子。

209
00:12:28,980 --> 00:12:36,240
即使你找到了一个适合你用例的好Prompt，你可能会注意到模型响应的质量并不完全一致。

211
00:12:36,240 --> 00:12:39,840
我们可以做的一件事是调整模型。

212
00:12:39,840 --> 00:12:41,100
那么什么是调优（Tuning）呢？

213
00:12:41,100 --> 00:12:44,700
你可能熟悉的一个版本是微调。

214
00:12:44,700 --> 00:12:49,019
在这种情况下，我们使用一个在通用数据集上预训练的模型。

215
00:12:49,019 --> 00:12:50,639
我们复制这个模型。

216
00:12:50,639 --> 00:12:57,240
然后，以这些学到的权重为起点，我们在一个新的领域特定数据集上重新训练模型。

218
00:12:57,240 --> 00:13:01,200
这种技术在很多不同的用例中都非常有效。

219
00:13:01,200 --> 00:13:04,919
但是当我们尝试微调大语言模型时，我们遇到了一些挑战。

220
00:13:04,919 --> 00:13:08,338
正如名称所示，大语言模型是很大的。

221
00:13:08,340 --> 00:13:11,519
因此，更新每个权重可能需要一个长时间的训练任务。

222
00:13:11,519 --> 00:13:16,860
再加上运行这个巨大模型的成本和麻烦…

224
00:13:16,860 --> 00:13:21,419
因此，微调大语言模型可能不是最佳选择。

225
00:13:21,419 --> 00:13:26,159
但是有一种创新的调优方法叫做参数高效调优。

226
00:13:26,159 --> 00:13:33,899
这是一个非常令人兴奋的研究领域，旨在减少微调大语言模型的挑战，只训练一部分参数。

228
00:13:33,899 --> 00:13:37,678
这些参数可能是现有模型参数的一个子集。

229
00:13:37,679 --> 00:13:40,438
或者它们可能是一整套新的参数。

230
00:13:40,440 --> 00:13:46,559
例如，你可以在模型中添加一些额外的层或者在Prompt中添加一个额外的Embedding。

232
00:13:46,559 --> 00:13:53,459
如果你想了解更多关于参数高效调整和一些不同方法的信息，可以参考本课程阅读列表中的总结论文。

235
00:13:53,460 --> 00:13:59,759
但如果你只想开始构建，那么让我们转到生成式AI工作室，看看如何开始调整工作。

237
00:13:59,759 --> 00:14:03,478
从生成式AI工作室的语言部分，选择TUNING。

239
00:14:03,480 --> 00:14:05,700
要创建一个调整过的模型，我们需要提供一个名称。

240
00:14:05,700 --> 00:14:09,779
然后指向你的训练数据的本地或云存储位置。

241
00:14:09,779 --> 00:14:18,240
参数高效调整非常适合于你拥有适量训练数据的场景，比如说几百或者几千个训练样本。

243
00:14:18,960 --> 00:14:24,299
你的训练数据应该以文本到文本格式构建为一个监督式训练数据集。

245
00:14:24,299 --> 00:14:31,860
数据中的每条记录或行都包含输入文本，换句话说，是Prompt，后面跟着模型的预期输出。

247
00:14:31,860 --> 00:14:37,319
这意味着该模型可以针对可建模为文本到文本的任务进行调整。

249
00:14:38,039 --> 00:14:45,179
指定数据集路径后，你可以开始调优任务并在谷歌云控制台中监控状态。

251
00:14:45,179 --> 00:14:50,278
调优任务完成后，你将在Vertex AI模型注册表中看到调优后的模型。

252
00:14:50,279 --> 00:14:55,198
你可以将其部署到端点进行服务，或者在生成式AI工作室中进行测试。

253
00:14:56,220 --> 00:15:04,139
在本课程中，你了解了生成式AI是什么以及谷歌云提供的工具。

255
00:15:04,139 --> 00:15:07,259
具体来说，你关注了生成式AI工作室。

256
00:15:07,259 --> 00:15:14,278
通过快速原型设计和定制生成式AI模型，在你的应用程序中使用GenAI。

258
00:15:14,279 --> 00:15:19,918
你了解到生成式AI工作室支持三个选项：语言、视觉和语音。

259
00:15:19,919 --> 00:15:26,698
然后，你浏览了语言中的三个主要功能：设计和测试Prompt、创建对话和调整模型。

261
00:15:27,240 --> 00:15:31,679
这是一个关于Vertex AI上生成式AI工作室的简短课程。

262
00:15:31,679 --> 00:15:45,539
有关自然语言处理和不同类型的语言模型（如解码器-编码器、变压器和LLM）的更多信息，请查看阅读列表中的名为“谷歌云上的自然语言处理”的课程。

265
00:15:46,440 --> 00:15:50,399
现在是时候在动手实验室中尝试生成式AI工作室了，你将：

266
00:15:50,399 --> 00:15:53,818
在自由形式和结构化模式下设计和测试Prompt。

267
00:15:53,820 --> 00:15:57,779
创建对话，浏览Prompt库。

269
00:15:57,779 --> 00:16:04,200
在这个实验室的最后，你将能够使用我们在这门课程中讨论过的生成式AI工作室的功能。

271
00:16:04,200 --> 00:16:05,879
尽情探索吧！
