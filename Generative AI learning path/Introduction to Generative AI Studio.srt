1
00:00:00,420 --> 00:00:03,960
Welcome to the Introduction to the Generative
AI Studio course.

2
00:00:03,960 --> 00:00:09,299
In this video, you learn what Generative AI
Studio is and describe its options for use.

3
00:00:09,300 --> 00:00:13,319
You also demo the Generative AI Studio’s
language tool yourself.

4
00:00:13,320 --> 00:00:15,420
What is Generative AI?

5
00:00:15,960 --> 00:00:19,739
It is a type of artificial intelligence that
generates content for you.

6
00:00:20,399 --> 00:00:22,078
What kind of content?

7
00:00:22,079 --> 00:00:27,838
Well, the generated content can be multi-modal,
including text, images, audio, and video.

8
00:00:27,839 --> 00:00:33,478
When given a prompt or a request, Generative
AI can help you achieve various tasks, such

9
00:00:33,479 --> 00:00:39,778
as document summarization, information extraction,
code generation, marketing campaign creation,

10
00:00:39,780 --> 00:00:42,659
virtual assistance, and call center bot.

11
00:00:42,659 --> 00:00:44,519
And these are just a few examples!

12
00:00:44,520 --> 00:00:46,919
How does AI generate new content?

13
00:00:46,920 --> 00:00:49,979
It learns from a massive amount of existing
content.

14
00:00:49,979 --> 00:00:52,258
This includes text, audio and video.

15
00:00:52,259 --> 00:00:57,058
The process of learning from existing content
is called training, which results in the creation

16
00:00:57,060 --> 00:00:58,140
of a “foundation model.”

17
00:00:58,140 --> 00:01:04,859
An LLM, or large language model, which powers
chat bots like Bard, is a typical example

18
00:01:04,859 --> 00:01:05,939
of a foundation model.

19
00:01:05,939 --> 00:01:07,378
The foundation model

20
00:01:07,379 --> 00:01:12,419
can then be used to generate content and solve
general problems, such as content extraction

21
00:01:12,420 --> 00:01:13,859
and document summarization.

22
00:01:13,859 --> 00:01:18,599
It can also be trained further with new datasets
in your field to solve specific problems,

23
00:01:18,599 --> 00:01:22,259
such as financial model generation and healthcare
consulting.

24
00:01:22,260 --> 00:01:26,340
This results in the creation of a new model
that is tailored to your specific needs.

25
00:01:27,359 --> 00:01:31,979
How can you use the foundation model to power
your applications, and how can you further

26
00:01:31,980 --> 00:01:35,939
train, or tune, the foundation model to solve
a problem in your specific field?

27
00:01:35,939 --> 00:01:41,278
Google Cloud provides several easy-to-use
tools that help you use generative AI in your

28
00:01:41,280 --> 00:01:44,400
projects with or without an AI and machine
learning background.

29
00:01:45,060 --> 00:01:47,280
One such tool is Vertex AI.

30
00:01:47,819 --> 00:01:53,278
Vertex AI is an end-to-end ML development
platform on Google Cloud that helps you build,

31
00:01:53,280 --> 00:01:55,859
deploy, and manage machine learning models.

32
00:01:55,859 --> 00:01:59,039
With Vertex AI, if you are an app developer 

33
00:01:59,040 --> 00:02:01,439
or data scientist and want 
to build an application,

34
00:02:01,439 --> 00:02:07,139
you can use Generative AI Studio to quickly
prototype and customize generative AI models

35
00:02:07,140 --> 00:02:08,639
with no code or low code.

36
00:02:08,639 --> 00:02:13,738
If you are a data scientist or ML developer
who wants to build and automate a generative

37
00:02:13,740 --> 00:02:16,199
AI model, you can start from Model Garden.

38
00:02:16,860 --> 00:02:21,599
Model Garden lets you discover and interact
with Google’s foundation and third-party

39
00:02:21,599 --> 00:02:26,818
open source models and has built-in MLOps
tools to automate the ML pipeline.

40
00:02:28,080 --> 00:02:31,320
In this course, you focus on Generative AI
Studio.

41
00:02:32,039 --> 00:02:35,519
Generative AI Studio supports language, vision,
and speech.

42
00:02:35,520 --> 00:02:37,799
The list grows as you are learning this course.

43
00:02:38,400 --> 00:02:43,199
For language, you can design a prompt to perform
tasks and tune language models.

44
00:02:43,199 --> 00:02:48,239
For vision, you can generate an image based
on a prompt and further edit the image.

45
00:02:48,240 --> 00:02:52,440
For speech, you can generate text from speech
or vice versa.

46
00:02:53,099 --> 00:02:56,639
Let’s focus on what you can do with Language
in Generative AI Studio.

47
00:02:56,639 --> 00:03:01,438
specifically, you can:
Design prompts for tasks relevant to your

48
00:03:01,439 --> 00:03:03,778
business use case including code generation.

49
00:03:04,439 --> 00:03:09,358
Create conversations by specifying the context
that instructs how the model should respond.

50
00:03:09,360 --> 00:03:14,159
And tune a model so it is better equipped
for your use case, which allows you to then

51
00:03:14,159 --> 00:03:17,699
deploy it to an endpoint to get predictions
or test it in prompt design.

52
00:03:17,699 --> 00:03:20,699
Let’s walk through these three features
in detail.

53
00:03:20,699 --> 00:03:22,138
First is prompt design.

54
00:03:22,139 --> 00:03:27,539
To get started experimenting with large language
models, or LLMs, click on NEW PROMPT.

55
00:03:31,680 --> 00:03:36,539
In the world of Generative AI, a prompt is
just a fancy name for the input text that

56
00:03:36,539 --> 00:03:37,439
you feed to your model.

57
00:03:37,439 --> 00:03:41,639
You can feed your desired input text like
questions and instructions to the model.

58
00:03:41,639 --> 00:03:46,079
The model will then provide a response based
on how you structured your prompt, therefore,

59
00:03:46,080 --> 00:03:48,719
the answers you get depend on the questions
you ask.

60
00:03:48,719 --> 00:03:53,579
The process of figuring out and designing
the best input text to get the desired response

61
00:03:53,580 --> 00:03:57,959
back from the model is called Prompt Design,
which often involves a lot of experimentation.

62
00:03:57,960 --> 00:04:00,180
Let’s start with a free-form prompt.

63
00:04:00,180 --> 00:04:03,780
One way to design a prompt is to simply tell
the model what you want.

64
00:04:03,780 --> 00:04:05,1000
In other words, provide an instruction.

65
00:04:06,000 --> 00:04:07,439
For example,

66
00:04:07,439 --> 00:04:11,458
Generate a list of items I need for a camping
trip to Joshua Tree National Park.

67
00:04:11,460 --> 00:04:13,139
We send this text to the model,

68
00:04:13,139 --> 00:04:17,038
And…you can see that the model outputs a
useful list of items we don’t want to camp

69
00:04:17,040 --> 00:04:17,340
without.

70
00:04:18,360 --> 00:04:23,399
This approach of writing a single command
so that the LLM can adopt a certain behavior,

71
00:04:23,399 --> 00:04:25,199
is called zero shot prompting.

72
00:04:25,199 --> 00:04:29,399
Generally, there are 3 methods that you can
use to shape the model's response in a way

73
00:04:29,399 --> 00:04:29,999
that you desire.

74
00:04:30,000 --> 00:04:35,459
Zero-shot prompting - is a method where the
LLM is given no additional data on the specific

75
00:04:35,459 --> 00:04:37,138
task that it is being asked to perform.

76
00:04:37,139 --> 00:04:40,379
Instead, it is only given a prompt that describes
the task.

77
00:04:40,379 --> 00:04:46,258
For example, if you want the LLM to answer
a question, you just prompt "what is prompt

78
00:04:46,259 --> 00:04:46,439
design?".

79
00:04:46,439 --> 00:04:51,719
One-shot prompting - is a method where the
LLM is given a single example of the task

80
00:04:51,720 --> 00:04:52,980
that it is being asked to perform.

81
00:04:52,980 --> 00:04:58,199
For example, if you want the LLM to write
a poem, you might provide a single example

82
00:04:58,199 --> 00:05:04,139
poem. and Few-shot prompting - is a method
where the LLM is given a small number of examples

83
00:05:04,139 --> 00:05:06,418
of the task that it is being asked to perform.

84
00:05:06,420 --> 00:05:11,1000
For example, if you want the LLM to write
a news article, you might give it a few news

85
00:05:12,000 --> 00:05:12,839
articles to read.

86
00:05:12,839 --> 00:05:17,879
You can use the structured mode to design
the few-shot prompting by providing a context

87
00:05:17,879 --> 00:05:20,519
and additional examples for the model to learn
from.

88
00:05:21,360 --> 00:05:24,119
The structured prompt contains a few different
components:

89
00:05:24,120 --> 00:05:27,600
First we have the context, which instructs
how the model should respond.

90
00:05:27,600 --> 00:05:33,720
You can specify words the model can or cannot
use, topics to focus on or avoid, or a particular

91
00:05:33,720 --> 00:05:34,740
response format.

92
00:05:34,740 --> 00:05:37,860
And the context applies each time you send
a request to the model.

93
00:05:37,860 --> 00:05:42,480
Let’s say we want to use an LLM to answer
questions based on some background text.

94
00:05:42,480 --> 00:05:47,220
In this case, a passage that describes changes
in rainforest vegetation in the Amazon.

95
00:05:47,220 --> 00:05:50,039
We can paste in the background text as the
context.

96
00:05:50,759 --> 00:05:54,479
Then, we add some examples of questions that
could be answered from this passage

97
00:05:54,480 --> 00:05:56,759
Like what does LGM stand for?

98
00:05:56,759 --> 00:06:00,538
Or what did the analysis from the sediment
deposits indicate?

99
00:06:00,540 --> 00:06:05,219
We’ll need to add in the corresponding answers
to these questions, to demonstrate how we

100
00:06:05,220 --> 00:06:06,300
want the model to respond.

101
00:06:06,300 --> 00:06:10,319
Then, we can test out the prompt we’ve designed
by sending a new question as input.

102
00:06:10,319 --> 00:06:15,118
And there you go, you’ve prototyped a q&a
system based on background text in just a

103
00:06:15,120 --> 00:06:16,019
few minutes!

104
00:06:16,019 --> 00:06:18,719
Please note a few best practices around prompt
design.

105
00:06:18,720 --> 00:06:26,579
Be concise Be specific and well-defined Ask
one task at a time Turn generative tasks into

106
00:06:26,579 --> 00:06:28,319
classification tasks.

107
00:06:28,319 --> 00:06:33,778
For example, instead of asking what programming
language to learn, ask if Python, Java, or

108
00:06:33,779 --> 00:06:36,419
C is a better fit for a beginner in programming.

109
00:06:36,420 --> 00:06:39,420
and Improve response quality by including
examples

110
00:06:40,019 --> 00:06:44,759
Adding instructions and a few examples tends
to yield good results however there’s currently

111
00:06:44,759 --> 00:06:46,438
no one best way to write a prompt.

112
00:06:46,439 --> 00:06:50,879
You may need to experiment with different
structures, formats, and examples to see what

113
00:06:50,879 --> 00:06:52,139
works best for your use case.

114
00:06:52,139 --> 00:06:57,299
For more information about prompt design,
please check text prompt design in the reading

115
00:06:57,300 --> 00:06:57,480
list.

116
00:06:58,439 --> 00:07:02,399
So if you designed a prompt that you think
is working pretty well, you can save it and

117
00:07:02,399 --> 00:07:03,179
return to it later.

118
00:07:03,180 --> 00:07:07,920
Your saved prompt will be visible in the prompt
gallery, which is a curated collection of

119
00:07:07,920 --> 00:07:12,660
sample prompts that show how generative AI
models can work for a variety of use cases.

120
00:07:12,660 --> 00:07:17,639
Finally, in addition to testing different
prompts and prompt structures, there are a

121
00:07:17,639 --> 00:07:21,898
few model parameters you can experiment with
to try to improve the quality of responses.

122
00:07:22,439 --> 00:07:24,959
First, there are different models you can
choose from.

123
00:07:24,959 --> 00:07:28,619
Each model is tuned to perform well on specific
tasks.

124
00:07:28,620 --> 00:07:34,560
You can also specify the temperature, top
P, and top K. These parameters all adjust

125
00:07:34,560 --> 00:07:38,699
the randomness of responses by controlling
how the output tokens are selected.

126
00:07:38,699 --> 00:07:43,319
When you send a prompt to the model, it produces
an array of probabilities over the words that

127
00:07:43,319 --> 00:07:44,039
could come next.

128
00:07:44,040 --> 00:07:47,879
And from this array, we need some strategy
to decide what to return.

129
00:07:48,600 --> 00:07:52,560
A simple strategy might be to select the most
likely word at every timestep.

130
00:07:52,560 --> 00:07:56,699
But this method can result in uninteresting
and sometimes repetitive answers.

131
00:07:56,699 --> 00:08:01,979
On the contrary, if you randomly sample over
the distribution returned by the model, you

132
00:08:01,980 --> 00:08:03,540
might get some unlikely responses.

133
00:08:03,540 --> 00:08:08,519
By controlling the degree of randomness, you
can get more unexpected, and some might say

134
00:08:08,519 --> 00:08:09,839
creative, responses.

135
00:08:09,839 --> 00:08:15,058
Back to the model parameters, temperature
is a number used to tune the degree of randomness.

136
00:08:15,060 --> 00:08:18,180
Low temperature:
Means to select the words that are highly

137
00:08:18,180 --> 00:08:19,500
possible and more predictable.

138
00:08:19,500 --> 00:08:23,879
In this case, those are flowers and the other
words that are located at the beginning of

139
00:08:23,879 --> 00:08:24,179
the list.

140
00:08:24,180 --> 00:08:29,459
This setting is generally better for tasks
like q&a and summarization where you expect

141
00:08:29,459 --> 00:08:31,979
a more “predictable” answer with less
variation.

142
00:08:31,980 --> 00:08:33,240
…
High temperature:

143
00:08:33,240 --> 00:08:35,999
Means to select the words 
that have low possibility 

144
00:08:36,000 --> 00:08:37,439
and are more unusual.

145
00:08:37,440 --> 00:08:41,699
In this case, those are bugs and the other
words that that are located at the end of

146
00:08:41,700 --> 00:08:41,1000
the list.

147
00:08:42,000 --> 00:08:46,740
This setting is good if you want to generate
more “creative” or unexpected content.

148
00:08:46,740 --> 00:08:51,839
In addition to adjusting the temperature,
top K lets the model randomly return a word

149
00:08:51,840 --> 00:08:54,960
from the top K number of words in terms of
possibility.

150
00:08:55,740 --> 00:09:01,139
For example, top 2 means you get a random
word from the top 2 possible words including

151
00:09:01,139 --> 00:09:02,219
flowers and trees.

152
00:09:02,220 --> 00:09:06,539
This approach allows the other high-scoring
word a chance of being selected.

153
00:09:06,539 --> 00:09:11,819
However, if the probability distribution of
the words is highly skewed and you have one

154
00:09:11,820 --> 00:09:16,559
word that is very likely and everything else
is very unlikely, this approach can result

155
00:09:16,559 --> 00:09:18,059
in some strange responses.

156
00:09:19,200 --> 00:09:24,179
The difficulty of selecting the best top-k
value, leads to another popular approach that

157
00:09:24,179 --> 00:09:26,699
dynamically sets the size of the shortlist
of words.

158
00:09:27,360 --> 00:09:32,400
Top P allows the model to randomly return
a word from the top P probability of words.

159
00:09:33,240 --> 00:09:38,220
With top P, you choose from a set of words
with the sum of the likelihoods not exceeding

160
00:09:38,220 --> 00:09:44,879
P. For example, p of 0.75 means you sample
from a set of words that have a cumulative

161
00:09:44,879 --> 00:09:47,158
probability greater than 0.75.

162
00:09:47,159 --> 00:09:52,199
In this case, it includes three words: flowers,
trees, and herbs.

163
00:09:52,919 --> 00:09:57,838
This way, the size of the set of words can
dynamically increase and decrease according

164
00:09:57,840 --> 00:10:01,020
to the probability distribution of the next
word on the list.

165
00:10:01,740 --> 00:10:06,539
In sum, Generative AI Studio provides a few
model parameters for you to play with such

166
00:10:06,539 --> 00:10:11,819
as the model, temperature, top K, and top
P. Note that, you are not required to adjust

167
00:10:11,820 --> 00:10:14,639
them constantly, especially top k and top
p.

168
00:10:16,019 --> 00:10:18,959
Now let’s look at the second feature, which
creates conversations.

169
00:10:19,740 --> 00:10:22,619
First, you need to specify the conversation
context.

170
00:10:22,620 --> 00:10:25,499
Context instructs how the model should respond.

171
00:10:26,039 --> 00:10:31,979
For example, specifying words the model can
or cannot use, topics to focus on or avoid,

172
00:10:31,980 --> 00:10:33,179
or response format.

173
00:10:33,179 --> 00:10:36,538
Context applies each time you send a request
to the model.

174
00:10:37,139 --> 00:10:42,299
For a simple example, you can define a scenario
and tell the AI how to respond to help desk

175
00:10:42,299 --> 00:10:42,839
queries.

176
00:10:42,840 --> 00:10:44,279
Your name is Roy.

177
00:10:44,279 --> 00:10:47,219
You are a support technician of an IT department.

178
00:10:47,220 --> 00:10:51,239
You only respond with "Have you tried turning
it off and on again?"

179
00:10:51,240 --> 00:10:52,200
to any queries.

180
00:10:52,200 --> 00:10:56,159
You can tune the parameters on the right,
the same as you do when designing the prompt.

181
00:10:56,159 --> 00:11:01,859
To to see how it works, you can type My computer
is slow in the chat box and press enter.

182
00:11:01,860 --> 00:11:06,299
The AI responds: Have you tried turning it
off and on again?

183
00:11:06,299 --> 00:11:08,339
Exactly as you told the AI to do.

184
00:11:09,419 --> 00:11:11,758
The cool thing is that Google provides the 

185
00:11:11,759 --> 00:11:15,058
APIs and SDKs to help you 
build your own application.

186
00:11:15,059 --> 00:11:17,338
You can simply click view code.

187
00:11:17,340 --> 00:11:22,559
First, you need to download the Vertex AI
SDKs that fit your programming language, like

188
00:11:22,559 --> 00:11:23,579
Python and Curl.

189
00:11:23,580 --> 00:11:26,639
SDK stands for software design kits.

190
00:11:26,639 --> 00:11:29,398
They implement the functions and do the job
for you.

191
00:11:29,399 --> 00:11:32,218
You can use them like you call libraries from
the code.

192
00:11:32,220 --> 00:11:37,080
You then follow the sample code and the API,
and insert the code into your application.

193
00:11:38,279 --> 00:11:41,399
Now let’s look at the third feature, tune
a language model.

194
00:11:41,399 --> 00:11:45,538
If you’ve been prototyping with large language
models, you might be wondering if there’s

195
00:11:45,539 --> 00:11:48,719
a way you can improve the quality of responses
beyond just prompt design.

196
00:11:48,720 --> 00:11:53,399
So let’s learn how to tune a large language
model and how to launch a tuning job from

197
00:11:53,399 --> 00:11:54,538
Generative AI Studio.

198
00:11:54,539 --> 00:11:58,859
As a quick recap, the prompt is your text
input that you pass to the model.

199
00:11:58,860 --> 00:12:00,659
Your prompt might look like an instruction…

200
00:12:00,659 --> 00:12:02,519
And maybe you add some examples…

201
00:12:02,519 --> 00:12:06,959
Then you send this text to the model so that
it adopts the behavior that you want.

202
00:12:09,059 --> 00:12:12,539
Prompt design allows for fast experimentation
and customization.

203
00:12:12,539 --> 00:12:17,098
And because you’re not writing any complicated
code, you don’t need to be an ML expert

204
00:12:17,100 --> 00:12:17,940
to get started.

205
00:12:18,539 --> 00:12:20,339
But producing prompts can be tricky.

206
00:12:20,340 --> 00:12:25,080
Small changes in wording or word order can
affect the model results in ways that aren’t

207
00:12:25,080 --> 00:12:25,860
totally predictable.

208
00:12:25,860 --> 00:12:28,980
And you can’t really fit all that many examples
into a prompt.

209
00:12:28,980 --> 00:12:33,720
Even when you do discover a good prompt for
your use case, you might notice the quality

210
00:12:33,720 --> 00:12:36,240
of model responses isn’t totally consistent.

211
00:12:36,240 --> 00:12:39,840
One thing we can do to alleviate these issues
is to tune the model.

212
00:12:39,840 --> 00:12:41,100
So what’s tuning?

213
00:12:41,100 --> 00:12:44,700
Well, one version you might be familiar with
is fine-tuning.

214
00:12:44,700 --> 00:12:49,019
In this scenario, we take a model that was
pretrained on a generic dataset.

215
00:12:49,019 --> 00:12:50,639
We make a copy of this model.

216
00:12:50,639 --> 00:12:55,079
Then, using those learned weights as a starting
point, we re-train the model

217
00:12:55,080 --> 00:12:57,240
on a new domain-specific dataset.

218
00:12:57,240 --> 00:13:01,200
This technique has been pretty effective for
lots of different use cases.

219
00:13:01,200 --> 00:13:04,919
But when we try to fine tune LLMs, we run
into some challenges.

220
00:13:04,919 --> 00:13:08,338
LLMs are, as the name suggests, large.

221
00:13:08,340 --> 00:13:11,519
So updating every weight can take a long training
job.

222
00:13:11,519 --> 00:13:16,438
Compound all of that computation with the
hassle and cost of now having to serve this

223
00:13:16,440 --> 00:13:16,860
giant model…

224
00:13:16,860 --> 00:13:21,419
And as a result, fine-tuning a large language
model might not be the best option for you.

225
00:13:21,419 --> 00:13:26,159
But there’s an innovative approach to tuning
called parameter-efficient tuning.

226
00:13:26,159 --> 00:13:30,838
This is a super exciting research area that
aims to reduce the challenges of fine-tuning

227
00:13:30,840 --> 00:13:33,899
LLMs, by only training a subset of parameters.

228
00:13:33,899 --> 00:13:37,678
These parameters might be a subset of the
existing model parameters.

229
00:13:37,679 --> 00:13:40,438
Or they could be an entirely 
new set of parameters.

230
00:13:40,440 --> 00:13:45,959
For example, maybe you add on some additional
layers to the model or an extra embedding

231
00:13:45,960 --> 00:13:46,559
to the prompt.

232
00:13:46,559 --> 00:13:48,958
If you want to learn more 
about parameter-efficient 

233
00:13:48,960 --> 00:13:50,700
tuning and some of the different methods,

234
00:13:50,700 --> 00:13:53,459
a summary paper is included in the reading
list of this course.

235
00:13:53,460 --> 00:13:58,200
But if you just want to get to building, then
let's move to Generative AI Studio and see

236
00:13:58,200 --> 00:13:59,759
how to start a tuning job.

237
00:13:59,759 --> 00:14:02,279
From the language section of Generative AI
Studio,

238
00:14:02,279 --> 00:14:03,478
Select TUNING.

239
00:14:03,480 --> 00:14:05,700
To create a tuned model, we provide a name.

240
00:14:05,700 --> 00:14:09,779
Then point to the local or Cloud Storage location
of your training data.

241
00:14:09,779 --> 00:14:14,578
Parameter efficient tuning is ideally suited
for scenarios where you have "modest" amounts

242
00:14:14,580 --> 00:14:18,240
of training data, say hundreds or maybe thousands
of training examples.

243
00:14:18,960 --> 00:14:23,519
Your training data should be structured as
a supervised training dataset in a text to

244
00:14:23,519 --> 00:14:24,299
text format.

245
00:14:24,299 --> 00:14:29,218
Each record or row in the data will contain
the input text, in other words, the prompt,

246
00:14:29,220 --> 00:14:31,860
which is followed by the expected output of
the model.

247
00:14:31,860 --> 00:14:36,899
This means that the model can be tuned for
a task that can be modeled as a text-to-text

248
00:14:36,899 --> 00:14:37,319
problem.

249
00:14:38,039 --> 00:14:42,838
After specifying the path to your dataset,
you can start the tuning job and monitor the

250
00:14:42,840 --> 00:14:45,179
status in the Google Cloud console.

251
00:14:45,179 --> 00:14:50,278
When the tuning job completes, you’ll see
the tuned model in the Vertex AI Model Registry

252
00:14:50,279 --> 00:14:55,198
and you can deploy it to an endpoint for serving,
or you can test it in the Generative AI Studio.

253
00:14:56,220 --> 00:15:00,720
In this course, you learned what Generative
AI is and the tools provided by Google Cloud

254
00:15:00,720 --> 00:15:04,139
to empower your project with Generative AI
capabilities.

255
00:15:04,139 --> 00:15:07,259
Specifically, you focused on Generative AI 

256
00:15:07,259 --> 00:15:10,379
Studio, where you can use 
genAI in your application

257
00:15:10,379 --> 00:15:14,278
by quickly prototyping and customizing generative
AI models.

258
00:15:14,279 --> 00:15:19,918
You learned that Generative AI Studio supports
three options: language, vision, and speech.

259
00:15:19,919 --> 00:15:24,779
You then walked through the three major features
in Language: design and test prompt, create

260
00:15:24,779 --> 00:15:26,698
conversations, and tune models.

261
00:15:27,240 --> 00:15:31,679
This was a short lesson introducing Generative
AI studio on Vertex AI.

262
00:15:31,679 --> 00:15:36,299
For more information about natural language
processing and different types of language

263
00:15:36,299 --> 00:15:42,119
models like decoder-encoder, transformer,
and LLM, please check the course titled Natural

264
00:15:42,120 --> 00:15:45,539
Language Processing on Google Cloud listed
in the reading list.

265
00:15:46,440 --> 00:15:50,399
Now it’s time to play with Generative AI
Studio in a hands-on lab, where you:

266
00:15:50,399 --> 00:15:53,818
Design and test prompts in both free-form
and structured modes.

267
00:15:53,820 --> 00:15:55,679
Create conversations.

268
00:15:55,679 --> 00:15:57,779
And explore the prompt gallery.

269
00:15:57,779 --> 00:16:02,578
By the end of this lab, you will be able to
use the capabilities of Generative AI Studio

270
00:16:02,580 --> 00:16:04,200
that we’ve discussed in this course.

271
00:16:04,200 --> 00:16:05,879
Have fun exploring!

