1
00:00:00,080 --> 00:00:04,330
大家好，欢迎来到《负责任的人工智能入门》。

2
00:00:04,330 --> 00:00:07,189
这门课程将帮助你：

3
00:00:07,190 --> 00:00:11,000
了解为什么谷歌制定了AI原则。

4
00:00:11,000 --> 00:00:16,618
认识到一个组织内负责任AI实践的必要性。

5
00:00:16,618 --> 00:00:21,629
意识到项目各个阶段的决策对负责任AI的影响。

6
00:00:21,629 --> 00:00:29,598
并且，认识到组织可以根据自己的业务需求和价值观设计AI。

7
00:00:29,600 --> 00:00:34,359
我们中的许多人已经与人工智能（或AI）进行了日常互动，

8
00:00:34,359 --> 00:00:40,679
从交通和天气的预测，到可能喜欢观看的电视节目的推荐。

9
00:00:40,679 --> 00:00:48,049
随着AI变得越来越普遍，许多没集成AI的技术可能开始显得不够好。

10
00:00:48,049 --> 00:00:57,320
现在，AI系统使计算机能够以前所未有的方式看到、理解和与世界互动。

12
00:00:57,320 --> 00:01:02,299
这些系统正在以惊人的速度发展。

13
00:01:02,299 --> 00:01:07,579
尽管有这些显著的进步，但AI并非万无一失。

14
00:01:07,579 --> 00:01:17,629
负责任的AI开发需要了解可能的问题、局限性或意外后果。

15
00:01:17,630 --> 00:01:22,114
技术是社会现象的反映，没有良好的实践，

16
00:01:22,115 --> 00:01:27,158
AI可能复制现有的问题或偏见，并放大它们。

17
00:01:27,159 --> 00:01:33,659
但是，并没有一个关于“负责任的AI”的通用定义，也没有一个简单的

18
00:01:33,659 --> 00:01:39,489
清单或公式来定义如何实施负责任的AI实践。

19
00:01:39,489 --> 00:01:46,948
相反，各个组织正在制定自己的AI原则，反映了他们的使命和价值观。

20
00:01:46,950 --> 00:01:51,868
虽然这些原则因组织而异，但如果寻找共同点，

21
00:01:51,868 --> 00:01:59,298
你会发现透明度、公平性、问责制和隐私方面有一致的观念。

22
00:01:59,299 --> 00:02:07,729
在谷歌，我们负责任的AI方法植根于致力于为每个人构建AI的承诺，

23
00:02:07,730 --> 00:02:14,598
它要有责任感、安全，尊重隐私，并以科学卓越为驱动。

24
00:02:14,598 --> 00:02:26,989
我们制定了自己的AI原则、实践、治理流程和工具，共同体现了我们的价值观，指导我们负责任地使用AI。

25
00:02:26,990 --> 00:02:35,389
我们将责任设计融入了产品，更重要的是，融入了我们的组织。

26
00:02:35,389 --> 00:02:42,489
像许多公司一样，我们使用AI原则作为指导负责任决策的框架。

27
00:02:42,490 --> 00:02:46,689
我们都在负责任地应用AI方面有作用。

28
00:02:46,689 --> 00:02:55,299
无论您参与AI过程的哪个阶段，从设计到部署或应用，您所做的决策都会产生影响。

29
00:02:55,300 --> 00:03:01,598
因此，您也需要有明确且可重复的负责任使用AI的流程。

30
00:03:01,598 --> 00:03:09,269
关于人工智能的一个普遍误解是，机器在决策中起着核心作用。

31
00:03:09,269 --> 00:03:16,870
实际上，设计和构建这些机器以及决定如何使用它们的是人。

32
00:03:16,870 --> 00:03:23,039
人们参与到AI开发的每个方面。他们收集或创建模型训练所需的数据。

33
00:03:23,039 --> 00:03:27,817
他们控制AI的部署以及在特定场景中的应用。

34
00:03:27,818 --> 00:03:33,039
从本质上讲，人类决策贯穿于我们的技术产品中。

35
00:03:33,039 --> 00:03:39,568
每当一个人做出决策时，他们实际上是根据自己的价值观做出选择。

36
00:03:39,568 --> 00:03:42,829
无论是决定使用生成式AI来解决问题，

37
00:03:42,830 --> 00:03:50,449
还是在整个机器学习生命周期中的其他方法，那个人都会引入自己的价值观。

38
00:03:50,449 --> 00:03:56,019
这意味着每个决策点都需要考虑和评估，以确保

39
00:03:56,020 --> 00:04:00,979
从概念到部署和维护，选择都是负责任的。

40
00:04:00,979 --> 00:04:12,739
因为这些技术有可能影响到社会的许多领域，更不用说人们的日常生活了，所以在开发这些技术时要牢记道德。

41
00:04:12,740 --> 00:04:18,350
负责任的AI并不意味着只关注明显有争议的用例。

42
00:04:18,350 --> 00:04:30,439
没有负责任的AI实践，即使看似无害的AI应用案例，或者那些出于好意的案例，仍然可能导致道德问题或意外后果，或者没有达到它们可能的最大效益。

43
00:04:30,439 --> 00:04:42,850
道德和责任很重要，不仅因为它们代表了正确的事情，而且因为它们可以指导AI设计，使人们的生活更有益。

44
00:04:42,850 --> 00:04:52,379
在谷歌，我们了解到，在任何AI部署中建立责任感可以使模型更好，并与我们的客户和客户的客户建立信任。

45
00:04:52,379 --> 00:04:59,308
如果在任何时候信任破裂，我们就有可能使AI部署停滞不前，

46
00:04:59,310 --> 00:05:04,369
不成功，或者在最坏的情况下，对受影响的利益相关者造成伤害。

47
00:05:04,370 --> 00:05:12,680
这都符合我们在谷歌的信念，即负责任的AI等于成功的AI。

48
00:05:12,680 --> 00:05:18,039
我们通过一系列评估和审查来做出关于AI的产品和业务决策。

49
00:05:18,040 --> 00:05:22,809
这些评估和审查确保了我们在产品领域和地理区域的方法的严谨性和一致性。

50
00:05:22,810 --> 00:05:29,600
这些评估和审查从确保任何项目与我们的AI原则保持一致开始。

51
00:05:29,600 --> 00:05:39,538
虽然AI原则有助于团队在共同承诺方面保持一致，但并非每个人都会同意关于如何负责任地设计产品的每个决策。

52
00:05:39,538 --> 00:05:50,839
这就是为什么我们要建立可靠的流程让人们信任，即使他们不同意最终决定，也会信任作出决定的过程。

53
00:05:50,839 --> 00:05:57,718
2018年6月，我们公布了七项AI原则来指导我们的工作。

54
00:05:57,720 --> 00:06:04,069
这些具体标准积极地指导我们的研究和产品开发，并影响我们的业务决策。

55
00:06:05,657 --> 00:06:07,686
以下是每个原则的概述：

56
00:06:08,429 --> 00:06:12,118
1. AI应具有社会效益。

57
00:06:12,120 --> 00:06:24,571
任何项目都应考虑广泛的社会和经济因素，只有在我们认为总体可能的收益明显超过可预见的风险和缺点时，才会继续进行。

58
00:06:25,214 --> 00:06:30,578
2. AI应避免产生或加强不公平的偏见。

59
00:06:30,579 --> 00:06:48,049
我们努力避免对人们产生不公正的影响，特别是与种族、民族、性别、国籍、收入、性取向、能力以及政治或宗教信仰等敏感特征相关的影响。

60
00:06:48,050 --> 00:06:52,660
3. AI应该在安全方面进行构建和测试。

62
00:06:52,660 --> 00:06:59,957
我们将继续发展并应用强大的安全保障措施，以避免产生可能导致伤害的意外结果。

63
00:07:01,000 --> 00:07:04,749
4. AI应对人类负责。

64
00:07:04,750 --> 00:07:12,680
我们将设计提供适当反馈、相关解释和申诉机会的AI系统。

65
00:07:13,171 --> 00:07:17,299
5. AI应融入隐私设计原则。

66
00:07:17,300 --> 00:07:23,589
我们将提供通知和同意的机会，鼓励具有隐私保护的架构，

67
00:07:23,589 --> 00:07:28,739
并在数据使用方面提供适当的透明度和控制。

68
00:07:28,740 --> 00:07:32,779
6. AI应维护高科学水平。

69
00:07:32,779 --> 00:07:37,838
我们将与各方利益相关者合作，推动这一领域的深思熟虑的领导力，

70
00:07:37,839 --> 00:07:42,287
运用科学严谨和多学科方法。

71
00:07:42,288 --> 00:07:52,360
我们将负责任地分享AI知识，发布教育材料、最佳实践和研究，使更多人能够开发有用的AI应用。

72
00:07:52,360 --> 00:07:58,938
7. AI应遵循这些原则进行开发。

73
00:07:58,939 --> 00:08:05,658
许多技术有多种用途，我们将努力限制可能有害或滥用的应用。

74
00:08:05,658 --> 00:08:11,728
除了这七个原则外，我们还有一些AI应用不会开发。

75
00:08:11,728 --> 00:08:21,970
我们不会设计或部署以下四个应用领域的AI：可能导致或很可能导致整体伤害的技术。

76
00:08:21,970 --> 00:08:30,149
主要目的或实施是导致或直接促成人员受伤的武器或其他技术。

77
00:08:30,149 --> 00:08:35,299
收集或使用违反国际公认准则的监控信息的技术。

78
00:08:35,299 --> 00:08:42,379
以及违反国际法和人权广泛接受原则的技术。

79
00:08:42,379 --> 00:08:47,837
制定原则是一个起点，而不是终点。

80
00:08:47,839 --> 00:08:54,599
我们的AI原则很少能直接回答我们关于如何构建产品的问题。

81
00:08:54,600 --> 00:08:59,989
它们不会（也不应该）让我们回避艰难的对话。

82
00:08:59,990 --> 00:09:04,200
它们是我们立场、我们构建的基础，

83
00:09:04,201 --> 00:09:09,719
以及为什么我们要构建它，它们是我们企业AI产品成功的核心。
