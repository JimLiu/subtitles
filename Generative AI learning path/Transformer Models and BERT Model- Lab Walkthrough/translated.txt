嗨，我是Sanjana Reddy，一名在谷歌高级解决方案实验室的机器学习工程师。

欢迎来到Transformer模型和BERT模型的实验室演示。
在这个实验室演示中，我们将通过使用预训练的Bert模型进行分类。

您可以在我们的GitHub仓库中找到设置说明。
让我们开始吧。
为了在这个Notebook上工作，您需要登录谷歌云，进入Vertex AI，然后点击Workbench。

确保在Notebook实例创建后，您已经创建了一个Notebook。

点击打开Jupyter Lab。
按照我们GitHub仓库中的说明操作后，导航到使用Bert对文本进行分类。

在这个Notebook中，我们将学习如何从TensorFlow Hub加载一个预训练的Bert模型，并使用预训练的Bert模型构建我们自己的分类。


我们将学习如何通过微调来训练Bert模型。
在开始之前，请注意，这个Notebook需要一个GPU，因为训练确实需要相当长的时间。


当你打开这个Notebook时，有一个设置说明，以便设置一个bert_kernel来安装这个Notebook所需的所有库。


对于这个Notebook，我们将使用TensorFlow和TensorFlow Hub TensorFlow Text，这是为BERT模型预处理输入所必需的。


你可以看到我在检查是否有GPU连接，我发现这个Notebook有一个GPU连接。

在这个Notebook中，我们将训练一个情感分析模型，根据评论的文本将电影评论分类为正面或负面，我们将使用IMDB数据集，你可以从这个URL下载。




一旦我们下载了数据集，我们可以查看数据以了解其中的内容。

我们看到我们有25,000个文件，分为正面和负面两个类别，我们将使用20,000个文件进行训练，剩下的5000个文件进行测试。


这个数据集的一个样本向您展示了电影评论和相关标签。

所以对于这里的一个，我们看到关联的标签是负面的，而下面这个是正面的。

一旦我们检查了数据集并对其感到满意，我们将从TensorFlow Hub加载一个预训练的BERT模型。

TensorFlow Hub提供了多种不同大小的BERT模型。

在今天的Notebook中，我们将使用一个小型BERT。
这个Bert模型有四个不同的层。
它有512个隐藏单元，有8个注意力头。
对于我们从TensorFlow Hub加载的每一个BERT模型，它都有一个相应的预处理模型。

您也可以在TensorFlow Hub上找到相应的预处理模型。

我们将研究预处理模型。
我们将加载预处理模型，就像在前面的步骤中看到的那样，并在这里传递一个示例文本。

我们刚刚通过了。
这是一部很棒的电影，我们将检查输出。
预处理模型给我们提供了多个输出。
第一个是输入单词ID。
输入单词ID是被分词的句子（Tokenized Sentence）中单词的ID。

预处理模型还为每个单词提供了掩码（Masking）。

每个句子都被转换成固定长度的输入，屏蔽掉无效的词语。

所以一旦我们预处理了输入文本，就可以在这个特定的单元格中使用从TensorFlow Hub加载的Bert模型。

这没有什么意义，因为我们还没有训练模型。

这只是一个随机的数字列表。
但是一旦你把预处理的文本传递给这个Bert模型，你就会得到一些嵌入（Embedding）。

所以为了定义我们的分类模型，我们从一个输入层开始。

输入层接收原始文本作为输入，将其传递给处理层进行预处理，将其转换为Token ID、Bert ID 和 Mask ID。


预处理后的单词然后传递给预训练模型。
这里有一个名为trainable（可训练）的参数。
可训练在这里决定了您是否希望使用您正在训练的新数据对预训练模型进行微调。

在我们的例子中，我们将trainable设置为True，这意味着我们将更新预训练模型的初始权重。


您将此设置为True或False的决定取决于两件事：你是否想更新权重，以及你的数据集的大小。



如果您有一个相对较小的数据集，建议将此设置为False，以免在预训练权重中引入噪声。

但是，如果您有足够大的数据集，可以将其设置为True。
一旦我们有了预训练模型，我们将其通过一个密集层，以获得每个类别的概率。

这就是模型输出的样子。
输出将是这个特定句子是真实的、正面的还是负面的概率。

因为我们正在处理一个二分类问题，所以我们将使用BinaryCrossentropy作为损失函数，优化的指标将是BinaryAccuracy，


通过定义优化器，我们开始初始化我们的训练。

在我们的例子中，我们使用的是Adam，这是神经网络模型的热门选择。

一旦我们初始化训练，我们可以开始使用model.fit进行训练。

我们可以传递训练数据集和验证数据集以及我们想要训练的周期数。

一旦模型训练完成，让我们评估模型的性能。
在我们的例子中，我们看到模型达到了85%的准确率，考虑到我们只训练了五个周期，这是相当不错的。

您可以潜在地阻止准确性和损失随时间变化，以便可视化模型的性能。

我们看到训练损失正在减少，我们可以稍微改进一下验证损失。

但是为了演示，我们只训练了五个周期。

一旦你对你训练的模型感到满意，你可以使用model.save EXPORT_PATH保存模型，将TensorFlow模型保存到本地路径。


因此，这行中的EXPORT_PATH将是您Notebook实例中的一部分。

保存模型后，您可以加载它以获得预测。
在这个例子中，我们有这是一部令人惊叹的电影。
这部电影很棒。
电影还可以，电影很糟糕。
我们根据训练的模型得到每个句子的预测。

如果您想进一步部署您的模型到 Vertex AI 以获得在线预测，您可以将本地保存的模型导出到 Vertex AI。


为了做到这一点，您需要检查模型的签名，以了解如何将预测传递给模型。

模型的签名会告诉您哪个是接收输入的第一层。

所以一旦我们有了本地保存的模型，我们将使用这些命令将模型推送到 Vertex 的模型注册表。


要将模型放入Vertex的模型注册表中，您需要确保拥有一个Google Cloud存储桶。

这些代码行可以让您创建一个存储桶。
如果它还不存在，我们将使用gsutils cp复制本地保存的模型，


它从EXPORT_PATH中获取本地保存的模型，并将其放入Google Cloud存储桶中。

一旦模型在Google Cloud存储桶中，我们将把它上传到Vertex AI的模型注册表中。

在这个例子中，我们使用Python SDK。
我们有aiplatform.model.upload，它从Google Cloud存储桶获取模型并将其放入模型注册表中。

一旦模型上传完毕，我们就可以在Vertex上部署模型并获得在线预测。


为了实现这一点，我们可以再次使用Python SDK，使用已上传的模型进行部署，这是一个将执行两个操作的函数。


首先，它将创建一个端点（Endpoint），其次，它将把模型上传到这个特定的端点。

如您所见，这里创建了端点，并提供了端点位置。

一旦端点创建完成，模型就会部署到这个端点。

这个步骤大约需要5到10分钟。
当您在Notebook中运行时，如果花费时间较长，请不要担心。

一旦模型部署到端点，您就可以从这个端点获取预测，然后创建一个实例对象。


根据模型的签名，我们知道第一个输入层的名称是"text"。

我们将把评论文本传递给这个特定的键。
我们创建这个实例对象，它将被传递给endpoint.predict函数。

endpoint.predict函数将接收这个实例，并给我们预测结果。

我们可以看到，对于我们的第一个实例，我喜欢这部电影并强烈推荐它。
我们有一个0.99的预测。
对于这是一部还可以的电影，这是我的看法。
我们有84%，而对于我讨厌这部电影，我们有2%，这意味着这是一种负面情绪。

所以这就是如何从一个预先训练的BERT模型创建一个分类模型，然后将其部署到Vertex上以获得在线预测。
