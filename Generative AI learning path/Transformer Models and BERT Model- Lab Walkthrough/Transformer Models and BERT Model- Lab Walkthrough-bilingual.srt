1
00:00:00,100 --> 00:00:05,100
嗨，我是Sanjana Reddy，一名在谷歌高级解决方案实验室的机器学习工程师。
Hi. I'm Sanjana Reddy, a machine learning 0 engineer at Google's Advanced Solutions Lab.

3
00:00:06,865 --> 00:00:11,499
欢迎来到Transformer模型和BERT模型的实验室演示。
Welcome to the lab walkthrough for Transformer models and BERT model.

4
00:00:12,599 --> 00:00:18,131
在这个实验室演示中，我们将通过使用预训练的Bert模型进行分类。
In this lab walkthrough, we'll be going through classification using a Pre-Trained Bert model.

6
00:00:18,666 --> 00:00:21,766
您可以在我们的GitHub仓库中找到设置说明。
You'll find the setup instructions in our GitHub repository.

7
00:00:22,666 --> 00:00:24,466
让我们开始吧。
Let's get started.

8
00:00:24,500 --> 00:00:32,699
为了在这个Notebook上工作，您需要登录谷歌云，进入Vertex AI，然后点击Workbench。
In order to work on this notebook, you'll need to log into Google Cloud, go into Vertex AI and click on Workbench.

10
00:00:33,732 --> 00:00:38,265
确保在Notebook实例创建后，您已经创建了一个Notebook。
Make sure that you have a notebook created once a notebook instance has been created.

12
00:00:38,265 --> 00:00:41,664
点击打开Jupyter Lab。
Click on Open Jupyter Lab.

13
00:00:41,665 --> 00:00:47,864
按照我们GitHub仓库中的说明操作后，导航到使用Bert对文本进行分类。
Once you've followed the instructions in our GitHub repository, navigate to classify text with Bert

15
00:00:48,832 --> 00:00:59,832
在这个Notebook中，我们将学习如何从TensorFlow Hub加载一个预训练的Bert模型，并使用预训练的Bert模型构建我们自己的分类。
in this notebook, we're going to learn how to load a Pre-Trained Bert model from TensorFlow Hub and build our own classification using the Pre-Trained Bert model,

18
00:00:59,832 --> 00:01:03,032
我们将学习如何通过微调来训练Bert模型。
we learn how to train a Bert model by fine tuning it

19
00:01:05,099 --> 00:01:13,932
在开始之前，请注意，这个Notebook需要一个GPU，因为训练确实需要相当长的时间。
before you get started. Note that this notebook requires a GPU because the training does take quite a bit of time.

22
00:01:14,433 --> 00:01:23,432
当你打开这个Notebook时，有一个设置说明，以便设置一个bert_kernel来安装这个Notebook所需的所有库。
When you open this notebook, there is a setup instruction in order to set up a bert kernel to install all the required libraries for this notebook.

25
00:01:24,799 --> 00:01:35,066
对于这个Notebook，我们将使用TensorFlow和TensorFlow Hub TensorFlow Text，这是为BERT模型预处理输入所必需的。
For this notebook, we're going to be using TensorFlow and TensorFlow Hub TensorFlow Text, which is required to pre process the input for the BERT model.

28
00:01:36,632 --> 00:01:42,932
你可以看到我在检查是否有GPU连接，我发现这个Notebook有一个GPU连接。
You can see that I'm checking if a GPU is attached and I see that I have one GPU attached to this notebook.

30
00:01:43,599 --> 00:02:02,233
在这个Notebook中，我们将训练一个情感分析模型，根据评论的文本将电影评论分类为正面或负面，我们将使用IMDB数据集，你可以从这个URL下载。
In this notebook we're going to train a sentiment analysis model to classify movie reviews as either being positive or negative based on the text of the review, we're going to be working with the IMDB data set that you can download from this URL.

35
00:02:03,933 --> 00:02:09,498
一旦我们下载了数据集，我们可以查看数据以了解其中的内容。
Once we have downloaded the data set, we can examine the data to see what's in it.

37
00:02:10,066 --> 00:02:24,300
我们看到我们有25,000个文件，分为正面和负面两个类别，我们将使用20,000个文件进行训练，剩下的5000个文件进行测试。
We see that we have 25,000 files that belong to two classes, positive and negative, and we're going to be using 20,000 files for training and the remaining 5000 for testing.

40
00:02:24,300 --> 00:02:31,532
这个数据集的一个样本向您展示了电影评论和相关标签。
A sample of this dataset shows you the movie review over here and an associated label.

42
00:02:31,532 --> 00:02:42,433
所以对于这里的一个，我们看到关联的标签是负面的，而下面这个是正面的。
So for the one over here, we see that the label that is associated is negative and the one below here it's positive.

44
00:02:42,966 --> 00:02:50,032
一旦我们检查了数据集并对其感到满意，我们将从TensorFlow Hub加载一个预训练的BERT模型。
Once we've examined our dataset and we're happy with that, we're going to load a Pre-Trained BERT model from TensorFlow Hub.

46
00:02:51,233 --> 00:02:57,564
TensorFlow Hub提供了多种不同大小的BERT模型。
TensorFlow Hub offers multiple different variations of BERT models in all different sizes.

48
00:02:58,099 --> 00:03:02,499
在今天的Notebook中，我们将使用一个小型BERT。
We're going to use a small BERT for today's notebook.

49
00:03:02,500 --> 00:03:06,065
这个Bert模型有四个不同的层。
So this bert model has four different layers.

50
00:03:06,432 --> 00:03:11,664
它有512个隐藏单元，有8个注意力头。
It has 512 hidden units, and it has eight attention heads.

51
00:03:11,665 --> 00:03:19,633
对于我们从TensorFlow Hub加载的每一个BERT模型，它都有一个相应的预处理模型。
For every BERT model that we load from TensorFlow Hub, it is associated with a corresponding pre-processing model.

53
00:03:20,432 --> 00:03:28,398
您也可以在TensorFlow Hub上找到相应的预处理模型。
You can find the corresponding pre-processing model on TensorFlow Hub as well.

55
00:03:28,400 --> 00:03:30,599
我们将研究预处理模型。
We're going to examine the pre processing model.

56
00:03:30,599 --> 00:03:38,299
我们将加载预处理模型，就像在前面的步骤中看到的那样，并在这里传递一个示例文本。
So we have we're going to load the pre processing model we see in the previous step and we pass a sample text over here.

58
00:03:38,300 --> 00:03:39,233
我们刚刚通过了。
So we just passed.

59
00:03:39,233 --> 00:03:42,600
这是一部很棒的电影，我们将检查输出。
This is an amazing movie and we're going to examine the output.

60
00:03:43,265 --> 00:03:47,164
预处理模型给我们提供了多个输出。
The pre processing model gives us multiple outputs.

61
00:03:47,400 --> 00:03:49,900
第一个是输入单词ID。
The first is the input word ID.

62
00:03:50,400 --> 00:03:56,332
输入单词ID是被分词的句子（Tokenized Sentence）中单词的ID。
The input word ID is the idea of the words in the tokenized sentence.

64
00:03:56,332 --> 00:04:02,032
预处理模型还为每个单词提供了掩码（Masking）。
The pre-processing model also provides us the masking for each word.

66
00:04:02,800 --> 00:04:14,066
每个句子都被转换成固定长度的输入，屏蔽掉无效的词语。
Every sentence is converted into a fixed length input, and it masks words that are not valid.

68
00:04:14,066 --> 00:04:20,532
所以一旦我们预处理了输入文本，就可以在这个特定的单元格中使用从TensorFlow Hub加载的Bert模型。
So once we have pre processed our input text, we can use the loaded bert model from TensorFlow Hub

70
00:04:21,632 --> 00:04:26,899
这没有什么意义，因为我们还没有训练模型。
in this particular cell block. It doesn't really make any sense because we've not trained the model.

72
00:04:26,899 --> 00:04:30,198
这只是一个随机的数字列表。
This is just a random list of numbers at this point.

73
00:04:30,632 --> 00:04:41,332
但是一旦你把预处理的文本传递给这个Bert模型，你就会得到一些嵌入（Embedding）。
But once you pass the pre process text into this bert model, you get certain embeddings.

75
00:04:41,333 --> 00:04:47,399
所以为了定义我们的分类模型，我们从一个输入层开始。
So in order to define our classification model, we start with an input layer.

77
00:04:47,399 --> 00:04:59,699
输入层接收原始文本作为输入，将其传递给处理层进行预处理，将其转换为Token ID、Bert ID 和 Mask ID。
The input layer takes the raw text as input passes it on to the processing layer for pre-processing that converted into token IDs, bert IDs and mask IDs.

80
00:04:59,699 --> 00:05:04,165
预处理后的单词然后传递给预训练模型。
The pre processed words are then passed to the Pre-Trained model.

81
00:05:05,699 --> 00:05:07,832
这里有一个名为trainable（可训练）的参数。
There is an argument here called trainable.

82
00:05:08,399 --> 00:05:16,266
可训练在这里决定了您是否希望使用您正在训练的新数据对预训练模型进行微调。
Trainable here determines if you want to fine tune the Pre-Trained model using the new data that you're training with or not.

84
00:05:16,665 --> 00:05:24,932
在我们的例子中，我们将trainable设置为True，这意味着我们将更新预训练模型的初始权重。
In our example, we are setting trainable to true, which means that we're going to update the initial weights of the Pre-Trained model.

87
00:05:26,065 --> 00:05:36,599
您将此设置为True或False的决定取决于两件事：你是否想更新权重，以及你的数据集的大小。
Your decision to set this to true or false depends on two things whether you want to update the weights and second, on the size of your dataset,

91
00:05:37,199 --> 00:05:46,566
如果您有一个相对较小的数据集，建议将此设置为False，以免在预训练权重中引入噪声。
if you have a relatively small data set, it is recommended to set this to false so that you're not introducing noise into the pre-trained weights.

93
00:05:47,065 --> 00:05:50,665
但是，如果您有足够大的数据集，可以将其设置为True。
But if you have a large enough dataset, you can set this to true.

94
00:05:52,199 --> 00:06:02,432
一旦我们有了预训练模型，我们将其通过一个密集层，以获得每个类别的概率。
Once we have our pre-trained model, we pass it through a dense layer to get probabilities for each of our classes.

96
00:06:02,432 --> 00:06:05,032
这就是模型输出的样子。
This is what the output from the model is going to look like.

97
00:06:05,033 --> 00:06:15,565
输出将是这个特定句子是真实的、正面的还是负面的概率。
The output is going to be a probability of whether this particular sentence is true, is positive or negative.

99
00:06:15,565 --> 00:06:26,831
因为我们正在处理一个二分类问题，所以我们将使用BinaryCrossentropy作为损失函数，优化的指标将是BinaryAccuracy，
Since we're working with a binary classification problem, we're going to use BinaryCrossentropy as our loss function and the metric to optimize for is going to be BinaryAccuracy

102
00:06:28,565 --> 00:06:33,032
通过定义优化器，我们开始初始化我们的训练。
for initializing our training by defining the optimizer.

104
00:06:33,033 --> 00:06:39,998
在我们的例子中，我们使用的是Adam，这是神经网络模型的热门选择。
In our case, we're using Adam, which is a popular choice for neural network models.

106
00:06:40,899 --> 00:06:45,698
一旦我们初始化训练，我们可以开始使用model.fit进行训练。
Once we initialize the training, we can start training using model dot fit.

108
00:06:46,100 --> 00:06:55,199
我们可以传递训练数据集和验证数据集以及我们想要训练的周期数。
We can pass the train dataset and the validation dataset and the number of epochs that we want to train for.

110
00:06:55,199 --> 00:06:59,532
一旦模型训练完成，让我们评估模型的性能。
Once the model has trained, let's evaluate the performance of the model.

111
00:06:59,565 --> 00:07:08,700
在我们的例子中，我们看到模型达到了85%的准确率，考虑到我们只训练了五个周期，这是相当不错的。
So in our case we see that the model achieved an accuracy of 85%, which is pretty decent considering we only trained it for five epochs.

113
00:07:09,899 --> 00:07:16,665
您可以潜在地阻止准确性和损失随时间变化，以便可视化模型的性能。
You can potentially thwart the accuracy and loss over time in order to visualize the model's performance.

115
00:07:17,766 --> 00:07:24,866
我们看到训练损失正在减少，我们可以稍微改进一下验证损失。
0 We see that the training loss is going down and we could work on our validation loss a little bit.

117
00:07:25,266 --> 00:07:31,499
但是为了演示，我们只训练了五个周期。
But for the sake of demonstration, we've only trained it for five epochs.

119
00:07:31,500 --> 00:07:42,864
一旦你对你训练的模型感到满意，你可以使用model.save EXPORT_PATH保存模型，将TensorFlow模型保存到本地路径。
Once you're satisfied with the model that you've trained, you can save the model using model dot safe model dot save export. So TensorFlow model to a local path.

122
00:07:43,432 --> 00:07:51,665
因此，这行中的EXPORT_PATH将是您Notebook实例中的一部分。
So the export path in this line is going to be a part in your notebook instance.

124
00:07:51,665 --> 00:07:55,298
保存模型后，您可以加载它以获得预测。
Once you've saved your model, you can load it to get predictions.

125
00:07:55,600 --> 00:07:59,766
在这个例子中，我们有这是一部令人惊叹的电影。
So in this example we have this is such an amazing movie.

126
00:07:59,766 --> 00:08:01,333
这部电影很棒。
This movie was great.

127
00:08:01,333 --> 00:08:04,466
电影还可以，电影很糟糕。
The movie was okay-ish, the movie was terrible.

128
00:08:04,932 --> 00:08:09,865
我们根据训练的模型得到每个句子的预测。
And we get predictions for each of these sentences based on the model that we have trained.

130
00:08:11,665 --> 00:08:23,332
如果您想进一步部署您的模型到 Vertex AI 以获得在线预测，您可以将本地保存的模型导出到 Vertex AI。
If you would like to take this further and deploy your model on Vertex AI to get online predictions, you could take the locally saved model and export it to Vertex AI.

133
00:08:25,733 --> 00:08:31,800
为了做到这一点，您需要检查模型的签名，以了解如何将预测传递给模型。
in order to do this, you need to check the signature of the model to see how you can pass predictions to the model.

135
00:08:32,566 --> 00:08:40,365
模型的签名会告诉您哪个是接收输入的第一层。
The signature of the model shows you what is the first layer that is taking input.

137
00:08:40,365 --> 00:08:52,799
所以一旦我们有了本地保存的模型，我们将使用这些命令将模型推送到 Vertex 的模型注册表。
So once we have the locally saved model, we are going to push the model to Vertex’s Model Registry using these commands.

140
00:08:52,799 --> 00:08:59,033
要将模型放入Vertex的模型注册表中，您需要确保拥有一个Google Cloud存储桶。
In order to put the model in Vertex’s Model Registry, you need to ensure that you have a Google Cloud storage bucket.

142
00:08:59,432 --> 00:09:02,499
这些代码行可以让您创建一个存储桶。
And these lines over here, let you create a bucket.

143
00:09:02,500 --> 00:09:10,199
如果它还不存在，我们将使用gsutils cp复制本地保存的模型，
If it doesn't already exist, we're going to copy the locally saved model using GS Utils CP

146
00:09:10,700 --> 00:09:16,564
它从EXPORT_PATH中获取本地保存的模型，并将其放入Google Cloud存储桶中。
which takes a locally saved model from the export pack and puts it in the Google Cloud storage bucket.

148
00:09:17,865 --> 00:09:24,065
一旦模型在Google Cloud存储桶中，我们将把它上传到Vertex AI的模型注册表中。
Once the model is in the Google Cloud Storage bucket, we're going to upload it to Vertex AI's model registry.

150
00:09:24,799 --> 00:09:27,964
在这个例子中，我们使用Python SDK。
We're using the Python SDK in this case.

151
00:09:28,133 --> 00:09:36,964
我们有aiplatform.model.upload，它从Google Cloud存储桶获取模型并将其放入模型注册表中。
So we have a platform dot model dot upload, which takes the model from Google Cloud Storage Bucket and puts it in the model registry.

153
00:09:38,732 --> 00:09:47,332
一旦模型上传完毕，我们就可以在Vertex上部署模型并获得在线预测。
Once the model has been uploaded, we're ready to deploy the model on Vertex and get online predictions.

156
00:09:47,332 --> 00:09:56,732
为了实现这一点，我们可以再次使用Python SDK，使用已上传的模型进行部署，这是一个将执行两个操作的函数。
In order to do this, we can use the Python SDK again so we can use uploaded model to deploy, which is a function that is going to do two things.

159
00:09:56,732 --> 00:10:04,666
首先，它将创建一个端点（Endpoint），其次，它将把模型上传到这个特定的端点。
One, it's going to create an end point, and two, it's going to upload the model to this particular endpoint.

161
00:10:05,466 --> 00:10:11,065
如您所见，这里创建了端点，并提供了端点位置。
So you can see here that it's creating the endpoint, providing you the endpoint location.

163
00:10:11,066 --> 00:10:15,633
一旦端点创建完成，模型就会部署到这个端点。
And then once the endpoint has been created, the model is deployed to this endpoint.

165
00:10:16,566 --> 00:10:20,066
这个步骤大约需要5到10分钟。
This step is going to take around 5 to 10 minutes.

166
00:10:20,066 --> 00:10:24,466
当您在Notebook中运行时，如果花费时间较长，请不要担心。
When you run through your notebook. So just don't worry if it takes too long,

168
00:10:25,365 --> 00:10:35,199
一旦模型部署到端点，您就可以从这个端点获取预测，然后创建一个实例对象。
once the model has been deployed to the endpoint, you're ready to get predictions from this endpoint so you can create an instance object.

171
00:10:35,966 --> 00:10:41,665
根据模型的签名，我们知道第一个输入层的名称是"text"。
So using the signature of the model, we know that the name of the first input layer is text.

173
00:10:42,100 --> 00:10:47,233
我们将把评论文本传递给这个特定的键。
So we're going to pass our review text to this particular key.

174
00:10:48,299 --> 00:10:53,533
我们创建这个实例对象，它将被传递给endpoint.predict函数。
We create this instances object that is going to be passed to the endpoint dot predictive function.

176
00:10:54,232 --> 00:11:00,332
endpoint.predict函数将接收这个实例，并给我们预测结果。
And the endpoint our predict function is going to take this instance and it's going to give us predictions.

178
00:11:00,332 --> 00:11:04,498
我们可以看到，对于我们的第一个实例，我喜欢这部电影并强烈推荐它。
So we can see for our first instance, I love the movie and highly recommend it.

179
00:11:04,799 --> 00:11:08,364
我们有一个0.99的预测。
We have a prediction of 0.99.

180
00:11:08,365 --> 00:11:10,099
对于这是一部还可以的电影，这是我的看法。
For it was an okay movie in my opinion.

181
00:11:10,100 --> 00:11:16,333
我们有84%，而对于我讨厌这部电影，我们有2%，这意味着这是一种负面情绪。
We have 84% and for I hated the movie, we have 2%, which means it's a negative sentiment.

183
00:11:17,232 --> 00:11:27,699
所以这就是如何从一个预先训练的BERT模型创建一个分类模型，然后将其部署到Vertex上以获得在线预测。
So this is how you can create a classification model from a pre-trained BERT model and then deploy it on Vertex to get online predictions.
