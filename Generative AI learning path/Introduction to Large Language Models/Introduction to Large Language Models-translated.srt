1
00:00:00,570 --> 00:00:04,386
JOHN EWALD：大家好，欢迎来到大型语言模型介绍课程。

3
00:00:04,386 --> 00:00:08,158
我是JOHN EWALD，谷歌云的培训开发人员。

5
00:00:08,160 --> 00:00:10,859
在这门课程中，您将学习定义大型语言模型，

6
00:00:10,859 --> 00:00:14,669
即LLM，描述LLM的用例，

7
00:00:14,669 --> 00:00:20,429
解释提示调优（Prompt Tuning），以及介绍谷歌的Gen AI开发工具。

9
00:00:20,429 --> 00:00:24,419
大型语言模型，即LLM，是深度学习的一个子集。

10
00:00:24,420 --> 00:00:26,219
想了解更多关于深度学习的信息，

11
00:00:26,219 --> 00:00:29,669
请观看我们的生成式AI课程视频。

12
00:00:29,670 --> 00:00:32,429
LLMs和生成式AI相交，它们都是

13
00:00:32,429 --> 00:00:35,399
深度学习的一部分。

14
00:00:35,399 --> 00:00:38,039
你可能经常听到的另一个AI领域

15
00:00:38,039 --> 00:00:39,718
是生成式AI。

16
00:00:39,719 --> 00:00:41,969
这是一种能够

17
00:00:41,969 --> 00:00:45,728
生成新内容的人工智能，包括文本、图像、音频、

18
00:00:45,729 --> 00:00:48,289
和合成数据。

19
00:00:48,289 --> 00:00:50,648
那么大型语言模型是什么？

20
00:00:50,649 --> 00:00:53,979
大型语言模型是指可以预先训练，然后进行微调

21
00:00:53,979 --> 00:00:59,020
以适应特定目的的大型通用语言模型。

23
00:00:59,020 --> 00:01:03,140
什么是预训练和微调（Fine Tuned）？

24
00:01:03,140 --> 00:01:05,029
想象一下训练一条狗。

25
00:01:05,030 --> 00:01:07,069
通常，你会教狗基本的命令

26
00:01:07,069 --> 00:01:12,779
比如坐下、过来、趴下和待在原地。

27
00:01:12,780 --> 00:01:15,750
这些命令对日常生活来说足够了

28
00:01:15,750 --> 00:01:19,489
帮助你的狗成为一个好的犬类公民。

29
00:01:19,489 --> 00:01:22,489
但是，如果你需要特殊服务犬，比如警犬、

30
00:01:22,489 --> 00:01:28,479
导盲犬或猎犬，你需要增加特殊训练。

31
00:01:28,480 --> 00:01:33,279
这个概念也适用于大型语言模型。

32
00:01:33,280 --> 00:01:35,489
这些模型是为通用目的而训练的

33
00:01:35,489 --> 00:01:41,219
解决常见的语言问题，如文本分类、问答、

35
00:01:41,219 --> 00:01:47,089
文档摘要和跨行业的文本生成。

36
00:01:47,090 --> 00:01:58,789
然后，可以使用相对较小规模的领域数据集，将模型定制来解决零售、金融、娱乐等不同领域的特定问题。

40
00:01:58,790 --> 00:02:00,560
我们进一步分解这个概念，

41
00:02:00,560 --> 00:02:04,699
将其分为大型语言模型的三个主要特点。

42
00:02:04,700 --> 00:02:06,989
大有两层含义。

43
00:02:06,989 --> 00:02:10,037
首先是训练数据集的庞大规模，

44
00:02:10,038 --> 00:02:12,649
有时达到PB级别。

45
00:02:12,650 --> 00:02:15,289
其次，它指的是参数数量。

46
00:02:15,289 --> 00:02:18,889
在机器学习中，参数通常被称为超参数。

47
00:02:18,889 --> 00:02:24,259
参数基本上是机器从模型训练中学到的记忆和知识。

49
00:02:24,259 --> 00:02:29,998
参数定义了模型在解决问题（如预测文本）的技能。

51
00:02:30,000 --> 00:02:31,830
通用性意味着这些模型

52
00:02:31,830 --> 00:02:34,439
足以解决常见问题。

53
00:02:34,439 --> 00:02:36,869
有两个原因导致了这个想法。

54
00:02:36,870 --> 00:02:40,019
首先是人类语言的共性，无论

55
00:02:40,020 --> 00:02:41,580
具体任务如何。

56
00:02:41,580 --> 00:02:44,819
其次是资源限制。

57
00:02:44,819 --> 00:02:47,428
只有某些组织具备能力

58
00:02:47,430 --> 00:02:51,209
用大量数据集训练这种大型语言模型

59
00:02:51,210 --> 00:02:54,150
和大量参数。

60
00:02:54,150 --> 00:02:58,869
让他们为其他人创建基本语言模型怎么样？

62
00:02:58,870 --> 00:03:02,680
这导致了最后一点，预训练和微调，

63
00:03:02,680 --> 00:03:07,659
意味着用大型数据集预训练一个通用目的的大型语言模型

65
00:03:07,659 --> 00:03:13,699
然后用更小的数据集为特定目标进行微调。

67
00:03:13,699 --> 00:03:17,189
使用大型语言模型的好处很明显。

69
00:03:17,189 --> 00:03:20,879
首先，一个模型可以应用于不同的任务。

70
00:03:20,879 --> 00:03:22,548
这是梦想成真。

71
00:03:22,550 --> 00:03:27,949
这些经过大量数据训练并生成数十亿参数的大型语言模型

73
00:03:27,949 --> 00:03:31,319
足够智能，能解决不同的任务，

74
00:03:31,319 --> 00:03:38,810
包括语言翻译、句子补全、文本分类、问答等。

76
00:03:38,810 --> 00:03:42,560
其次，当你定制大型语言模型来解决特定问题时，

77
00:03:42,560 --> 00:03:46,219
它们需要的领域训练数据很少。

78
00:03:46,219 --> 00:03:49,009
大型语言模型表现不错

79
00:03:49,009 --> 00:03:51,419
即使域训练数据很少。

80
00:03:51,419 --> 00:03:55,789
换句话说，它们可用于少量样本（Few-shot）甚至零样本（Zero-shot）场景。

82
00:03:55,789 --> 00:03:57,619
在机器学习中，少样本是指

83
00:03:57,620 --> 00:03:59,870
用最少的数据训练模型，

84
00:03:59,870 --> 00:04:02,539
而零样本意味着模型能识别

85
00:04:02,539 --> 00:04:07,108
在以前的训练中未明确教过的事物。

87
00:04:07,110 --> 00:04:09,719
第三，大型语言模型的性能

88
00:04:09,719 --> 00:04:15,549
随着数据和参数的增加而不断提高。

89
00:04:15,550 --> 00:04:17,949
以PaLM为例。

90
00:04:17,949 --> 00:04:21,308
2022年4月，谷歌发布了PaLM，

91
00:04:21,310 --> 00:04:26,050
简称Pathways Language Model，这是一个拥有5400亿参数的模型，

92
00:04:26,050 --> 00:04:31,658
在多种语言任务中都实现了最先进的性能。

94
00:04:31,660 --> 00:04:35,079
PaLM是一个仅解码器的密集型Transformer模型，

95
00:04:35,079 --> 00:04:38,529
它有5400亿个参数。

96
00:04:38,529 --> 00:04:40,788
它利用了新的路径系统，

97
00:04:40,790 --> 00:04:47,859
这使得谷歌能够在多个TPU V4集群上有效地训练一个模型。

99
00:04:47,860 --> 00:04:50,259
Pathway是一种新的AI架构，它可以

100
00:04:50,259 --> 00:04:54,308
同时处理多个任务，快速学习新任务，

101
00:04:54,310 --> 00:04:57,889
并更好地理解世界。

102
00:04:57,889 --> 00:05:01,339
该系统使PaLM能够协调分布式

103
00:05:01,339 --> 00:05:04,629
加速器的计算。

104
00:05:04,629 --> 00:05:07,928
我们之前提到过PaLM是一个Transformer模型。

105
00:05:07,930 --> 00:05:11,479
Transformer模型包括编码器和解码器。

106
00:05:11,480 --> 00:05:13,629
编码器对输入序列进行编码，

107
00:05:13,629 --> 00:05:15,669
然后将其传递给解码器，解码器

108
00:05:15,670 --> 00:05:18,009
学会如何解码表示，

109
00:05:18,009 --> 00:05:20,959
以完成相关任务。

110
00:05:20,959 --> 00:05:27,050
我们已经从传统编程发展到了神经网络和生成模型

112
00:05:27,050 --> 00:05:28,939
在传统编程中，我们需要

113
00:05:28,939 --> 00:05:32,059
硬编码区分猫的规则，例如：

114
00:05:32,060 --> 00:05:37,879
类型：动物；腿：四条；耳朵：两个；毛发：有；

115
00:05:37,879 --> 00:05:40,749
喜欢：纱线，猫薄荷。

116
00:05:40,750 --> 00:05:42,790
在神经网络浪潮中，我们

117
00:05:42,790 --> 00:05:45,819
可以给网络提供猫和狗的图片，然后询问，

118
00:05:45,819 --> 00:05:47,048
这是一只猫吗？

119
00:05:47,050 --> 00:05:49,029
它会预测出是猫。

120
00:05:49,029 --> 00:05:51,969
在生成式中，我们作为用户

121
00:05:51,970 --> 00:05:55,399
可以生成自己的内容，无论是文本、图片、

122
00:05:55,399 --> 00:05:57,579
音频、视频还是其他。

123
00:05:57,579 --> 00:06:00,398
例如，像PaLM或LaMDA这样的模型，

124
00:06:00,399 --> 00:06:03,488
或者用于对话应用的语言模型，

125
00:06:03,490 --> 00:06:06,970
从互联网的多个来源获取大量数据

126
00:06:06,970 --> 00:06:10,059
建立基础语言模型，

127
00:06:10,060 --> 00:06:12,910
我们只需提问，

128
00:06:12,910 --> 00:06:17,290
无论是输入提示还是口头对话提示。

130
00:06:17,290 --> 00:06:19,449
当你问它什么是猫时，它会

131
00:06:19,449 --> 00:06:23,489
告诉你它所学到的关于猫的一切。

132
00:06:23,490 --> 00:06:26,789
我们来比较一下使用预训练模型的LLM开发

133
00:06:26,790 --> 00:06:29,279
和传统的ML开发。

134
00:06:29,279 --> 00:06:33,069
首先，使用LLM开发，你不需要成为专家。

135
00:06:33,069 --> 00:06:34,799
你不需要训练样本。

136
00:06:34,800 --> 00:06:37,199
也不需要训练模型。

137
00:06:37,199 --> 00:06:40,109
你只需要考虑提示设计，即

138
00:06:40,110 --> 00:06:44,669
创建清晰、简洁、有信息量的提示。

140
00:06:44,670 --> 00:06:48,540
自然语言处理的重要部分。

141
00:06:48,540 --> 00:06:50,189
在传统的机器学习中，你

142
00:06:50,189 --> 00:06:52,648
需要训练样本来训练模型。

143
00:06:52,649 --> 00:06:56,508
还需要计算时间和硬件。

144
00:06:56,509 --> 00:07:01,759
让我们看一个文本生成用例。

145
00:07:01,759 --> 00:07:05,989
问答，或者说QA，是自然语言的一个子领域

146
00:07:05,990 --> 00:07:09,439
处理自动回答的任务

147
00:07:09,439 --> 00:07:12,709
用自然语言提出的问题。

148
00:07:12,709 --> 00:07:16,099
QA系统通常在大量的

149
00:07:16,100 --> 00:07:17,269
文本和代码上进行训练。

150
00:07:17,269 --> 00:07:19,889
他们能回答各种问题，

151
00:07:19,889 --> 00:07:23,599
包括事实、定义和观点类问题。

152
00:07:23,600 --> 00:07:26,059
关键是需要领域知识

153
00:07:26,060 --> 00:07:29,560
来开发这些问答模型。

154
00:07:29,560 --> 00:07:32,169
例如，需要领域知识

155
00:07:32,170 --> 00:07:38,158
来开发客户支持、医疗或供应链的问答模型。

157
00:07:38,160 --> 00:07:43,439
使用生成式问答，模型根据上下文直接生成自由文本。

159
00:07:43,439 --> 00:07:47,358
不需要领域知识。

160
00:07:47,360 --> 00:07:49,699
让我们看看给Bard提出的三个问题，

161
00:07:49,699 --> 00:07:55,258
Bard是由谷歌AI开发的大型语言模型聊天机器人。

162
00:07:55,259 --> 00:07:56,549
问题一。

163
00:07:56,550 --> 00:07:59,309
"今年的销售额是100,000美元。

164
00:07:59,310 --> 00:08:01,499
支出是60,000美元。

165
00:08:01,500 --> 00:08:03,729
净利润是多少？"

166
00:08:03,730 --> 00:08:07,660
Bard首先分享了如何计算净利润，然后

167
00:08:07,660 --> 00:08:09,459
进行计算。

168
00:08:09,459 --> 00:08:14,128
接着Bard给出了净利润的定义。

169
00:08:14,129 --> 00:08:15,409
这是另一个问题。

170
00:08:15,410 --> 00:08:18,919
现有库存为6,000个单位。

171
00:08:18,920 --> 00:08:21,829
新订单需要8,000个单位。

172
00:08:21,829 --> 00:08:25,309
我需要补充多少单位才能完成订单？

173
00:08:25,310 --> 00:08:29,750
巴德再次通过计算回答了这个问题。

174
00:08:29,750 --> 00:08:32,509
在我们的最后一个例子中，我们有1,000个传感器

175
00:08:32,509 --> 00:08:34,638
分布在10个地理区域。

176
00:08:34,639 --> 00:08:38,639
每个区域平均有多少个传感器？

177
00:08:38,639 --> 00:08:40,528
巴德用一个例子回答了这个问题

178
00:08:40,529 --> 00:08:44,489
解决问题的方法和一些额外的背景。

179
00:08:44,490 --> 00:08:47,980
在我们的每个问题中，都得到了期望的回答。

180
00:08:47,980 --> 00:08:50,669
这是因为提示设计（Prompt design）。

181
00:08:50,669 --> 00:08:52,499
提示设计和提示工程（Prompt Engineering）

182
00:08:52,500 --> 00:08:56,639
是自然语言处理中密切相关的两个概念。

183
00:08:56,639 --> 00:08:58,589
两者都涉及创建

184
00:08:58,590 --> 00:09:01,440
一个清晰、简洁、有信息量的提示。

185
00:09:01,440 --> 00:09:05,019
然而，两者之间存在一些关键差异。

186
00:09:05,019 --> 00:09:08,318
提示设计是创建一个提示的过程，

187
00:09:08,320 --> 00:09:11,649
它针对的是这个系统正在

188
00:09:11,649 --> 00:09:13,599
执行的特定任务。

189
00:09:13,600 --> 00:09:15,370
例如，如果系统正在

190
00:09:15,370 --> 00:09:18,340
被要求将文本从英语翻译成法语，

191
00:09:18,340 --> 00:09:20,469
提示应该用英语写

192
00:09:20,470 --> 00:09:24,610
并且应该指定翻译应该用法语。

194
00:09:24,610 --> 00:09:26,289
提示工程是一个过程

195
00:09:26,289 --> 00:09:30,289
旨在创建有助于提高性能的提示

197
00:09:30,289 --> 00:09:32,958
这可能涉及使用特定领域的知识，

198
00:09:32,960 --> 00:09:35,200
提供期望输出的示例，

199
00:09:35,200 --> 00:09:40,179
或使用已知对特定系统有效的关键词。

201
00:09:40,179 --> 00:09:42,728
提示设计是一个更通用的概念，

202
00:09:42,730 --> 00:09:45,789
而提示工程则是一个更专业的概念。

203
00:09:45,789 --> 00:09:48,608
提示设计至关重要，而提示工程

204
00:09:48,610 --> 00:09:54,909
只对需要高度准确性或性能的系统才是必要的。

206
00:09:54,909 --> 00:09:57,509
有三种类型的大型语言模型，

207
00:09:57,509 --> 00:10:00,178
通用语言模型、指令调优、

208
00:10:00,179 --> 00:10:01,828
和对话调优。

209
00:10:01,830 --> 00:10:04,740
每种都需要用不同的方式提示。

210
00:10:04,740 --> 00:10:07,110
通用语言模型根据训练数据中的语言预测下一个词，

211
00:10:07,110 --> 00:10:10,289
基于训练数据中的语言。

212
00:10:10,289 --> 00:10:13,068
这是一个通用语言模型的例子。

213
00:10:13,070 --> 00:10:17,519
下一个词是基于训练数据中的语言标记（Token）。

215
00:10:17,519 --> 00:10:20,059
在这个例子中，"the cat sat on,"

216
00:10:20,059 --> 00:10:22,248
下一个词应该是"the."。

217
00:10:22,250 --> 00:10:26,629
你可以看到"the."是最可能的下一个词。

218
00:10:26,629 --> 00:10:30,978
把这种类型想象成搜索中的自动补全。

219
00:10:30,980 --> 00:10:33,200
在指令调整中，模型被训练成

220
00:10:33,200 --> 00:10:37,779
根据输入的指令预测响应。

222
00:10:37,779 --> 00:10:40,569
例如，总结某某文章中的文本，

223
00:10:40,570 --> 00:10:43,059
以某某的风格创作一首诗，

224
00:10:43,059 --> 00:10:48,688
给我一个基于某某的语义相似度的关键词列表。

225
00:10:48,690 --> 00:10:51,120
在这个例子中，将文本分类为

226
00:10:51,120 --> 00:10:54,979
中性、消极或积极。

227
00:10:54,980 --> 00:11:00,999
在对话调整中，模型被训练成通过下一个回应来进行对话。

229
00:11:01,000 --> 00:11:03,639
对话调优模型是一种特殊情况

230
00:11:03,639 --> 00:11:06,819
指令调优通常是将请求构建成

231
00:11:06,820 --> 00:11:09,039
与聊天机器人的问题。

232
00:11:09,039 --> 00:11:10,989
对话调优预计会在

233
00:11:10,990 --> 00:11:13,990
较长的来回对话中发挥作用，

234
00:11:13,990 --> 00:11:19,168
通常更适合自然的类似问题的表述。

236
00:11:19,169 --> 00:11:21,628
思维链推理（Chain of thought reasoning）是指观察到到的一种现象：

237
00:11:21,629 --> 00:11:24,028
如果让模型先解释答案的原因，

238
00:11:24,029 --> 00:11:28,319
更容易得出正确答案。

240
00:11:28,320 --> 00:11:29,899
我们来看这个问题。

241
00:11:29,899 --> 00:11:31,849
罗杰有五个网球。

242
00:11:31,850 --> 00:11:34,429
他又买了两罐网球。

243
00:11:34,429 --> 00:11:36,679
每罐有三个网球。

244
00:11:36,679 --> 00:11:39,629
现在他有多少个网球？

245
00:11:39,629 --> 00:11:42,869
这个问题一开始没有回答。

246
00:11:42,870 --> 00:11:46,360
模型直接得出正确答案的可能性较小。

247
00:11:46,360 --> 00:11:49,210
但到了第二个问题时，

248
00:11:49,210 --> 00:11:53,879
输出更有可能得出正确答案。

249
00:11:53,879 --> 00:11:57,088
一个能做所有事的模型在实际应用中是有局限性的。

250
00:11:57,090 --> 00:12:01,359
针对特定任务的调优可以使LLMs更可靠。

251
00:12:01,360 --> 00:12:05,259
Vertex AI提供针对特定任务的基础模型。

252
00:12:05,259 --> 00:12:06,729
假设你有一个用例，

253
00:12:06,730 --> 00:12:08,889
你需要收集情绪，或者了解

254
00:12:08,889 --> 00:12:11,829
你的客户对你的产品或服务的感受。

255
00:12:11,830 --> 00:12:13,929
你可以使用分类任务

256
00:12:13,929 --> 00:12:17,038
情感分析任务模型。

257
00:12:17,039 --> 00:12:18,569
对于视觉任务也是一样。

258
00:12:18,570 --> 00:12:21,210
如果你需要进行占用率分析

259
00:12:21,210 --> 00:12:24,690
那么有一个特定于你用例的任务模型。

260
00:12:24,690 --> 00:12:32,109
调优一个模型可以让你根据你希望模型执行的任务的示例来定制模型的响应。

263
00:12:32,110 --> 00:12:34,469
本质上，它是通过在新数据上训练模型，

264
00:12:34,470 --> 00:12:37,229
将模型适应到新的领域，

265
00:12:37,230 --> 00:12:40,239
或一套自定义的用例集合的过程。

266
00:12:40,240 --> 00:12:42,449
例如，我们可以收集训练数据

267
00:12:42,450 --> 00:12:47,509
并针对法律或医学领域专门调整模型。

269
00:12:47,509 --> 00:12:50,839
你还可以通过微调进一步调整模型

270
00:12:50,840 --> 00:12:52,549
在这里你可以用自己的数据集

271
00:12:52,549 --> 00:12:56,659
并通过调整LLM中的每个权重来重新训练模型。

272
00:12:56,659 --> 00:12:59,328
这需要大规模的训练工作，

273
00:12:59,330 --> 00:13:01,600
以及托管你自己微调过的模型。

274
00:13:01,600 --> 00:13:05,798
比如，这里有一个由医疗数据训练出的医疗基础模型，

276
00:13:05,799 --> 00:13:12,749
其任务包括问答，图像分析，寻找相似患者等等。

278
00:13:12,750 --> 00:13:17,509
但是，微调的费用高昂，在许多情况下并不现实。

279
00:13:17,509 --> 00:13:21,528
那么，有没有更高效的调整方法呢？

280
00:13:21,529 --> 00:13:22,339
是的。

281
00:13:22,340 --> 00:13:25,759
参数效率调整方法，或称为PETM，

282
00:13:25,759 --> 00:13:27,888
是在不复制模型的情况下调整大型语言模型

283
00:13:27,889 --> 00:13:31,908
的方法，而无需复制模型。

284
00:13:31,909 --> 00:13:34,609
基础模型本身并没有改变，

285
00:13:34,610 --> 00:13:36,799
只是调整了少数几层附加层，

286
00:13:36,799 --> 00:13:41,529
这些层可以在推理时交换。

287
00:13:41,529 --> 00:13:47,498
生成式AI工作室让你能快速探索和定制

289
00:13:47,500 --> 00:13:50,950
可在Google Cloud上用于你的应用的生成式AI模型。

290
00:13:50,950 --> 00:13:58,479
生成式AI工作室帮助开发者通过提供各种工具和资源创建和部署生成式AI模型，

293
00:13:58,480 --> 00:14:00,100
使得入门变得简单。

294
00:14:00,100 --> 00:14:03,459
例如，它提供了预训练模型库、

295
00:14:03,460 --> 00:14:07,359
用于精细调整模型的工具、用于将模型部署到生产环境的工具，

296
00:14:07,360 --> 00:14:13,278
它提供了预训练模型库、以及供开发者分享想法和合作的社区论坛。

298
00:14:13,279 --> 00:14:15,589
生成式AI应用构建器让你

299
00:14:15,590 --> 00:14:19,609
无需编写任何代码就能创建Gen AI应用。

300
00:14:19,610 --> 00:14:22,699
Gen AI应用构建器具有拖放界面

301
00:14:22,700 --> 00:14:24,799
使设计和构建应用变得简单，

302
00:14:24,799 --> 00:14:28,669
一个可视化编辑器，方便创建和编辑

303
00:14:28,669 --> 00:14:32,389
应用内容，一个内置搜索引擎，允许用户

304
00:14:32,389 --> 00:14:34,759
在应用内搜索信息，

305
00:14:34,759 --> 00:14:36,888
以及一个会话式AI引擎，

306
00:14:36,889 --> 00:14:40,879
允许用户使用自然语言与应用进行交互。

307
00:14:40,879 --> 00:14:44,088
你可以创建自己的聊天机器人、数字助手、

308
00:14:44,090 --> 00:14:48,259
定制搜索引擎、知识库、培训应用、

309
00:14:48,259 --> 00:14:50,029
等等。

310
00:14:50,029 --> 00:14:52,939
PaLM API 让你可以测试和尝试

311
00:14:52,940 --> 00:14:56,999
谷歌的大型语言模型和 Gen AI 工具。

312
00:14:57,000 --> 00:14:59,909
为了使原型设计更快速、更易于使用，

313
00:14:59,909 --> 00:15:03,748
开发者可以将 PaLM API 与 Maker Suite 集成

314
00:15:03,750 --> 00:15:08,339
并通过图形用户界面访问 API。

316
00:15:08,340 --> 00:15:10,649
该套件包括许多不同的工具，

317
00:15:10,649 --> 00:15:14,068
如模型训练工具、模型部署工具、

318
00:15:14,070 --> 00:15:16,480
以及模型监控工具。

319
00:15:16,480 --> 00:15:22,029
模型训练工具帮助开发者用不同算法训练机器学习模型在他们的数据上。

321
00:15:22,029 --> 00:15:25,589
模型部署工具帮助开发者用多种部署选项

322
00:15:25,590 --> 00:15:29,740
将机器学习模型部署到生产环境。

323
00:15:29,740 --> 00:15:31,929
模型监控工具则帮助

324
00:15:31,929 --> 00:15:35,199
开发者通过仪表盘和多种指标

325
00:15:35,200 --> 00:15:40,480
监控生产环境中机器学习模型的性能。

327
00:15:40,480 --> 00:15:41,720
目前就是这些。

328
00:15:41,720 --> 00:15:46,469
感谢收看本课程，大型语言模型简介。
