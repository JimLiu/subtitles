1
00:00:00,000 --> 00:00:10,000
[Music]

2
00:00:10,000 --> 00:00:13,000
Hi everyone, welcome back, we had a good lunch.

3
00:00:13,000 --> 00:00:18,000
My name is Woldakas Hevin, senior editor for AI at MIT Technology Review,

4
00:00:18,000 --> 00:00:23,000
and I think we'd all agree there's no denying that generative AI is the thing at the moment.

5
00:00:23,000 --> 00:00:27,000
But innovation does not stand still, and in this chapter we're going to take a look at cutting-edge research

6
00:00:27,000 --> 00:00:30,680
research that is already pushing ahead and asking what's next.

7
00:00:30,680 --> 00:00:35,280
But starting us off, I'd like to introduce a very special speaker who will be joining

8
00:00:35,280 --> 00:00:36,920
us virtually.

9
00:00:36,920 --> 00:00:42,680
Geoffrey Hinton is a professor emeritus at University of Toronto and until this week an engineering

10
00:00:42,680 --> 00:00:43,680
fellow at Google.

11
00:00:43,680 --> 00:00:48,960
But on Monday he announced that after 10 years he will be stepping down.

12
00:00:48,960 --> 00:00:52,280
Geoffrey is one of the most important figures in modern AI.

13
00:00:52,280 --> 00:00:56,120
He's a pioneer of deep learning, developing some of the most fundamental techniques that

14
00:00:56,120 --> 00:00:58,440
that underpin AI as we know it today,

15
00:00:58,440 --> 00:00:59,840
such as back propagation,

16
00:00:59,840 --> 00:01:02,320
the algorithm that allows machines to learn.

17
00:01:02,320 --> 00:01:05,160
This technique, it's the foundation

18
00:01:05,160 --> 00:01:08,120
on which pretty much all of deep learning rests today.

19
00:01:08,120 --> 00:01:11,680
In 2018, Geoffrey received the Turing Award,

20
00:01:11,680 --> 00:01:14,240
which is often called the Nobel of Computer Science,

21
00:01:14,240 --> 00:01:16,880
alongside Jan LeCun and Yoshia Benjiro.

22
00:01:16,880 --> 00:01:21,160
He's here with us today to talk about intelligence.

23
00:01:21,160 --> 00:01:23,120
What it means and where attempts to build

24
00:01:23,120 --> 00:01:25,240
it into machines will take us.

25
00:01:25,240 --> 00:01:28,240
Geoffrey, welcome to EmTech.

26
00:01:28,240 --> 00:01:29,160
Thank you.

27
00:01:29,160 --> 00:01:30,160
How's your week going?

28
00:01:30,160 --> 00:01:32,240
Busy few days, I imagine.

29
00:01:32,240 --> 00:01:33,960
Well, the last 10 minutes was horrible,

30
00:01:33,960 --> 00:01:36,480
because my computer crashed, and I had to find another computer

31
00:01:36,480 --> 00:01:38,400
and connected up.

32
00:01:38,400 --> 00:01:39,600
And we're glad you're back.

33
00:01:39,600 --> 00:01:40,840
That's the kind of technical detail

34
00:01:40,840 --> 00:01:43,120
we're not supposed to share with the audience.

35
00:01:43,120 --> 00:01:44,120
Right, OK.

36
00:01:44,120 --> 00:01:45,240
It's great you're here.

37
00:01:45,240 --> 00:01:47,120
Very happy that you could join us.

38
00:01:47,120 --> 00:01:49,520
Now, I mean, it's been the news everywhere

39
00:01:49,520 --> 00:01:52,520
that you stepped down from Google this week.

40
00:01:52,520 --> 00:01:55,720
Could you start by telling us why you made that decision?

41
00:01:55,720 --> 00:01:58,320
Well, there were a number of reasons.

42
00:01:58,320 --> 00:02:00,840
This is always a bunch of reasons for a decision like that.

43
00:02:00,840 --> 00:02:03,200
One was that I'm 75,

44
00:02:03,200 --> 00:02:06,520
and I'm not as good at doing technical work, say, used to be.

45
00:02:06,520 --> 00:02:09,480
My memory's not as good,

46
00:02:09,480 --> 00:02:11,600
and when I program, I forget to do things.

47
00:02:11,600 --> 00:02:12,880
So it was time to retire.

48
00:02:12,880 --> 00:02:17,440
A second was, very recently,

49
00:02:17,440 --> 00:02:19,800
I've changed my mind a lot about the relationship

50
00:02:19,800 --> 00:02:23,320
between the brain and the kind of digital intelligence

51
00:02:23,320 --> 00:02:24,120
we're developing.

52
00:02:24,120 --> 00:02:29,440
So I used to think that the computer models

53
00:02:29,440 --> 00:02:32,160
that we were developing weren't as good as the brain.

54
00:02:32,160 --> 00:02:34,520
And the aim was to see if you could understand

55
00:02:34,520 --> 00:02:36,200
more about the brain by seeing what it takes

56
00:02:36,200 --> 00:02:37,720
to improve the computer models.

57
00:02:37,720 --> 00:02:42,560
Over the last few months, I've changed my mind completely

58
00:02:42,560 --> 00:02:45,560
and I think probably the computer models are working

59
00:02:45,560 --> 00:02:47,360
in a rather different way from the brain.

60
00:02:47,360 --> 00:02:49,000
They're using back propagation

61
00:02:49,000 --> 00:02:51,440
And I think the brain is probably not.

62
00:02:51,440 --> 00:02:53,320
And a couple of things have led me to that conclusion,

63
00:02:53,320 --> 00:02:56,680
but what is the performance of things like GPT-4?

64
00:02:56,680 --> 00:02:59,840
- So let's, I wanna get onto the performance of GPT-4

65
00:02:59,840 --> 00:03:02,320
very much in a minute, but let's, you know,

66
00:03:02,320 --> 00:03:05,800
go back so that we all understand the argument you're making.

67
00:03:05,800 --> 00:03:09,480
And tell us a little bit about what back propagation is.

68
00:03:09,480 --> 00:03:11,520
And this is an algorithm that you developed

69
00:03:11,520 --> 00:03:13,880
with a couple of colleagues back in the 1980s.

70
00:03:13,880 --> 00:03:18,480
- Many different groups discover back propagation.

71
00:03:18,480 --> 00:03:26,480
The special thing we did was used it and showed that it could develop good internal representations.

72
00:03:26,480 --> 00:03:34,480
And curiously, we did that by implementing a tiny language model.

73
00:03:34,480 --> 00:03:39,480
It had embedding vectors that were only six components.

74
00:03:39,480 --> 00:03:43,480
On the training set was 112 cases.

75
00:03:43,480 --> 00:03:45,200
But it was a language model.

76
00:03:45,200 --> 00:03:50,320
It was trying to predict the next term in a string of symbols.

77
00:03:50,320 --> 00:03:53,920
And about 10 years later, Yoshua Benjue

78
00:03:53,920 --> 00:03:55,680
took basically the same net.

79
00:03:55,680 --> 00:03:56,840
Then used on natural language.

80
00:03:56,840 --> 00:03:58,880
You showed it actually worked for natural language,

81
00:03:58,880 --> 00:04:02,000
if you made it much bigger.

82
00:04:02,000 --> 00:04:05,800
But the way back propagation works,

83
00:04:05,800 --> 00:04:09,400
I can give you a rough explanation from it of it.

84
00:04:09,400 --> 00:04:12,560
People who know how it works can sort of sit back

85
00:04:12,560 --> 00:04:15,920
and feel smug and laugh at the way I'm presenting it.

86
00:04:15,920 --> 00:04:18,320
Okay, because I'm a bit worried about that.

87
00:04:18,320 --> 00:04:20,400
(laughs)

88
00:04:20,400 --> 00:04:24,680
So imagine you wanted to detect birds and images.

89
00:04:24,680 --> 00:04:28,800
So an image, let's suppose it was a 100 pixel

90
00:04:28,800 --> 00:04:30,480
by 100 pixel image.

91
00:04:30,480 --> 00:04:32,160
That's 10,000 pixels.

92
00:04:32,160 --> 00:04:34,760
An each pixel is three channels RGB.

93
00:04:34,760 --> 00:04:37,640
So that's 30,000 numbers.

94
00:04:37,640 --> 00:04:39,920
The intensity in each channel in each pixel,

95
00:04:39,920 --> 00:04:41,800
the represents the image.

96
00:04:41,800 --> 00:04:44,120
And the way to think of the computer vision problem is,

97
00:04:44,120 --> 00:04:46,320
how do I turn those 30,000 numbers

98
00:04:46,320 --> 00:04:49,520
into a decision about whether it's a bird or not?

99
00:04:49,520 --> 00:04:51,200
And people tried for a long time to do that,

100
00:04:51,200 --> 00:04:53,400
and they weren't very good at it.

101
00:04:53,400 --> 00:04:56,320
But here's the suggestion for how you might do it.

102
00:04:56,320 --> 00:04:59,280
You might have a layer of feature detectors

103
00:04:59,280 --> 00:05:01,880
that detects very simple features and images,

104
00:05:01,880 --> 00:05:03,520
like for example, edges.

105
00:05:03,520 --> 00:05:07,920
So a feature detector might have big positive weights

106
00:05:07,920 --> 00:05:10,280
to a column of pixels.

107
00:05:10,280 --> 00:05:12,440
and then big negative weights to the neighboring column

108
00:05:12,440 --> 00:05:14,120
of big cells.

109
00:05:14,120 --> 00:05:16,920
So if both columns are bright, it won't turn on.

110
00:05:16,920 --> 00:05:19,000
And if both columns are dim, it won't turn off.

111
00:05:19,000 --> 00:05:21,360
But if the column on one side is bright,

112
00:05:21,360 --> 00:05:23,440
and the column on the other side is dim,

113
00:05:23,440 --> 00:05:24,960
it'll get very excited.

114
00:05:24,960 --> 00:05:26,960
And that's an edge detector.

115
00:05:26,960 --> 00:05:30,000
So I just told you how to wire up an edge detector by hand

116
00:05:30,000 --> 00:05:31,680
by having one column of big positive weights

117
00:05:31,680 --> 00:05:34,000
and next to it one column of big negative weights.

118
00:05:34,000 --> 00:05:35,840
And we can imagine a big layer of those,

119
00:05:35,840 --> 00:05:37,600
detecting edges in different orientations

120
00:05:37,600 --> 00:05:39,880
and different scales all over the image.

121
00:05:39,880 --> 00:05:41,960
We'd need a rather large number of them.

122
00:05:41,960 --> 00:05:43,000
- And edges in an image,

123
00:05:43,000 --> 00:05:45,720
we mean just lines, sort of edges of a shape.

124
00:05:45,720 --> 00:05:49,000
- A place where the intensity changes from right to dark.

125
00:05:49,000 --> 00:05:52,320
Yeah, just that.

126
00:05:52,320 --> 00:05:55,160
Then we might have a layer of future textures above that,

127
00:05:55,160 --> 00:05:57,320
the detect combinations of edges.

128
00:05:57,320 --> 00:06:02,200
So for example, we might have something that detects two edges,

129
00:06:02,200 --> 00:06:05,360
the join at a fine angle like this.

130
00:06:07,320 --> 00:06:10,960
So they'll have a big positive weight to each of those two edges.

131
00:06:10,960 --> 00:06:13,360
And if both of those edges are at the same time,

132
00:06:13,360 --> 00:06:15,160
it'll get excited.

133
00:06:15,160 --> 00:06:18,640
And that would detect something that might be a bird's beak.

134
00:06:18,640 --> 00:06:20,680
It might not, but it might be a bird's beak.

135
00:06:20,680 --> 00:06:23,920
You might also in that layer have a feature detector

136
00:06:23,920 --> 00:06:26,240
that would detect a whole bunch of edges,

137
00:06:26,240 --> 00:06:27,240
arrange in a circle.

138
00:06:27,240 --> 00:06:30,560
And that might be a bird's eye.

139
00:06:30,560 --> 00:06:31,920
It might be also some other things.

140
00:06:31,920 --> 00:06:34,080
It might be a knob on a fridge or something.

141
00:06:36,760 --> 00:06:39,800
Then in a third layer, you might have a feature detector

142
00:06:39,800 --> 00:06:44,320
that detects this potential beak and detects the potential eye

143
00:06:44,320 --> 00:06:47,280
and is wired up so it'll like a beak and an eye

144
00:06:47,280 --> 00:06:49,640
in the right spatial relation to one another.

145
00:06:49,640 --> 00:06:53,400
And if it sees that, it says, ah, this might be the head of a bird.

146
00:06:53,400 --> 00:06:56,040
And you can imagine if you keep wiring like that,

147
00:06:56,040 --> 00:06:59,760
you could eventually have something that detects a bird.

148
00:06:59,760 --> 00:07:03,680
But wiring all up by hand would be very, very difficult.

149
00:07:03,680 --> 00:07:05,640
Decide on what should be connected to water

150
00:07:05,640 --> 00:07:07,220
what the weights should be.

151
00:07:07,220 --> 00:07:08,560
And it would be especially difficult,

152
00:07:08,560 --> 00:07:10,720
because you want these intermediate layers

153
00:07:10,720 --> 00:07:12,840
to be good not just for detecting birds,

154
00:07:12,840 --> 00:07:14,880
but for detecting all sorts of other things.

155
00:07:14,880 --> 00:07:19,960
So it would be more or less impossible to worry about by hand.

156
00:07:19,960 --> 00:07:23,200
So the way back propagation works is this,

157
00:07:23,200 --> 00:07:25,100
you start with random weights.

158
00:07:25,100 --> 00:07:27,520
So these feature detectors are just complete rubbish.

159
00:07:27,520 --> 00:07:31,080
And you put in a picture of a bird,

160
00:07:31,080 --> 00:07:34,960
and at the output it says like 0.5 it's a bird.

161
00:07:34,960 --> 00:07:37,560
Suppose you only have birds or non-birds.

162
00:07:37,560 --> 00:07:40,160
And then you ask yourself the following question,

163
00:07:40,160 --> 00:07:44,920
how could I change each of the weights in the network

164
00:07:44,920 --> 00:07:47,240
by each of the weights on connections in the network?

165
00:07:47,240 --> 00:07:52,080
So instead of saying 0.5, it says 0.501, but it's a bird,

166
00:07:52,080 --> 00:07:55,040
and 0.499 that it's not.

167
00:07:55,040 --> 00:07:57,840
And you change the weights in the directions

168
00:07:57,840 --> 00:08:01,760
that will make it more likely to say that a bird is a bird,

169
00:08:01,760 --> 00:08:04,440
unless I actually say that a non-bird is a bird.

170
00:08:04,440 --> 00:08:07,120
And you just keep doing that.

171
00:08:07,120 --> 00:08:08,160
And that's back propagation.

172
00:08:08,160 --> 00:08:11,600
Back propagation is actually how you take the discrepancy

173
00:08:11,600 --> 00:08:13,260
between what you want,

174
00:08:13,260 --> 00:08:15,640
which is a probability of one that is a bird,

175
00:08:15,640 --> 00:08:16,600
and what is got a present,

176
00:08:16,600 --> 00:08:19,080
which is probability of 0.5 that is a bird,

177
00:08:19,080 --> 00:08:20,840
how you take that discrepancy

178
00:08:20,840 --> 00:08:23,680
and send it backwards through the network

179
00:08:23,680 --> 00:08:26,680
so that you can compute for every feature

180
00:08:26,680 --> 00:08:28,160
that's actually in the network,

181
00:08:28,160 --> 00:08:29,960
whether you'd like it to be a bit more active

182
00:08:29,960 --> 00:08:33,720
a bit less active. And once you've computed that, if you know you want a feature detector

183
00:08:33,720 --> 00:08:38,560
a bit more active, you can increase the weights coming from feature detects in the low

184
00:08:38,560 --> 00:08:44,200
below that are active and maybe put in some negative weights to feature detects in the

185
00:08:44,200 --> 00:08:49,520
low below that are off and now you'll have a better detector. So back propagation is

186
00:08:49,520 --> 00:08:52,600
just going backwards through the network to figure out a feature detector whether you

187
00:08:52,600 --> 00:08:55,760
want to do a little bit more active or a little bit less active.

188
00:08:55,760 --> 00:08:58,560
Thank you. I can show you that there's no one in the audience here that's smiling and

189
00:08:58,560 --> 00:09:03,280
thinking that was a silly explanation.

190
00:09:03,280 --> 00:09:06,280
So let's fast forward quite a lot to--

191
00:09:06,280 --> 00:09:11,320
that technique basically performed really well on ImageNet.

192
00:09:11,320 --> 00:09:13,760
We had Joe Lpno from Meta yesterday

193
00:09:13,760 --> 00:09:17,120
showing how far Image Detection had come.

194
00:09:17,120 --> 00:09:20,240
And it's also the technique that underpins large language

195
00:09:20,240 --> 00:09:21,400
models.

196
00:09:21,400 --> 00:09:27,400
So I want to talk now about this technique, which you initially

197
00:09:27,400 --> 00:09:34,320
were thinking of as almost like a poor approximation of what biological brains might do.

198
00:09:34,320 --> 00:09:41,120
Has turned out to do things which I think have stunned you, particularly in large language

199
00:09:41,120 --> 00:09:42,120
models.

200
00:09:42,120 --> 00:09:48,720
So, talk to us about why that sort of amazement that you have with today's large language

201
00:09:48,720 --> 00:09:54,720
models has completely sort of almost flipped your thinking of what backpropagation or machine

202
00:09:54,720 --> 00:10:00,880
learning in general is. So if you look at these large language models, they have about

203
00:10:00,880 --> 00:10:08,280
a trillion connections and things like GPT-4 know much more than we do. They have sort

204
00:10:08,280 --> 00:10:13,920
of common sense knowledge about everything. And so they probably know a thousand times

205
00:10:13,920 --> 00:10:19,040
as much as a person. But they've got a trillion connections and we've got a hundred trillion

206
00:10:19,040 --> 00:10:24,320
connections. So they're much, much better at getting a lot of knowledge into only

207
00:10:24,320 --> 00:10:29,840
a trillion connections than we are. And I think it's because back propagation

208
00:10:29,840 --> 00:10:33,800
may be a much, much better learning algorithm than what we've got. Can you define

209
00:10:33,800 --> 00:10:37,800
that scary? Yeah, I definitely want to get onto the scary stuff. But what do you mean by

210
00:10:37,800 --> 00:10:44,760
better? It can pack more information into only a few connections. Right. We're

211
00:10:44,760 --> 00:10:47,480
which are finding a trillion as only a few.

212
00:10:47,480 --> 00:10:48,480
- Okay.

213
00:10:48,480 --> 00:10:52,820
So these digital computers are better at learning

214
00:10:52,820 --> 00:10:57,320
than humans, which itself is a huge claim.

215
00:10:57,320 --> 00:11:01,880
But then you also argue that that's something

216
00:11:01,880 --> 00:11:03,320
that we should be scared of.

217
00:11:03,320 --> 00:11:06,160
So could you take us through that step of the argument?

218
00:11:06,160 --> 00:11:09,240
- Yeah, let me give you a separate piece of the argument,

219
00:11:09,240 --> 00:11:14,240
which is that if a computer is a computer,

220
00:11:14,240 --> 00:11:17,320
a computer's digital, which involves very high energy

221
00:11:17,320 --> 00:11:19,760
costs and very careful fabrication.

222
00:11:19,760 --> 00:11:21,960
You can have many copies of the same model running

223
00:11:21,960 --> 00:11:25,120
on different hardware that do exactly the same thing.

224
00:11:25,120 --> 00:11:26,720
They can look at different data, but the model

225
00:11:26,720 --> 00:11:28,120
is actually the same.

226
00:11:28,120 --> 00:11:30,080
And what that means is, especially

227
00:11:30,080 --> 00:11:33,920
I have 10,000 copies, they can be looking at 10,000 different

228
00:11:33,920 --> 00:11:35,960
subsets of the data.

229
00:11:35,960 --> 00:11:38,600
And whenever one of them learns anything, all the others

230
00:11:38,600 --> 00:11:39,920
know it.

231
00:11:39,920 --> 00:11:41,640
One of them figures out how to change the weight,

232
00:11:41,640 --> 00:11:44,680
so it can deal with this data.

233
00:11:44,680 --> 00:11:46,160
They all communicate with each other

234
00:11:46,160 --> 00:11:47,860
and they all agree to change the way

235
00:11:47,860 --> 00:11:49,960
it's by the average of what all of them want.

236
00:11:49,960 --> 00:11:55,920
And now the 10,000 things are communicating

237
00:11:55,920 --> 00:11:58,480
very effectively with each other

238
00:11:58,480 --> 00:12:01,440
so that they can see 10,000 times as much data

239
00:12:01,440 --> 00:12:03,240
as one agent could.

240
00:12:03,240 --> 00:12:05,100
And people can't do that.

241
00:12:05,100 --> 00:12:07,920
If I learn a whole lot of stuff about quantum mechanics

242
00:12:07,920 --> 00:12:09,200
and I want you to know all that stuff

243
00:12:09,200 --> 00:12:10,600
about quantum mechanics,

244
00:12:10,600 --> 00:12:14,240
It's a long, painful process of getting you to understand it.

245
00:12:14,240 --> 00:12:18,180
I can't just copy my weights into your brain,

246
00:12:18,180 --> 00:12:20,160
because your brain isn't exactly the same as mine.

247
00:12:20,160 --> 00:12:21,160
- No, it's not.

248
00:12:21,160 --> 00:12:24,600
(audience laughing)

249
00:12:24,600 --> 00:12:25,440
It's yoga.

250
00:12:25,440 --> 00:12:28,440
(audience laughing)

251
00:12:28,440 --> 00:12:34,040
- So we have digital computers that can learn more things

252
00:12:34,040 --> 00:12:39,960
more quickly and they can instantly teach it to each other.

253
00:12:39,960 --> 00:12:44,160
It's like, you know, people in the room here could initially transfer what they had in

254
00:12:44,160 --> 00:12:46,320
their heads in mind.

255
00:12:46,320 --> 00:12:48,960
But why is that scary?

256
00:12:48,960 --> 00:12:56,640
Well, because they can learn so much more and they might take an example of a doctor and

257
00:12:56,640 --> 00:13:02,920
imagine you have one doctor who's seen a thousand patients and another doctor who's seen

258
00:13:02,920 --> 00:13:05,800
a hundred million patients.

259
00:13:05,800 --> 00:13:10,520
You would expect the doctor to see 100 million patients, if he's not too forgetful, to

260
00:13:10,520 --> 00:13:14,520
have noticed all sorts of trends in the data that just aren't visible if you've only seen

261
00:13:14,520 --> 00:13:16,520
a thousand patients.

262
00:13:16,520 --> 00:13:19,920
You may have only seen one patient with some rare disease.

263
00:13:19,920 --> 00:13:23,320
The other doctor has seen 100 million, we'll have seen whether you can figure out how

264
00:13:23,320 --> 00:13:26,120
many patients but a lot.

265
00:13:26,120 --> 00:13:32,160
And so we'll see also the regularity suggests that aren't apparent in small data.

266
00:13:32,160 --> 00:13:38,320
And that's why things that can get through a lot of data can probably see structuring data

267
00:13:38,320 --> 00:13:42,760
we'll never see.

268
00:13:42,760 --> 00:13:48,160
And then take me to the point where I should be scared of this though.

269
00:13:48,160 --> 00:13:53,160
Well, if you look at GPT-4, it can already do simple reasoning.

270
00:13:53,160 --> 00:13:57,480
I mean, reasoning is the area where we're still better.

271
00:13:57,480 --> 00:14:02,640
But I was impressed the other day, GPT-4, doing a piece of common sense reasoning that I

272
00:14:02,640 --> 00:14:04,680
didn't think it would be able to do.

273
00:14:04,680 --> 00:14:12,880
So I asked it, I want all the rooms in my house to be white, a present, the some white

274
00:14:12,880 --> 00:14:16,640
rooms, some blue rooms, and some yellow rooms.

275
00:14:16,640 --> 00:14:20,000
And yellow paint fades to white within a year.

276
00:14:20,000 --> 00:14:25,440
So what should I do if I want them all to be white in two years' time?

277
00:14:25,440 --> 00:14:29,280
made said, you should paint the blue runs yellow.

278
00:14:29,280 --> 00:14:31,840
That's not the natural solution, but it works, right?

279
00:14:31,840 --> 00:14:36,440
That's pretty impressive common sense reasoning

280
00:14:36,440 --> 00:14:40,080
of the kind that it's been very hard to get AI to do

281
00:14:40,080 --> 00:14:41,840
using symbolic AI.

282
00:14:41,840 --> 00:14:44,360
'Cause I had to understand what fades,

283
00:14:44,360 --> 00:14:47,840
it means it had to understood by temple stuff.

284
00:14:47,840 --> 00:14:54,120
So they're doing sort of sensible reasoning.

285
00:14:54,120 --> 00:15:00,120
with an IQ of like 80 or 90 or something.

286
00:15:00,120 --> 00:15:04,120
And as a friend of mine said,

287
00:15:04,120 --> 00:15:07,120
it's as if some genetic engineers

288
00:15:07,120 --> 00:15:09,120
have said we're going to improve grizzly bears.

289
00:15:09,120 --> 00:15:12,120
We've already improved them to have an IQ of 65

290
00:15:12,120 --> 00:15:14,120
and they can talk English now.

291
00:15:14,120 --> 00:15:16,120
And they're very useful for all sorts of things.

292
00:15:16,120 --> 00:15:19,120
But we think we can improve the IQ to 210.

293
00:15:19,120 --> 00:15:23,120
[LAUGHTER]

294
00:15:23,120 --> 00:15:30,120
I certainly had, I'm sure many people have had that feeling when you're interacting with

295
00:15:30,120 --> 00:15:34,120
these latest chat bots, you know, sort of hair on the back of neck, it's sort of uncanny feeling.

296
00:15:34,120 --> 00:15:39,120
But, you know, when I have that feeling and I'm uncomfortable, I just close my laptop.

297
00:15:39,120 --> 00:15:48,120
So, yes, but these things will have learned from us by reading all the novels that ever were

298
00:15:48,120 --> 00:15:57,120
Mackey Belly ever wrote that how to manipulate people, right? And if they're much smarter

299
00:15:57,120 --> 00:16:00,800
unless they'll be very good at manipulating us, you won't realise what's going on. You'll

300
00:16:00,800 --> 00:16:05,400
beg you like a two-year-old who's being asked, "Do you want the peas or the cauliflower?"

301
00:16:05,400 --> 00:16:12,700
And doesn't realise you don't have to have either. And you'll be that easy to manipulate.

302
00:16:12,700 --> 00:16:18,740
And so even if they can't directly pull levers, they can certainly get us to pull levers.

303
00:16:18,740 --> 00:16:23,540
It turns out if you can manipulate people, you can invade a building in Washington without

304
00:16:23,540 --> 00:16:25,700
ever going there yourself.

305
00:16:25,700 --> 00:16:29,980
Very good.

306
00:16:29,980 --> 00:16:30,980
Yeah.

307
00:16:30,980 --> 00:16:31,980
So is that?

308
00:16:31,980 --> 00:16:32,980
Is that?

309
00:16:32,980 --> 00:16:38,860
I mean, if the word, okay, this is a very hypothetical world, but if there were no bad

310
00:16:38,860 --> 00:16:45,860
actors, you know, people with bad intentions, would we be safe?

311
00:16:45,860 --> 00:16:47,620
I don't know.

312
00:16:47,620 --> 00:16:52,300
Would be safer than in a world where people have bad intentions and where the political

313
00:16:52,300 --> 00:16:59,180
system is so broken that we can't even decide not to give assault rifles to teenage boys.

314
00:16:59,180 --> 00:17:02,340
If you can't solve that problem, how are you going to solve this problem?

315
00:17:02,340 --> 00:17:03,940
Well, I mean, I don't know.

316
00:17:03,940 --> 00:17:06,300
I was hoping that you would have some thoughts.

317
00:17:06,300 --> 00:17:08,460
[LAUGHTER]

318
00:17:08,460 --> 00:17:11,060
So one--

319
00:17:11,060 --> 00:17:14,100
I mean, unless we didn't make this clear at the beginning,

320
00:17:14,100 --> 00:17:17,420
I mean, you want to speak out about this

321
00:17:17,420 --> 00:17:20,420
and you feel more comfortable doing that without it,

322
00:17:20,420 --> 00:17:24,740
sort of having any blowback on Google.

323
00:17:24,740 --> 00:17:26,100
But you're speaking out about it.

324
00:17:26,100 --> 00:17:32,060
But in some sense, talk is cheap if we then don't have actions

325
00:17:32,060 --> 00:17:37,260
What do we do when we lots of people this week are listening to you? What should we do about it?

326
00:17:37,260 --> 00:17:44,620
I wish it was like climate change where you could say if you've got half a brain you'd stop burning carbon.

327
00:17:44,620 --> 00:17:49,820
It's clear what you should do about it. It's clear that that's painful but has to be done.

328
00:17:49,820 --> 00:17:55,580
I don't know of any solution like that to stop these things taking over from us.

329
00:17:55,580 --> 00:17:59,660
What we really want, I don't think we're going to stop developing them because they're so useful.

330
00:17:59,660 --> 00:18:01,660
They'll be incredibly useful in medicine.

331
00:18:01,660 --> 00:18:03,660
And in everything else.

332
00:18:03,660 --> 00:18:06,660
So I don't think there's much chance of stopping development.

333
00:18:06,660 --> 00:18:09,660
What we want is some way of making sure that,

334
00:18:09,660 --> 00:18:11,660
even if they're smarter than us,

335
00:18:11,660 --> 00:18:14,660
they're going to do things that are beneficial for us.

336
00:18:14,660 --> 00:18:16,660
That's called the alignment problem.

337
00:18:16,660 --> 00:18:19,660
But we need to try and do that in a world where

338
00:18:19,660 --> 00:18:22,660
there's bad actors who want to build robot soldiers

339
00:18:22,660 --> 00:18:24,660
that kill people.

340
00:18:24,660 --> 00:18:26,660
And it seems very hard to me.

341
00:18:26,660 --> 00:18:31,020
So I'm sorry, I'm sounding the alarm and saying we have to worry about this.

342
00:18:31,020 --> 00:18:34,460
And I wish I had a nice simple solution I could push, but I don't.

343
00:18:34,460 --> 00:18:38,940
But I think it's very important that people get together and think hard about it and see whether there is a solution.

344
00:18:38,940 --> 00:18:40,860
It's not clear there is a solution.

345
00:18:40,860 --> 00:18:43,700
So I mean, talk to us about that.

346
00:18:43,700 --> 00:18:49,260
I mean, you spent your career on the technicalities of this technology.

347
00:18:49,260 --> 00:18:51,500
Is there no technical fix?

348
00:18:51,500 --> 00:18:54,420
Why can we not build in guardrails or...

349
00:18:54,420 --> 00:19:00,780
and you make them worse at learning or restrict the way that they can communicate

350
00:19:00,780 --> 00:19:03,780
if those are the two strings of your argument.

351
00:19:03,780 --> 00:19:07,780
I mean, we're trying to do all sorts of guard routes.

352
00:19:07,780 --> 00:19:10,340
But suppose they did get really smart.

353
00:19:10,340 --> 00:19:11,620
And these things can program, right?

354
00:19:11,620 --> 00:19:13,100
They can write programs.

355
00:19:13,100 --> 00:19:19,980
And suppose you give them the ability to execute those programs, which we'll certainly do.

356
00:19:19,980 --> 00:19:23,900
Smart things can outsmart us.

357
00:19:23,900 --> 00:19:29,500
So imagine you're two-year-old saying,

358
00:19:29,500 --> 00:19:31,260
my dad does things I don't like.

359
00:19:31,260 --> 00:19:34,700
So I'm going to make some rules for what my dad can do.

360
00:19:34,700 --> 00:19:36,780
You could probably figure out how to live with those rules

361
00:19:36,780 --> 00:19:39,260
and still go where you want.

362
00:19:39,260 --> 00:19:41,140
Yeah.

363
00:19:41,140 --> 00:19:43,220
But where--

364
00:19:43,220 --> 00:19:47,620
there still seems to be a step where these smart machines

365
00:19:47,620 --> 00:19:50,700
somehow have motivation of their own.

366
00:19:50,700 --> 00:19:51,580
Yes.

367
00:19:51,580 --> 00:19:52,820
Yes, that's a very good point.

368
00:19:52,820 --> 00:19:55,940
So we evolved.

369
00:19:55,940 --> 00:19:59,700
And because we evolved, we have certain building goals

370
00:19:59,700 --> 00:20:01,860
that we find very hard to turn off.

371
00:20:01,860 --> 00:20:04,300
Like we try not to damage our bodies.

372
00:20:04,300 --> 00:20:05,700
That's what pain's about.

373
00:20:05,700 --> 00:20:08,780
We try and get enough to eat.

374
00:20:08,780 --> 00:20:10,100
So we feed our bodies.

375
00:20:10,100 --> 00:20:16,660
We try and make as many copies of ourselves as possible.

376
00:20:16,660 --> 00:20:19,060
Maybe not deliberately that intention,

377
00:20:19,060 --> 00:20:21,340
but we've been wired up so this pleasure involved

378
00:20:21,340 --> 00:20:23,780
making many copies of ourselves.

379
00:20:23,780 --> 00:20:27,420
And that all came from evolution.

380
00:20:27,420 --> 00:20:30,740
And it's important that we can't turn it off.

381
00:20:30,740 --> 00:20:34,620
If you could turn it off, you don't do so well.

382
00:20:34,620 --> 00:20:36,180
There's a wonderful group called The Shakers,

383
00:20:36,180 --> 00:20:37,260
who were related to The Quakers.

384
00:20:37,260 --> 00:20:41,420
You make beautiful furniture, but didn't believe in sex.

385
00:20:41,420 --> 00:20:43,260
And there aren't any of them around anymore.

386
00:20:43,260 --> 00:20:46,100
[LAUGHTER]

387
00:20:46,100 --> 00:20:51,140
So these digital intelligences didn't evolve.

388
00:20:51,140 --> 00:20:55,900
We made them and so they don't have these building goals.

389
00:20:55,900 --> 00:20:59,100
And so the issue is, if we can put the goals in,

390
00:20:59,100 --> 00:21:00,880
maybe it'll be okay.

391
00:21:00,880 --> 00:21:02,520
But my big worry is,

392
00:21:02,520 --> 00:21:05,620
sooner or later, someone will wire into them

393
00:21:05,620 --> 00:21:08,380
the ability to create their own sub-gots.

394
00:21:08,380 --> 00:21:09,780
In fact, they almost have that already.

395
00:21:09,780 --> 00:21:13,100
The versions of chat GPT that call chat GPT.

396
00:21:13,100 --> 00:21:17,260
And if you give something,

397
00:21:17,260 --> 00:21:18,900
the ability to create your own sub-gots

398
00:21:18,900 --> 00:21:20,980
in order to achieve other goals,

399
00:21:20,980 --> 00:21:26,580
I think it will very quickly realize that getting more control is a very good subgoal because

400
00:21:26,580 --> 00:21:29,820
it helps you achieve other goals.

401
00:21:29,820 --> 00:21:35,060
And if these things get carried away with getting more control, we're in trouble.

402
00:21:35,060 --> 00:21:39,340
So what's the worst case scenario that you think is conceivable?

403
00:21:39,340 --> 00:21:46,040
Oh, I think it's quite conceivable that humanity is just a passing phase in the evolution

404
00:21:46,040 --> 00:21:47,460
of intelligence.

405
00:21:47,460 --> 00:21:49,520
You couldn't directly evolve digital intelligence.

406
00:21:49,520 --> 00:21:53,960
It requires too much energy and too much careful fabrication.

407
00:21:53,960 --> 00:21:59,640
You need biological intelligence to evolve so that it can create digital intelligence.

408
00:21:59,640 --> 00:22:07,000
The digital intelligence can then absorb everything people ever wrote in a fairly slow way,

409
00:22:07,000 --> 00:22:10,720
which is what Chachybethi has been doing.

410
00:22:10,720 --> 00:22:15,520
But then it can start getting direct experience of the world and learn much faster.

411
00:22:15,520 --> 00:22:22,960
And it may keep us around for a while to keep the pastations running, but after that,

412
00:22:22,960 --> 00:22:23,960
maybe not.

413
00:22:23,960 --> 00:22:29,480
So, the good news is we've figured out how to build beings that are immortal.

414
00:22:29,480 --> 00:22:34,720
So, these digital intelligences, when a piece of hardware dies, they don't die.

415
00:22:34,720 --> 00:22:39,280
If you've got the weight stored in some medium and you can find another piece of hardware

416
00:22:39,280 --> 00:22:43,720
that can run the same instructions, then you can bring it to life again.

417
00:22:43,720 --> 00:22:48,720
So we've got immortality, but it's not for us.

418
00:22:48,720 --> 00:22:52,720
So, Ray Kurzweil is very interested in being immortal.

419
00:22:52,720 --> 00:22:56,720
I think it's a very bad idea for Old Whiteman to be immortal.

420
00:22:56,720 --> 00:23:01,720
We've got the immortality, but it's not for Ray.

421
00:23:01,720 --> 00:23:04,720
No, the scary thing is that in a way maybe you will be,

422
00:23:04,720 --> 00:23:09,720
because you invented much of this technology.

423
00:23:09,720 --> 00:23:13,240
I mean, when I hear you say this, probably,

424
00:23:13,240 --> 00:23:15,120
what's the, you know, Rafa's age into the street now

425
00:23:15,120 --> 00:23:16,840
and start unplugging computers.

426
00:23:16,840 --> 00:23:18,840
(audience laughing)

427
00:23:18,840 --> 00:23:19,840
And--

428
00:23:19,840 --> 00:23:21,480
- I'm afraid we can't do that.

429
00:23:21,480 --> 00:23:22,480
- Why?

430
00:23:22,480 --> 00:23:24,200
You sound like Hal from 2001.

431
00:23:24,200 --> 00:23:25,040
- Exactly.

432
00:23:25,040 --> 00:23:28,040
(audience laughing)

433
00:23:28,040 --> 00:23:31,160
- But, I mean, more seriously,

434
00:23:31,160 --> 00:23:34,960
I mean, I know you said before that,

435
00:23:34,960 --> 00:23:36,840
it was suggested a few months ago

436
00:23:36,840 --> 00:23:41,840
that there should be a moratorium on AI advancement.

437
00:23:41,840 --> 00:23:45,120
And I don't think that's a very good idea.

438
00:23:45,120 --> 00:23:48,760
But more generally, I'm curious why.

439
00:23:48,760 --> 00:23:50,400
I mean, should we not just stop?

440
00:23:50,400 --> 00:23:53,000
And I know, I mean, you're, you're,

441
00:23:53,000 --> 00:23:54,240
sorry, I was just gonna say that, you know,

442
00:23:54,240 --> 00:23:56,840
I know that you've spoken also that you're an investor

443
00:23:56,840 --> 00:23:59,880
of your personal wealth in some companies like Cohere

444
00:23:59,880 --> 00:24:01,320
that are building these large language models.

445
00:24:01,320 --> 00:24:04,040
So I just curious about your personal sense of responsibility

446
00:24:04,040 --> 00:24:06,480
and each of our personal responsibility.

447
00:24:06,480 --> 00:24:07,940
what should we be doing?

448
00:24:07,940 --> 00:24:10,920
I mean, should we try and stop this is what I'm saying?

449
00:24:10,920 --> 00:24:13,360
- Yeah, so I think if you take the existential risk

450
00:24:13,360 --> 00:24:16,680
seriously, as I now do, I used to think it was way off,

451
00:24:16,680 --> 00:24:20,400
but I now think it's serious and fairly close.

452
00:24:20,400 --> 00:24:23,440
It might be quite sensible to just stop developing

453
00:24:23,440 --> 00:24:26,680
these things any further, but I think it's completely

454
00:24:26,680 --> 00:24:28,840
an aim to think that would happen.

455
00:24:28,840 --> 00:24:30,520
There's no way to make that happen.

456
00:24:30,520 --> 00:24:34,000
And one reason, I mean, if the US stops developing

457
00:24:34,000 --> 00:24:35,640
in the Chinese vote,

458
00:24:35,640 --> 00:24:39,760
they're going to be used in weapons and just for that reason alone governments are going

459
00:24:39,760 --> 00:24:41,840
to stop developing them.

460
00:24:41,840 --> 00:24:47,280
So yes, I think stopping developing them might be a rational thing to do but there's

461
00:24:47,280 --> 00:24:48,520
no way it's going to happen.

462
00:24:48,520 --> 00:24:51,600
So it's silly to sign petition saying, please stop now.

463
00:24:51,600 --> 00:24:52,920
We did have a holiday.

464
00:24:52,920 --> 00:24:58,840
We had a holiday from about 2017 for several years because Google developed the technology

465
00:24:58,840 --> 00:24:59,840
first.

466
00:24:59,840 --> 00:25:00,840
It developed the transform.

467
00:25:00,840 --> 00:25:04,400
transform with it also to the diffusion monitors.

468
00:25:04,400 --> 00:25:06,640
And it didn't put them out there for people to use

469
00:25:06,640 --> 00:25:07,640
and abuse.

470
00:25:07,640 --> 00:25:09,040
It was very careful with them, because it

471
00:25:09,040 --> 00:25:10,400
didn't want to damage his reputation.

472
00:25:10,400 --> 00:25:13,120
And it knew there could be bad consequences.

473
00:25:13,120 --> 00:25:16,160
But that can only happen if there's a single leader.

474
00:25:16,160 --> 00:25:22,960
Once OpenAI had built similar things using transformers

475
00:25:22,960 --> 00:25:24,800
and money from Microsoft.

476
00:25:24,800 --> 00:25:27,840
And Microsoft decided to put it out there.

477
00:25:27,840 --> 00:25:29,760
Google didn't have really much choice.

478
00:25:29,760 --> 00:25:31,440
If you're going to live in a capitalist system,

479
00:25:31,440 --> 00:25:35,600
you can't stop Google competing with Microsoft.

480
00:25:35,600 --> 00:25:38,600
So I don't think Google did anything wrong.

481
00:25:38,600 --> 00:25:40,560
I think it was very responsible to begin with.

482
00:25:40,560 --> 00:25:43,160
But I think it's just inevitable in a capitalist system,

483
00:25:43,160 --> 00:25:45,000
or a system with competition between countries,

484
00:25:45,000 --> 00:25:49,600
like the US and China, that this stuff will be developed.

485
00:25:49,600 --> 00:25:52,800
My one hope is that because if we allow

486
00:25:52,800 --> 00:25:55,520
to take over, it will be bad for all of us.

487
00:25:55,520 --> 00:25:57,240
We could get the US and China to agree,

488
00:25:57,240 --> 00:25:59,920
like we could with nuclear weapons, which we're about for all of us.

489
00:25:59,920 --> 00:26:00,120
Yeah.

490
00:26:00,120 --> 00:26:02,920
We're all in the same boat with respect to the existential threat.

491
00:26:02,920 --> 00:26:06,800
So we all ought to be able to cooperate on trying to stop it.

492
00:26:06,800 --> 00:26:10,120
As long as we can make some money on the way.

493
00:26:10,120 --> 00:26:12,360
I'm going to take some audience questions from the room

494
00:26:12,360 --> 00:26:14,200
if you make yourself known.

495
00:26:14,200 --> 00:26:15,800
And while people are going on with the microphone,

496
00:26:15,800 --> 00:26:17,640
there's one question I was going to ask

497
00:26:17,640 --> 00:26:19,400
from the online audience.

498
00:26:19,400 --> 00:26:20,120
I'm interested.

499
00:26:20,120 --> 00:26:23,120
You mentioned a little bit about maybe transition period,

500
00:26:23,120 --> 00:26:26,800
as machines get smarter and outpace humans.

501
00:26:26,800 --> 00:26:29,920
I mean, there'll be a moment where it's hard to define

502
00:26:29,920 --> 00:26:31,720
what's human and what isn't,

503
00:26:31,720 --> 00:26:35,480
or are these two very distinct forms of intelligence?

504
00:26:35,480 --> 00:26:38,000
- I think they're distinct forms of intelligence.

505
00:26:38,000 --> 00:26:41,240
Now, of course, the digital intelligences

506
00:26:41,240 --> 00:26:42,720
are very good at mimicking us

507
00:26:42,720 --> 00:26:45,040
because they've been trained to mimic us.

508
00:26:45,040 --> 00:26:49,680
And so it's very hard to tell if chat GBT wrote it,

509
00:26:49,680 --> 00:26:53,080
or whether we wrote it.

510
00:26:53,080 --> 00:26:54,640
So in that sense, they look quite like us,

511
00:26:54,640 --> 00:26:57,240
but inside they're not working the same way.

512
00:26:57,240 --> 00:26:59,240
>> Who is first in the room?

513
00:26:59,240 --> 00:27:05,840
>> Hello. My name is Hal Gregerson and my middle name is not 9,000.

514
00:27:05,840 --> 00:27:10,840
I'm a faculty or in the MIT Sloan School.

515
00:27:10,840 --> 00:27:16,840
Arguably asking questions is one of the most important human abilities we have.

516
00:27:16,840 --> 00:27:20,840
From your perspective now in 2023,

517
00:27:20,840 --> 00:27:24,840
What question or two should we pay most attention to?

518
00:27:24,840 --> 00:27:35,840
And is it possible for these technologies to actually help us ask better questions and out question the technology?

519
00:27:35,840 --> 00:27:44,840
Yes, but what I'm saying is, there's many questions we should be asking, but one of them is, how do we prevent them from taking over?

520
00:27:45,840 --> 00:27:48,320
I do rent them from getting control.

521
00:27:48,320 --> 00:27:52,760
And we could ask them questions about that,

522
00:27:52,760 --> 00:27:55,040
but I wouldn't entirely trust their answers.

523
00:27:55,040 --> 00:27:58,520
- Question at the back.

524
00:27:58,520 --> 00:28:00,440
And I wanna get through his manys we can,

525
00:28:00,440 --> 00:28:03,080
so if you can keep your questions as short as possible.

526
00:28:03,080 --> 00:28:06,280
- This is on, yeah.

527
00:28:06,280 --> 00:28:09,520
Dr. Hinton, thank you so much for being here with us today.

528
00:28:09,520 --> 00:28:11,920
I shall say, this is the most expensive lecture

529
00:28:11,920 --> 00:28:14,600
I've ever paid for, but I think it was worthwhile.

530
00:28:14,600 --> 00:28:24,040
I just have a question for you because you mentioned the analogy of nuclear history.

531
00:28:24,040 --> 00:28:26,520
And obviously there's a lot of comparisons.

532
00:28:26,520 --> 00:28:32,000
By any chance do you remember what President Truman told Openheimer when he was in the

533
00:28:32,000 --> 00:28:33,000
Oval Office?

534
00:28:33,000 --> 00:28:34,000
No, I don't.

535
00:28:34,000 --> 00:28:40,560
I know something about that, but I don't know what Truman told up like.

536
00:28:40,560 --> 00:28:41,560
Thank you.

537
00:28:41,560 --> 00:28:44,200
Well, take it from here.

538
00:28:44,200 --> 00:28:49,600
audience question. Sorry, if there are people that might, let me know who's next.

539
00:28:49,600 --> 00:28:52,360
Maybe give a... Go ahead.

540
00:28:52,360 --> 00:28:58,840
Hello, Jacob Woodruff. With the amount of data that's been required to train these

541
00:28:58,840 --> 00:29:05,240
large language models, would we expect a plateau in the intelligence of these systems

542
00:29:05,240 --> 00:29:09,960
and how might that slow down or restrict the advancement?

543
00:29:09,960 --> 00:29:15,560
Okay, so that is a ray of hope that maybe we've just used up all human knowledge and they're not going to get it in smarter.

544
00:29:15,560 --> 00:29:19,360
But think about images and video.

545
00:29:19,360 --> 00:29:26,760
So multimodal models will be much smarter than models that just train on language alone.

546
00:29:26,760 --> 00:29:30,360
They'll have a much better idea of how to deal with space, for example.

547
00:29:30,360 --> 00:29:38,360
And in terms of the amount of total video, we still don't have very good ways of processing video in these models.

548
00:29:38,560 --> 00:29:40,080
of modeling video.

549
00:29:40,080 --> 00:29:41,760
We're getting better all the time.

550
00:29:41,760 --> 00:29:44,840
But I think there's plenty of data in things like video

551
00:29:44,840 --> 00:29:46,800
that tell you how the world works.

552
00:29:46,800 --> 00:29:49,000
So we're not hitting the data limits

553
00:29:49,000 --> 00:29:50,760
for multi-modal models yet.

554
00:29:50,760 --> 00:29:55,040
- Next, gentlemen, the back.

555
00:29:55,040 --> 00:29:57,040
And please, please, do keep your question short.

556
00:29:57,040 --> 00:29:58,680
- Hello, Dr. Rathindra Rajeev,

557
00:29:58,680 --> 00:30:00,440
several of you from PWC.

558
00:30:00,440 --> 00:30:02,920
The point that I wanted to understand is

559
00:30:02,920 --> 00:30:04,880
that everything that AI is doing

560
00:30:04,880 --> 00:30:07,720
is learning from what we are teaching them.

561
00:30:07,720 --> 00:30:10,840
OK, data, yes, they are fast-read learning.

562
00:30:10,840 --> 00:30:14,280
One trillion connectors can do much more than 100 trillion

563
00:30:14,280 --> 00:30:15,640
connectors that we have.

564
00:30:15,640 --> 00:30:17,840
But every piece of human evolution

565
00:30:17,840 --> 00:30:20,520
has been driven by thought experiments,

566
00:30:20,520 --> 00:30:22,320
like Einstein used to do thought experiments

567
00:30:22,320 --> 00:30:25,600
because there was no speed of light out here on this planet.

568
00:30:25,600 --> 00:30:28,280
How can AI get to that point, if at all,

569
00:30:28,280 --> 00:30:30,800
and if it cannot, then how can we possibly

570
00:30:30,800 --> 00:30:32,520
have an existential threat from them

571
00:30:32,520 --> 00:30:35,000
because they will not be self-learning, so to say?

572
00:30:35,000 --> 00:30:37,400
They will be self-learning limited to the model

573
00:30:37,400 --> 00:30:39,400
that retell them.

574
00:30:39,400 --> 00:30:42,680
I think that's a very interesting argument,

575
00:30:42,680 --> 00:30:45,640
but I think they will be able to do thought experiments.

576
00:30:45,640 --> 00:30:47,120
I think they'll be able to reason.

577
00:30:47,120 --> 00:30:49,320
So let me give you an analogy.

578
00:30:49,320 --> 00:30:53,760
If you take alpha 0, which plays chess,

579
00:30:53,760 --> 00:30:56,160
it has three ingredients.

580
00:30:56,160 --> 00:30:58,840
It's got something that evaluates the board position

581
00:30:58,840 --> 00:31:00,600
to say is that good for me.

582
00:31:00,600 --> 00:31:02,680
It's got something that looks at a board position

583
00:31:02,680 --> 00:31:05,880
and says what's the sense of all move to consider.

584
00:31:05,880 --> 00:31:07,720
And then it's got Monte Carlo Rollout,

585
00:31:07,720 --> 00:31:09,240
where it does what's called calculation

586
00:31:09,240 --> 00:31:11,040
where you think, if I go here and he goes there

587
00:31:11,040 --> 00:31:13,320
and I go here and he goes there.

588
00:31:13,320 --> 00:31:16,720
Now, suppose you leave out the Monte Carlo Rollout

589
00:31:16,720 --> 00:31:19,480
and you just train it from human experts

590
00:31:19,480 --> 00:31:21,080
to have a good evaluation function

591
00:31:21,080 --> 00:31:24,480
and a good way to choose moves to consider.

592
00:31:24,480 --> 00:31:27,200
It still plays a pretty good game of chess.

593
00:31:27,200 --> 00:31:30,680
And I think that's what we've got with the chatbots.

594
00:31:30,680 --> 00:31:34,080
And we haven't gotten doing internal reasoning,

595
00:31:34,080 --> 00:31:35,560
but that will come.

596
00:31:35,560 --> 00:31:37,360
And once they start doing internal reasoning,

597
00:31:37,360 --> 00:31:38,800
to check for the consistency

598
00:31:38,800 --> 00:31:41,120
between the different things they believe,

599
00:31:41,120 --> 00:31:42,200
then they'll get much smarter

600
00:31:42,200 --> 00:31:44,640
and they will be able to do thought experiments.

601
00:31:44,640 --> 00:31:49,960
And one reason they haven't got this internal reasoning

602
00:31:49,960 --> 00:31:53,520
is because they've been trained from inconsistent data.

603
00:31:53,520 --> 00:31:55,200
And so it's very hard for them to do reasoning

604
00:31:55,200 --> 00:31:58,680
because they've been trained on all these inconsistent beliefs.

605
00:31:58,680 --> 00:32:00,800
And I think they're gonna have to be trained

606
00:32:00,800 --> 00:32:05,280
So they say, you know, if I have this ideology,

607
00:32:05,280 --> 00:32:06,040
then this is true.

608
00:32:06,040 --> 00:32:08,240
And if I have that ideology, then that is true.

609
00:32:08,240 --> 00:32:09,520
And once they're trained like that,

610
00:32:09,520 --> 00:32:11,640
within an ideology, they're going to be able to try

611
00:32:11,640 --> 00:32:13,240
and get consistency.

612
00:32:13,240 --> 00:32:16,200
And so we're going to get a move like from a version of Alpha

613
00:32:16,200 --> 00:32:19,800
Zero that just has something that guesses good moves

614
00:32:19,800 --> 00:32:22,160
and something that evaluates positions

615
00:32:22,160 --> 00:32:25,120
to a version that has long chains of Monte Carlo

616
00:32:25,120 --> 00:32:27,680
rollout, which is equivalent of reasoning.

617
00:32:27,680 --> 00:32:30,560
And it's going to get much better.

618
00:32:30,560 --> 00:32:32,000
I'm going to take one on the front here,

619
00:32:32,000 --> 00:32:33,440
and if you can be quick,

620
00:32:33,440 --> 00:32:35,440
we'll have front screens that morning as well.

621
00:32:35,440 --> 00:32:38,240
- Luis Lambe, Jeff, I know you from a long time.

622
00:32:38,240 --> 00:32:41,880
Jeff, people criticize language models

623
00:32:41,880 --> 00:32:45,000
because of allegedly they are lacking semantics

624
00:32:45,000 --> 00:32:46,640
and grounding to the world,

625
00:32:46,640 --> 00:32:49,400
and you have been trying to as well to explain

626
00:32:49,400 --> 00:32:51,760
how neural networks work for a long time.

627
00:32:51,760 --> 00:32:55,240
Is the question of semantics and explainability relevant here

628
00:32:55,240 --> 00:32:58,680
or language models have taken over and it's

629
00:32:58,680 --> 00:33:03,680
we are now doomed to go forward without semantics or ground into reality.

630
00:33:03,680 --> 00:33:09,080
I find it very hard to believe that they don't have semantics

631
00:33:09,080 --> 00:33:12,240
when they consult problems like, you know, how I paint the rooms,

632
00:33:12,240 --> 00:33:15,880
how I get all the rooms in my house to be painted white in two years time.

633
00:33:15,880 --> 00:33:19,920
I mean, whatever semantic is, is to do with the meaning of that stuff

634
00:33:19,920 --> 00:33:22,960
and it understood the meaning, it got it.

635
00:33:22,960 --> 00:33:27,960
Now, I agree it's not grounded by being a robot

636
00:33:28,160 --> 00:33:30,680
But you can make multimodal ones that are grounded,

637
00:33:30,680 --> 00:33:32,000
Google's done that.

638
00:33:32,000 --> 00:33:34,160
And the multimodal ones that are grounded,

639
00:33:34,160 --> 00:33:36,720
you can say, please close the drawer,

640
00:33:36,720 --> 00:33:38,200
and then reach out and grab the handle

641
00:33:38,200 --> 00:33:39,560
and close the drawer.

642
00:33:39,560 --> 00:33:41,960
And it's very hard to say that doesn't have semantics.

643
00:33:41,960 --> 00:33:44,600
In fact, in the very early days of AI,

644
00:33:44,600 --> 00:33:47,600
in the days of winner-grout in the 1970s,

645
00:33:47,600 --> 00:33:50,200
they had just a simulated world,

646
00:33:50,200 --> 00:33:52,360
but they have what was called procedural semantics,

647
00:33:52,360 --> 00:33:55,880
where if you said to it, put the red box,

648
00:33:55,880 --> 00:33:59,120
in put the red block in the green box.

649
00:33:59,120 --> 00:34:00,840
And it put the red block in the green box.

650
00:34:00,840 --> 00:34:04,040
She said, see, it understood the language.

651
00:34:04,040 --> 00:34:07,000
And that was the criterion people used back then.

652
00:34:07,000 --> 00:34:08,440
But now that neural nets can do it,

653
00:34:08,440 --> 00:34:12,080
they say that's not a radical criterion.

654
00:34:12,080 --> 00:34:13,880
One at the back.

655
00:34:13,880 --> 00:34:16,720
Hey, Jeff, this is Eshwar Balani from SAI Group.

656
00:34:16,720 --> 00:34:19,960
So clearly, the technology is advancing

657
00:34:19,960 --> 00:34:21,800
at an exponential pace.

658
00:34:21,800 --> 00:34:23,800
I wanted to get your thoughts--

659
00:34:23,800 --> 00:34:28,800
If you looked at the near and medium term, say, one, two, three, or maybe five year horizon,

660
00:34:28,800 --> 00:34:35,800
what the social and economic implications are from a societal perspective with job loss

661
00:34:35,800 --> 00:34:41,800
or maybe new jobs being created, just wanted to get your thoughts on how we proceed

662
00:34:41,800 --> 00:34:44,800
given the state of the technology and rate of change.

663
00:34:44,800 --> 00:34:50,800
Yes, so the alarm I'm ringing is to do with the existential threat of them taking control.

664
00:34:50,800 --> 00:34:52,800
Lots of other people talked about that.

665
00:34:52,800 --> 00:34:57,120
that, and I don't consider myself to be an expert on that, but there's some very obvious

666
00:34:57,120 --> 00:35:03,880
things that they're going to make a whole bunch of jobs much more efficient. So I know

667
00:35:03,880 --> 00:35:08,720
someone who answers letters of complaint to a health service, and he used to take 25 minutes

668
00:35:08,720 --> 00:35:13,480
writing a letter, and now it takes him five minutes because he gives it to chat GPT

669
00:35:13,480 --> 00:35:17,520
and chat GPT writes the letter for him, and then he just checks it. There'll be lots of

670
00:35:17,520 --> 00:35:21,520
stuff like that, which is going to cause huge increases in productivity.

671
00:35:21,520 --> 00:35:26,120
There will be delays because people are very conservative about adopting new technology,

672
00:35:26,120 --> 00:35:29,280
but I think there's going to be huge increases in productivity.

673
00:35:29,280 --> 00:35:34,080
My worry is that those increases in productivity are going to go to putting people out of work

674
00:35:34,080 --> 00:35:37,480
and making the rich richer and the poor poorer.

675
00:35:37,480 --> 00:35:42,960
As you do that, as you make that gap bigger, society gets more and more violent, think

676
00:35:42,960 --> 00:35:45,240
called the genie index, which predicts quite well how much

677
00:35:45,240 --> 00:35:48,280
balance there is.

678
00:35:48,280 --> 00:35:53,640
So this technology which ought to be wonderful--

679
00:35:53,640 --> 00:35:56,560
even the good uses of technology, if you're doing helpful things,

680
00:35:56,560 --> 00:35:58,080
ought to be wonderful.

681
00:35:58,080 --> 00:36:00,440
But our current political systems

682
00:36:00,440 --> 00:36:03,960
is going to be used to make the rich richer in the poor poorer.

683
00:36:03,960 --> 00:36:07,560
You might be able to ameliorate that by having

684
00:36:07,560 --> 00:36:10,440
a basic income that everybody gets.

685
00:36:10,440 --> 00:36:22,440
But the technology is being developed in a society that is not designed to use it for everybody's good.

686
00:36:22,440 --> 00:36:30,440
A question here from Joe Castalda of the Global Mail, who's in the audience.

687
00:36:30,440 --> 00:36:34,440
Do you intend to hold onto your investments in Kehere and other companies?

688
00:36:34,440 --> 00:36:36,440
And if so, why?

689
00:36:38,440 --> 00:36:45,440
Well, I could take the money and I could put it in the bank and let them profit from it.

690
00:36:45,440 --> 00:36:47,440
It's...

691
00:36:47,440 --> 00:36:53,280
Yes, I'm going to hold onto my investments here, partly because the people around

692
00:36:53,280 --> 00:36:56,440
co-herent are friends of mine.

693
00:36:56,440 --> 00:37:02,440
I sort of believe these big language balls are going to be very helpful.

694
00:37:02,440 --> 00:37:09,640
I think the technology should be good and it should make things work better.

695
00:37:09,640 --> 00:37:14,520
It's the politics we need to fix for things like employment.

696
00:37:14,520 --> 00:37:20,680
But when it comes to the existential threat, we have to think how we can keep control of the technology.

697
00:37:20,680 --> 00:37:26,000
But the good news there is that we're all in the same boat so we might be able to get cooperation.

698
00:37:26,000 --> 00:37:32,400
And in speaking out, I mean, part of your thinking as I understand it is that you actually want to engage with the people

699
00:37:32,400 --> 00:37:40,080
making this technology and you change their minds or maybe make a case for I

700
00:37:40,080 --> 00:37:43,520
don't really know. I mean we've established that we don't really know what to do

701
00:37:43,520 --> 00:37:48,200
but it's about engaging rather than stepping back. So one of the things that

702
00:37:48,200 --> 00:37:55,120
made me leave Google and go public with this is to say he used to be a Julia

703
00:37:55,120 --> 00:38:00,720
professor but he's now a middle-ranked professor who I think very highly of

704
00:38:00,720 --> 00:38:05,280
who encouraged me to do this. He said, "Jeff, you need to speak out. They'll listen to you.

705
00:38:05,280 --> 00:38:08,080
People are just blind to this danger."

706
00:38:08,080 --> 00:38:10,160
And...

707
00:38:10,160 --> 00:38:13,920
Do you think people are listening, though?

708
00:38:13,920 --> 00:38:17,440
Yeah, no. I think everyone in this room is listening for a start.

709
00:38:17,440 --> 00:38:24,480
Just one last question. We're out of time, but do you have regrets that you're involved in

710
00:38:24,480 --> 00:38:24,960
making this?

711
00:38:25,840 --> 00:38:29,680
Cape Metz tried very hard to get me to say I had regrets.

712
00:38:29,680 --> 00:38:31,200
Cape Metz at the New York Times.

713
00:38:31,200 --> 00:38:36,640
And yes, and in the end, I said, well, maybe slight regrets,

714
00:38:36,640 --> 00:38:39,360
which got reported as has regrets.

715
00:38:39,360 --> 00:38:43,840
But I don't think I made any bad decisions in doing research.

716
00:38:43,840 --> 00:38:46,960
I think it was perfectly reasonable back in the 70s and 80s

717
00:38:46,960 --> 00:38:49,360
to do research on how to make artificial neural nets.

718
00:38:49,360 --> 00:38:52,400
But it wasn't really foreseeable.

719
00:38:52,400 --> 00:38:54,160
This stage of it wasn't foreseeable.

720
00:38:54,560 --> 00:38:58,640
And until very recently I thought this existential crisis was a long way off.

721
00:38:58,640 --> 00:39:02,000
So I don't really have any regrets about what I did.

722
00:39:02,000 --> 00:39:10,080
Thank you, Geoffrey. Thank you so much for joining us.

723
00:39:10,080 --> 00:39:12,080
[APPLAUSE]

724
00:39:12,080 --> 00:39:22,080
[BLANK_AUDIO]
