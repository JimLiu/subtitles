[Script Info]

Title: DiffusionModels_Conclusion_v02
ScriptType: v4.00+
WrapStyle: 0
Collisions: Reverse
PlayResX: 384
PlayResY: 288
Timer: 100.0000
ScaledBorderAndShadow: no
Last Style Storage: Default
Video Aspect Ratio: 0
Video Zoom: 6
Video Position: 0

[V4+ Styles]
Format: Name,Fontname,Fontsize,PrimaryColour,SecondaryColour,OutlineColour,BackColour,Bold,Italic,Underline,StrikeOut,ScaleX,ScaleY,Spacing,Angle,BorderStyle,Outline,Shadow,Alignment,MarginL,MarginR,MarginV,Encoding
Style: Default,LXGW WenKai,20,&H000092FE,&H000000FF,&H00000000,&H00000000,-1,0,0,0,100,100,0,0,1,2,2,2,1,1,6,1
Style: Secondary,Helvetica,12,&H00FFFFFF,&H000000FF,&H00000000,&H00000000,-1,0,0,0,100,100,0,0,1,2,2,2,1,1,6,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
Dialogue: 0,0:00:05.0,0:00:09.52,Secondary,,0,0,0,,Congratulations on learning about the foundations of diffusion models.
Dialogue: 0,0:00:09.52,0:00:13.80,Secondary,,0,0,0,,Now putting it all together, you're able to train a diffusion model to predict noise,
Dialogue: 0,0:00:13.80,0:00:18.8,Secondary,,0,0,0,,and iteratively subtract the predicted noise from pure noise to get a good image.
Dialogue: 0,0:00:18.8,0:00:24.40,Secondary,,0,0,0,,You're able to sample images from that trained neural network, fast too with a more efficient sampler called DDIM.
Dialogue: 0,0:00:24.40,0:00:27.14,Secondary,,0,0,0,,You went through the model architecture, a unit.
Dialogue: 0,0:00:27.14,0:00:34.60,Secondary,,0,0,0,,You put context into the model so that you could decide whether you wanted food or spells or a hero sprite out, or something quirky in between.
Dialogue: 0,0:00:34.60,0:00:38.0,Secondary,,0,0,0,,Finally, you explored and ran the code for all of this.
Dialogue: 0,0:00:38.0,0:00:42.52,Secondary,,0,0,0,,Now you can create your own dataset and try to generate new things.
Dialogue: 0,0:00:42.52,0:00:47.4,Secondary,,0,0,0,,Diffusion models aren't bound to images either, that's just where they've been the most popular.
Dialogue: 0,0:00:47.4,0:00:54.36,Secondary,,0,0,0,,There are diffusion models for music, where you can give it any prompt and get music out, for proposing new molecules to accelerate drug discovery.
Dialogue: 0,0:00:54.36,0:00:56.90,Secondary,,0,0,0,,You can also try a larger dataset, try a new sampler.
Dialogue: 0,0:00:56.90,0:01:01.64,Secondary,,0,0,0,,There are actually a ton out there that are even faster and better than DDIM.
Dialogue: 0,0:01:01.64,0:01:08.48,Secondary,,0,0,0,,You can do more with these models, such as inpainting, which is letting the diffusion model paint something around an existing image you already have.
Dialogue: 0,0:01:08.48,0:01:15.76,Secondary,,0,0,0,,And textual inversion, which enables the model to capture an entirely new text concept with just a few sample images.
Dialogue: 0,0:01:15.76,0:01:18.44,Secondary,,0,0,0,,You covered the basics here, the foundations.
Dialogue: 0,0:01:18.44,0:01:20.68,Secondary,,0,0,0,,There are other important developments in this space.
Dialogue: 0,0:01:20.68,0:01:30.56,Secondary,,0,0,0,,For example, Stable Diffusion uses a method called latent diffusion, which operates on image embeddings instead of images directly to make the process even more efficient.
Dialogue: 0,0:01:30.56,0:01:35.72,Secondary,,0,0,0,,Other cool methods to call out are cross-attention text conditioning and classifier-free guidance.
Dialogue: 0,0:01:35.72,0:01:41.84,Secondary,,0,0,0,,And the research community is still working on faster sampling methods because it's still slower than other generative models at inference time.
Dialogue: 0,0:01:41.84,0:01:51.48,Secondary,,0,0,0,,All in all, this is an extremely exciting time for diffusion models and generative models as a whole, as they improve and their applications become ever more widespread.
Dialogue: 0,0:01:51.48,0:01:55.84,Secondary,,0,0,0,,Thank you so much for joining me in this course, and I look forward to seeing what you build.
Dialogue: 0,0:00:05.0,0:00:09.52,Default,,0,0,0,,恭喜你学会了扩散模型的基础知识。
Dialogue: 0,0:00:09.52,0:00:13.80,Default,,0,0,0,,现在把所有内容整合在一起，\N你已经能够训练扩散模型来预测噪声，
Dialogue: 0,0:00:13.80,0:00:18.8,Default,,0,0,0,,并迭代地从纯噪声中减去预测\N的噪声，得到一幅好的图像。
Dialogue: 0,0:00:18.8,0:00:24.40,Default,,0,0,0,,你可以从训练好的神经网络中快速\N采样图像，使用更高效的采样器DDIM。
Dialogue: 0,0:00:24.40,0:00:27.14,Default,,0,0,0,,你了解了模型结构，一个单元。
Dialogue: 0,0:00:27.14,0:00:34.60,Default,,0,0,0,,你将上下文融入模型，以便你可以选择想要的食物、\N魔法、英雄形象，或者介于两者之间的有趣事物。
Dialogue: 0,0:00:34.60,0:00:38.0,Default,,0,0,0,,最后，你探索并运行了所有这些代码。
Dialogue: 0,0:00:38.0,0:00:42.52,Default,,0,0,0,,现在你可以创建自己的数据集，尝试生成新的东西。
Dialogue: 0,0:00:42.52,0:00:47.4,Default,,0,0,0,,扩散模型并不仅限于图像，只是它们在这方面最受欢迎。
Dialogue: 0,0:00:47.4,0:00:54.36,Default,,0,0,0,,还有用于音乐的扩散模型，你可以给它任何提示，\N然后得到音乐，还可以提议新的分子来加速药物发现。
Dialogue: 0,0:00:54.36,0:00:56.90,Default,,0,0,0,,你也可以尝试更大的数据集，尝试一个新的采样器。
Dialogue: 0,0:00:56.90,0:01:01.64,Default,,0,0,0,,实际上有很多比DDIM更快更好的。
Dialogue: 0,0:01:01.64,0:01:08.48,Default,,0,0,0,,你可以用这些模型做更多事情，比如图像修复，\N让扩散模型在你已有的图像周围绘制一些东西。
Dialogue: 0,0:01:08.48,0:01:15.76,Default,,0,0,0,,还有文本反演，它可以让模型\N通过几个样本图像捕捉到全新的文本概念。
Dialogue: 0,0:01:15.76,0:01:18.44,Default,,0,0,0,,你已经掌握了基础知识，这是基础。
Dialogue: 0,0:01:18.44,0:01:20.68,Default,,0,0,0,,这个领域还有其他重要的发展。
Dialogue: 0,0:01:20.68,0:01:30.56,Default,,0,0,0,,例如，Stable Diffusion采用一种\N名为潜在扩散的方法，它直接在图像嵌入\N上操作，而不是在图像上操作，使过程更加高效。
Dialogue: 0,0:01:30.56,0:01:35.72,Default,,0,0,0,,其他值得一提的酷炫方法还有\N交叉注意力文本调节和无分类器引导。
Dialogue: 0,0:01:35.72,0:01:41.84,Default,,0,0,0,,研究界还在研究更快的采样方法，\N因为它在推理时仍然比其他生成模型慢。
Dialogue: 0,0:01:41.84,0:01:51.48,Default,,0,0,0,,总的来说，对于扩散模型和整个生成模型\N来说，这是一个非常激动人心的时期，\N因为它们在改进，并且应用越来越广泛。
Dialogue: 0,0:01:51.48,0:01:55.84,Default,,0,0,0,,非常感谢你参加这门课程，期待看到你的作品。