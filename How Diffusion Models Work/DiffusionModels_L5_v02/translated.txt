在这个视频中，你将学习\N如何控制模型和生成的内容。 对很多人来说，这是最令人兴奋的\N部分，因为你可以告诉模型想要\N什么，然后它帮你实现你想要的。 在控制这些模型时，我们实际上\N想要使用嵌入（Embeddings）。 嵌入是什么呢？我们在之前的视频中\N稍微了解过时间嵌入和上下文嵌入， 嵌入就是向量，\N是能够捕捉意义的数字。 在这里，它捕捉了这个句子或这个\N笑话的意义，可能是针对扩散模型的。 布朗运动的粒子经常相互碰撞。 所以它将这个意义编码到这个\N嵌入中，也就是向量中的一组数字。 嵌入的特殊之处在于，\N因为它们可以捕捉语义意义，\N内容相似的文本将具有相似的向量。 关于嵌入的神奇之处之一是，\N你甚至可以用它进行向量运算。 例如：将巴黎的嵌入减去法国的嵌入\N加上英国的嵌入等于伦敦的嵌入。 那么，在训练过程中，这些嵌入\N是如何成为模型的上下文的？ 好吧，这里有一个牛油果图片，\N你希望神经网络能理解它。 还有一个关于它的标题，\N一个成熟的牛油果。 实际上，你可以将其传递，获得\N一个嵌入并将其输入到神经网络中。 然后预测添加到这个\N牛油果图像的噪声，然后计算损失\N并像以前一样做同样的事情。 你可以在很多不同的\N带有标题的图片上做这个。 这里有一个舒适的扶手椅，\N你可以将其通过嵌入，传递到\N模型中，并成为训练的一部分。 现在这部分的神奇之处\N在于，虽然你能够从互联网上抓取\N这些牛油果和扶手椅的图片和标题， 但在抽样时，\N你能够生成模型从未见过的东西。 那可能是个牛油果扶手椅。 这个的神奇之处在于，\N你可以把牛油果扶手椅\N这几个词嵌入到这个词嵌入中， 里面有一点牛油果，一点扶手椅， 通过神经网络预测噪音，减去\N噪音，然后得到一个牛油果扶手椅。 从更广泛的角度来看，\N上下文是一个可以控制生成的向量。 上下文可以是我们现在看到\N的文本嵌入，比如牛油果扶手椅， 虽然很长，\N但上下文不一定要那么长。 上下文也可以是\N五个不同的类别，比如， 五个不同的维度，如英雄或非\N英雄，比如火球和蘑菇这样的物体。 还可以是食物，\N比如苹果、橙子、西瓜。 这可能是像这弓箭\N或蜡烛这样的法术和武器。 最后，这可能是\N这些游戏角色是否侧面的问题。 现在让我们在下一个实验室中\N看看如何为您的模型添加上下文。 在我们的实验室里，\N我们只需为所有这些东西\N运行设置，就像之前一样。 然后在这里的上下文中，\N我想再次实例化我们的神经网络。 同样，我们没有进行训练，但\N我会指出我们在哪里添加了上下文。 所以当我们在这里加载\N数据时，我们现在遍历数据点\N和与之相关的上下文因素。 我们拥有的上下文是这些\N英雄、非英雄、食物、法术和\N武器以及侧面的独热编码向量。 我们创建一个上下文遮罩。 这里重要的是，实际上，\N我们用一些随机性完全遮住 上下文，以便模型能够\N学习到什么是游戏角色。 这对扩散模型来说是很常见的。 然后我们在这里\N调用神经网络时添加上下文。 让我们加载一个\N带有上下文训练的模型检查点。 加载这个模型，运行它。 再次运行我们的采样代码。 这个Notebook就有了。 你可以看到运行时， 这里实际上是\N完全随机选择的上下文。 你可以看到\N不同类型的物体和人物输出。 现在稍微控制一下，\N你可以在这里定义它。 我在这里定义了两个英雄，\N第一个和第二个。 这两个都是英雄。 接下来的两个是侧面的。 最后一个值是侧面的。 接下来的两个不是英雄。 有点像野兽。 它们看起来很胖乎乎的。 最后两个是食物。 这个看起来像苹果。 这个看起来像梨子。 现在我们可以混合这些一下，\N就像牛油果扶手椅的感觉那样。 虽然我们在一个\N热编码的向量上训练了它， 但我们也可以用0到1之间\N的浮点数来获得各种混合效果。 所以这里的第二个\N是英雄和部分食物。 现在它看起来像个土豆人。 第三个也有点古怪。 它是部分食物和部分法术。 所以它看起来像这种药水。 不过，你可以自己试试。 你可以尝试输入一些矛盾的东西，\N比如它应该是个英雄，但同时\N又是侧面的，就像正面和侧面都有。 这很有趣。 建议你随时停下来，\N暂停，玩几次，尝试改变这些值。 现在你可以创建\N所有这些样本并控制它们， 在下一个视频中，你将学习如何\N加速采样过程，这样不用等待\N太久就能看到这些精彩的样本了。