1
00:00:05,000 --> 00:00:09,520
Congratulations on learning about the foundations of diffusion models.

2
00:00:09,520 --> 00:00:13,800
Now putting it all together, you're able to train a diffusion model to predict noise,

3
00:00:13,800 --> 00:00:18,080
and iteratively subtract the predicted noise from pure noise to get a good image.

4
00:00:18,080 --> 00:00:24,400
You're able to sample images from that trained neural network, fast too with a more efficient sampler called DDIM.

5
00:00:24,400 --> 00:00:27,140
You went through the model architecture, a unit.

6
00:00:27,140 --> 00:00:34,600
You put context into the model so that you could decide whether you wanted food or spells or a hero sprite out, or something quirky in between.

7
00:00:34,600 --> 00:00:38,000
Finally, you explored and ran the code for all of this.

8
00:00:38,000 --> 00:00:42,520
Now you can create your own dataset and try to generate new things.

9
00:00:42,520 --> 00:00:47,040
Diffusion models aren't bound to images either, that's just where they've been the most popular.

10
00:00:47,040 --> 00:00:54,360
There are diffusion models for music, where you can give it any prompt and get music out, for proposing new molecules to accelerate drug discovery.

11
00:00:54,360 --> 00:00:56,900
You can also try a larger dataset, try a new sampler.

12
00:00:56,900 --> 00:01:01,640
There are actually a ton out there that are even faster and better than DDIM.

13
00:01:01,640 --> 00:01:08,480
You can do more with these models, such as inpainting, which is letting the diffusion model paint something around an existing image you already have.

14
00:01:08,480 --> 00:01:15,760
And textual inversion, which enables the model to capture an entirely new text concept with just a few sample images.

15
00:01:15,760 --> 00:01:18,440
You covered the basics here, the foundations.

16
00:01:18,440 --> 00:01:20,680
There are other important developments in this space.

17
00:01:20,680 --> 00:01:30,560
For example, Stable Diffusion uses a method called latent diffusion, which operates on image embeddings instead of images directly to make the process even more efficient.

18
00:01:30,560 --> 00:01:35,720
Other cool methods to call out are cross-attention text conditioning and classifier-free guidance.

19
00:01:35,720 --> 00:01:41,840
And the research community is still working on faster sampling methods because it's still slower than other generative models at inference time.

20
00:01:41,840 --> 00:01:51,480
All in all, this is an extremely exciting time for diffusion models and generative models as a whole, as they improve and their applications become ever more widespread.

21
00:01:51,480 --> 00:01:55,840
Thank you so much for joining me in this course, and I look forward to seeing what you build.

